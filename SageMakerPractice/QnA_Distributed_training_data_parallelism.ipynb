{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f748d8-d3c6-4c42-a918-7ddd788e6143",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SageMaker distributed data parallelism (SMDDP)\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel-intro.html\n",
    "The SageMaker distributed data parallelism (SMDDP) library is a collective communication library that improves compute performance of distributed data parallel training. The SMDDP library addresses communications overhead of the key collective communication operations by offering the following.\n",
    "1. The library offers AllReduce optimized for AWS. AllReduce is a key operation used for synchronizing gradients across GPUs at the end of each training iteration during distributed data training.\n",
    "2. The library offers AllGather optimized for AWS. AllGather is another key operation used in sharded data parallel training, which is a memory-efficient data parallelism technique offered by popular libraries such as the SageMaker model parallelism (SMP) library, DeepSpeed Zero Redundancy Optimizer (ZeRO), and PyTorch Fully Sharded Data Parallelism (FSDP).\n",
    "3. The library performs optimized node-to-node communication by fully utilizing AWS network infrastructure and the Amazon EC2 instance topology.\n",
    "\n",
    "The following is the high-level workflow of the SMDDP AllReduce operation.\n",
    "\n",
    "1. The library assigns ranks to GPUs (workers).\n",
    "2. At each iteration, the library divides each global batch by the total number of workers (world size) and assigns small batches (batch shards) to the workers.\n",
    "   1. The size of the global batch is (number of nodes in a cluster) * (number of GPUs per node) * (per batch shard).\n",
    "   2. A batch shard (small batch) is a subset of dataset assigned to each GPU (worker) per iteration.\n",
    "3. The library launches a training script on each worker.\n",
    "4. The library manages copies of model weights and gradients from the workers at the end of every iteration.\n",
    "5. The library synchronizes model weights and gradients across the workers to aggregate a single trained model.\n",
    "\n",
    "AllGather is heavily used in distributed training techniques such as sharded data parallelism where each individual worker holds a fraction of a model, or a sharded layer. The workers call AllGather before forward and backward passes to reconstruct the sharded layers. The forward and backward passes continue onward after the parameters are all gathered. During the backward pass, each worker also calls ReduceScatter to collect (reduce) gradients and break (scatter) them into gradient shards to update the corresponding sharded layer.\n",
    "\n",
    "# How to use SMDDP\n",
    "\n",
    "```python\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}\n",
    "\n",
    "# git configuration to download our fine-tuning script\n",
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.26.0'}\n",
    "\n",
    "instance_type='ml.p3.16xlarge'\n",
    "instance_count=2\n",
    "volume_size=200\n",
    "\n",
    "huggingface_estimator = HuggingFace(entry_point='run_qa.py',\n",
    "                                    source_dir='./examples/pytorch/question-answering',\n",
    "                                    git_config=git_config,\n",
    "                                    instance_type=instance_type,\n",
    "                                    instance_count=instance_count,\n",
    "                                    volume_size=volume_size,\n",
    "                                    distribution= distribution)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb6593f5-674e-4fc3-aa18-5ec8fe25c932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker>=2.48.0 in /opt/conda/lib/python3.10/site-packages (2.203.1)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.33.3 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (1.33.9)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (1.26.2)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (4.25.1)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (4.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (21.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (2.1.3)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (0.3.0)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from sagemaker>=2.48.0) (6.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (4.20.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (2.5.2)\n",
      "Requirement already satisfied: tblib<3,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (1.7.0)\n",
      "Requirement already satisfied: urllib3<1.27 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (1.26.18)\n",
      "Requirement already satisfied: uvicorn==0.22.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (0.22.0)\n",
      "Requirement already satisfied: fastapi==0.95.2 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (0.95.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (2.31.0)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (6.1.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (4.64.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (5.9.0)\n",
      "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from fastapi==0.95.2->sagemaker>=2.48.0) (1.10.13)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi==0.95.2->sagemaker>=2.48.0) (0.27.0)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn==0.22.0->sagemaker>=2.48.0) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn==0.22.0->sagemaker>=2.48.0) (0.14.0)\n",
      "Requirement already satisfied: botocore<1.34.0,>=1.33.9 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker>=2.48.0) (1.33.9)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker>=2.48.0) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.9.0,>=0.8.2 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker>=2.48.0) (0.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker>=2.48.0) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->sagemaker>=2.48.0) (3.0.9)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from docker->sagemaker>=2.48.0) (0.58.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker>=2.48.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker>=2.48.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker>=2.48.0) (2023.11.17)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-pasta->sagemaker>=2.48.0) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker>=2.48.0) (2023.11.2)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker>=2.48.0) (0.31.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker>=2.48.0) (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker>=2.48.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker>=2.48.0) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker>=2.48.0) (2023.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2.48.0) (1.7.6.7)\n",
      "Requirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2.48.0) (0.3.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2.48.0) (0.3.3)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2.48.0) (0.70.14)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker>=2.48.0) (21.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi==0.95.2->sagemaker>=2.48.0) (4.3.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker>=2.48.0) (3.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker>=2.48.0) (1.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.48.0\"  --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56ed49b1-6a30-46a3-b891-7791d96c5cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aa5edd6-12b8-4e2f-a931-a4934856d1c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::802575742115:role/service-role/AmazonSageMaker-ExecutionRole-20230929T143152\n",
      "sagemaker bucket: sagemaker-us-east-1-802575742115\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ac968f-cf38-4163-b3d3-f41a2478c1e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating an Estimator and start a training job\n",
    "In this example we are going to use the capability to download/use a fine-tuning script from a git- repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f541b22-72d3-4444-85db-db964962aa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'model_name_or_path': 'bert-large-uncased-whole-word-masking',\n",
    "    'dataset_name':'squad',\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'fp16': True,\n",
    "    'per_device_train_batch_size': 4,\n",
    "    'per_device_eval_batch_size': 4,\n",
    "    'num_train_epochs': 2,\n",
    "    'max_seq_length': 384,\n",
    "    'max_steps': 100,\n",
    "    'pad_to_max_length': True,\n",
    "    'doc_stride': 128,\n",
    "    'output_dir': '/opt/ml/model'\n",
    "}\n",
    "\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}\n",
    "\n",
    "# git configuration to download our fine-tuning script\n",
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.26.0'}\n",
    "\n",
    "# instance configurations\n",
    "instance_type='ml.p3.16xlarge'\n",
    "instance_count=2\n",
    "volume_size=200\n",
    "\n",
    "# metric definition to extract the results\n",
    "metric_definitions=[\n",
    "     {\"Name\": \"train_runtime\", \"Regex\": \"train_runtime.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'train_samples_per_second', 'Regex': \"train_samples_per_second.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'epoch', 'Regex': \"epoch.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'f1', 'Regex': \"f1.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'exact_match', 'Regex': \"exact_match.*=\\D*(.*?)$\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a30f4ee2-9cca-4792-b99d-f16ed3ac57f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '/tmp/tmpc9ezp7cy'...\n",
      "Note: switching to 'v4.26.0'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n",
      "HEAD is now at 820c46a70 Hotifx remove tuple for git config image processor. (#21278)\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2024-01-23-03-28-44-354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-23 03:29:08 Starting - Starting the training job\n",
      "2024-01-23 03:29:08 Pending - Training job waiting for capacity............\n",
      "2024-01-23 03:30:52 Pending - Preparing the instances for training.........\n",
      "2024-01-23 03:32:20 Downloading - Downloading input data...\n",
      "2024-01-23 03:32:45 Downloading - Downloading the training image.....................\n",
      "2024-01-23 03:36:39 Training - Training image download completed. Training in progress....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:03,854 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:03,918 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:03,930 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:03,933 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:03,933 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:04,246 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate>=0.12.0 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (0.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=1.8.0 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3.0 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mCollecting evaluate\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 4.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (1.5.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (11.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2023.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.3.0->-r requirements.txt (line 3)) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2022.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: evaluate\u001b[0m\n",
      "\u001b[34mSuccessfully installed evaluate-0.4.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.3.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:07,607 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:07,607 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:07,693 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:07,769 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:07,782 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:07,782 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:07,783 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:07,783 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.74.239.              Can be ignored for worker when master completes and exits.\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:08,785 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:08,785 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.74.239.              Can be ignored for worker when master completes and exits.\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:07,199 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:07,263 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:07,276 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:07,279 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:07,279 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:07,548 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate>=0.12.0 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (0.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=1.8.0 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3.0 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mCollecting evaluate\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 4.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (11.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (1.5.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2023.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.3.0->-r requirements.txt (line 3)) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2022.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[35m2024-01-23 03:37:09,787 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2024-01-23 03:37:09,787 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.74.239.              Can be ignored for worker when master completes and exits.\u001b[0m\n",
      "\u001b[34mInstalling collected packages: evaluate\u001b[0m\n",
      "\u001b[34mSuccessfully installed evaluate-0.4.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.3.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:10,963 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:10,963 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:11,047 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:11,124 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:11,137 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:11,137 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:11,139 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:11,140 sagemaker-training-toolkit INFO     Cannot connect to host algo-2 at port 22. Retrying...\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:11,140 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[35m2024-01-23 03:37:10,789 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2024-01-23 03:37:10,789 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.74.239.              Can be ignored for worker when master completes and exits.\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:12,153 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:12,320 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:12,320 sagemaker-training-toolkit INFO     Can connect to host algo-2 at port 22\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:12,320 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:12,320 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:12,320 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:12,320 sagemaker-training-toolkit INFO     Host: ['algo-1', 'algo-2']\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:12,321 sagemaker-training-toolkit INFO     sagemaker_communication_backend: None\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:12,321 sagemaker-training-toolkit WARNING  Missing library /opt/conda/lib/libsmddp.so for SMDDP collective\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:12,321 sagemaker-training-toolkit WARNING  The system is not configured to run SMDDP collectives optimizedfor AWS infrastructure.Please use the latest SageMaker Deep Learning Container (DLC) to enable SMDDP Collectives support.\u001b[0m\n",
      "\u001b[34mContinuing model training with default NCCL communication backend.\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:12,322 sagemaker-training-toolkit INFO     instance type: ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:12,322 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1', 'algo-2'] Hosts: ['algo-1:8', 'algo-2:8'] process_per_hosts: 8 num_processes: 16\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:12,387 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:12,402 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": true,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dataset_name\": \"squad\",\n",
      "        \"do_eval\": true,\n",
      "        \"do_train\": true,\n",
      "        \"doc_stride\": 128,\n",
      "        \"fp16\": true,\n",
      "        \"max_seq_length\": 384,\n",
      "        \"max_steps\": 100,\n",
      "        \"model_name_or_path\": \"bert-large-uncased-whole-word-masking\",\n",
      "        \"num_train_epochs\": 2,\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"pad_to_max_length\": true,\n",
      "        \"per_device_eval_batch_size\": 4,\n",
      "        \"per_device_train_batch_size\": 4\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.16xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2024-01-23-03-28-44-354\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-802575742115/huggingface-pytorch-training-2024-01-23-03-28-44-354/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_qa\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.16xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_qa.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"dataset_name\":\"squad\",\"do_eval\":true,\"do_train\":true,\"doc_stride\":128,\"fp16\":true,\"max_seq_length\":384,\"max_steps\":100,\"model_name_or_path\":\"bert-large-uncased-whole-word-masking\",\"num_train_epochs\":2,\"output_dir\":\"/opt/ml/model\",\"pad_to_max_length\":true,\"per_device_eval_batch_size\":4,\"per_device_train_batch_size\":4}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_qa.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_qa\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-802575742115/huggingface-pytorch-training-2024-01-23-03-28-44-354/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.p3.16xlarge\",\"distribution_hosts\":[\"algo-1\",\"algo-2\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"dataset_name\":\"squad\",\"do_eval\":true,\"do_train\":true,\"doc_stride\":128,\"fp16\":true,\"max_seq_length\":384,\"max_steps\":100,\"model_name_or_path\":\"bert-large-uncased-whole-word-masking\",\"num_train_epochs\":2,\"output_dir\":\"/opt/ml/model\",\"pad_to_max_length\":true,\"per_device_eval_batch_size\":4,\"per_device_train_batch_size\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2024-01-23-03-28-44-354\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-802575742115/huggingface-pytorch-training-2024-01-23-03-28-44-354/source/sourcedir.tar.gz\",\"module_name\":\"run_qa\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_qa.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--dataset_name\",\"squad\",\"--do_eval\",\"True\",\"--do_train\",\"True\",\"--doc_stride\",\"128\",\"--fp16\",\"True\",\"--max_seq_length\",\"384\",\"--max_steps\",\"100\",\"--model_name_or_path\",\"bert-large-uncased-whole-word-masking\",\"--num_train_epochs\",\"2\",\"--output_dir\",\"/opt/ml/model\",\"--pad_to_max_length\",\"True\",\"--per_device_eval_batch_size\",\"4\",\"--per_device_train_batch_size\",\"4\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_NAME=squad\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=true\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_DOC_STRIDE=128\u001b[0m\n",
      "\u001b[34mSM_HP_FP16=true\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LENGTH=384\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=100\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=bert-large-uncased-whole-word-masking\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_PAD_TO_MAX_LENGTH=true\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8,algo-2:8 -np 16 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 2 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_HOMOGENEOUS=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.9/site-packages/gethostname.cpython-39-x86_64-linux-gnu.so -x SMDATAPARALLEL_SERVER_ADDR=algo-1 -x SMDATAPARALLEL_SERVER_PORT=7592 -x SAGEMAKER_INSTANCE_TYPE=ml.p3.16xlarge -x USE_SMDDP_COLLECTIVES=0 smddprun /opt/conda/bin/python3.9 -m mpi4py run_qa.py --dataset_name squad --do_eval True --do_train True --doc_stride 128 --fp16 True --max_seq_length 384 --max_steps 100 --model_name_or_path bert-large-uncased-whole-word-masking --num_train_epochs 2 --output_dir /opt/ml/model --pad_to_max_length True --per_device_eval_batch_size 4 --per_device_train_batch_size 4\u001b[0m\n",
      "\u001b[35m2024-01-23 03:37:11,803 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[35m2024-01-23 03:37:11,969 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2024-01-23 03:37:11,969 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2024-01-23 03:37:11,969 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2024-01-23 03:37:11,969 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[35m2024-01-23 03:37:11,974 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m[2024-01-23 03:37:14.572: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2024-01-23 03:37:14,579 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,10.2.121.74' (ECDSA) to the list of known hosts.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[35m2024-01-23 03:37:14,981 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=64, name='orted', status='running', started='03:37:14')]\u001b[0m\n",
      "\u001b[35m2024-01-23 03:37:14,981 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=64, name='orted', status='running', started='03:37:14')]\u001b[0m\n",
      "\u001b[35m2024-01-23 03:37:14,982 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=64, name='orted', status='sleeping', started='03:37:14')]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Bootstrap : Using eth0:10.2.74.239<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:NCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.74.239<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Bootstrap : Using eth0:10.2.74.239<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Bootstrap : Using eth0:10.2.74.239<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Bootstrap : Using eth0:10.2.74.239<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Bootstrap : Using eth0:10.2.74.239<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO NET/Socket : Using [0]eth0:10.2.74.239<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Bootstrap : Using eth0:10.2.74.239<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO NET/Socket : Using [0]eth0:10.2.74.239<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Bootstrap : Using eth0:10.2.121.74<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO NET/Socket : Using [0]eth0:10.2.74.239<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO NET/Socket : Using [0]eth0:10.2.74.239<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Bootstrap : Using eth0:10.2.121.74<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO NET/Socket : Using [0]eth0:10.2.74.239<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Bootstrap : Using eth0:10.2.121.74<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Bootstrap : Using eth0:10.2.121.74<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO NET/Socket : Using [0]eth0:10.2.121.74<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Bootstrap : Using eth0:10.2.74.239<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Bootstrap : Using eth0:10.2.74.239<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO NET/Socket : Using [0]eth0:10.2.121.74<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Bootstrap : Using eth0:10.2.121.74<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Bootstrap : Using eth0:10.2.121.74<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO NET/Socket : Using [0]eth0:10.2.121.74<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO NET/Socket : Using [0]eth0:10.2.121.74<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO NET/Socket : Using [0]eth0:10.2.74.239<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO NET/Socket : Using [0]eth0:10.2.74.239<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.121.74<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO NET/Socket : Using [0]eth0:10.2.121.74<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Bootstrap : Using eth0:10.2.121.74<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO NET/Socket : Using [0]eth0:10.2.121.74<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Bootstrap : Using eth0:10.2.121.74<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO NET/Socket : Using [0]eth0:10.2.121.74<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Trees [0] 2/-1/-1->3->0 [1] 2/-1/-1->3->0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Trees [0] 6/-1/-1->5->1 [1] 6/-1/-1->5->1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 00/02 :    0   3   2   1   5   6   7   4   8  11  10   9  13  14  15  12\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 01/02 :    0   3   2   1   5   6   7   4   8  11  10   9  13  14  15  12\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Trees [0] 3/8/-1->0->-1 [1] 3/-1/-1->0->8\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Trees [0] 1/-1/-1->2->3 [1] 1/-1/-1->2->3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Trees [0] 5/-1/-1->1->2 [1] 5/-1/-1->1->2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Trees [0] 4/-1/-1->7->6 [1] 4/-1/-1->7->6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Trees [0] -1/-1/-1->4->7 [1] -1/-1/-1->4->7\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Trees [0] 14/-1/-1->13->9 [1] 14/-1/-1->13->9\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Trees [0] 12/-1/-1->15->14 [1] 12/-1/-1->15->14\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Trees [0] 13/-1/-1->9->10 [1] 13/-1/-1->9->10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Trees [0] -1/-1/-1->12->15 [1] -1/-1/-1->12->15\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Trees [0] 9/-1/-1->10->11 [1] 9/-1/-1->10->11\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Trees [0] 11/-1/-1->8->0 [1] 11/0/-1->8->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Trees [0] 10/-1/-1->11->8 [1] 10/-1/-1->11->8\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 00/0 : 13[1c0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 00/0 : 9[180] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 00/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 00/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 00/0 : 8[170] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 00/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 01/0 : 13[1c0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 01/0 : 8[170] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 01/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 01/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 01/0 : 9[180] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 01/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 00/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 00/0 : 14[1d0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 00/0 : 10[190] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 00/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 00/0 : 12[1b0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 01/0 : 14[1d0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 00/0 : 4[1b0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 01/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 01/0 : 10[190] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 01/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 00/0 : 11[1a0] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 00/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 01/0 : 11[1a0] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 01/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 00/0 : 15[1e0] -> 12[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 00/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 01/0 : 15[1e0] -> 12[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 00/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 00/0 : 14[1d0] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 00/0 : 13[1c0] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 00/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 01/0 : 14[1d0] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 01/0 : 13[1c0] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 01/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 01/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 01/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 00/0 : 9[180] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 01/0 : 9[180] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 00/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 01/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 00/0 : 10[190] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 01/0 : 10[190] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 00/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 01/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 00/0 : 11[1a0] -> 8[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 01/0 : 11[1a0] -> 8[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 00/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 00/1 : 10[190] -> 12[1b0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 01/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 00/1 : 9[180] -> 12[1b0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 00/1 : 2[190] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 00/1 : 1[180] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 01/1 : 10[190] -> 12[1b0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 01/1 : 2[190] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 01/1 : 9[180] -> 12[1b0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 01/1 : 1[180] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 01/0 : 4[1b0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 01/0 : 12[1b0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:1281 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 00/0 : 4[1b0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:1351 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 00/0 : 12[1b0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:1281 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 01/0 : 4[1b0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:1351 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 01/0 : 12[1b0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 00/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 01/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 00/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 01/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 00/0 : 12[1b0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 01/0 : 12[1b0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 00/0 : 15[1e0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 01/0 : 15[1e0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:1281 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 00/0 : 0[170] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:1351 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 00/0 : 8[170] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:1281 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 01/0 : 0[170] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:1351 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 01/0 : 8[170] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 00/0 : 8[170] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 00/0 : 0[170] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 01/0 : 8[170] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 01/0 : 0[170] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 00/1 : 0[170] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 00/1 : 3[1a0] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 00/1 : 8[170] -> 13[1c0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 00/1 : 11[1a0] -> 12[1b0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 01/1 : 3[1a0] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 01/1 : 0[170] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 01/1 : 8[170] -> 13[1c0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 01/1 : 11[1a0] -> 12[1b0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 00/1 : 3[1a0] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 00/1 : 11[1a0] -> 13[1c0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 01/1 : 3[1a0] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 01/1 : 11[1a0] -> 13[1c0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 00/1 : 3[1a0] -> 6[1d0] via P2P/indirect/7[1e0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 00/1 : 2[190] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 00/1 : 11[1a0] -> 14[1d0] via P2P/indirect/15[1e0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 00/1 : 10[190] -> 13[1c0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 01/1 : 3[1a0] -> 6[1d0] via P2P/indirect/7[1e0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 01/1 : 11[1a0] -> 14[1d0] via P2P/indirect/15[1e0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 01/1 : 2[190] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 01/1 : 10[190] -> 13[1c0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 00/1 : 4[1b0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 00/1 : 1[180] -> 6[1d0] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 00/1 : 2[190] -> 7[1e0] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 01/1 : 4[1b0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 00/1 : 12[1b0] -> 9[180] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 00/1 : 9[180] -> 14[1d0] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 00/1 : 10[190] -> 15[1e0] via P2P/indirect/14[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 01/1 : 12[1b0] -> 9[180] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 01/1 : 1[180] -> 6[1d0] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 01/1 : 2[190] -> 7[1e0] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 01/1 : 10[190] -> 15[1e0] via P2P/indirect/14[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 01/1 : 9[180] -> 14[1d0] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 00/1 : 1[180] -> 7[1e0] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 00/1 : 5[1c0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 00/1 : 0[170] -> 6[1d0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 00/1 : 9[180] -> 15[1e0] via P2P/indirect/11[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 01/1 : 5[1c0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 00/1 : 13[1c0] -> 8[170] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 00/1 : 8[170] -> 14[1d0] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 01/1 : 13[1c0] -> 8[170] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 01/1 : 1[180] -> 7[1e0] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 01/1 : 9[180] -> 15[1e0] via P2P/indirect/11[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 01/1 : 0[170] -> 6[1d0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 01/1 : 8[170] -> 14[1d0] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 00/1 : 6[1d0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 00/1 : 0[170] -> 7[1e0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 01/1 : 6[1d0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 00/1 : 8[170] -> 15[1e0] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 00/1 : 14[1d0] -> 8[170] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 01/1 : 0[170] -> 7[1e0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 01/1 : 8[170] -> 15[1e0] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 01/1 : 14[1d0] -> 8[170] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 00/1 : 7[1e0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 01/1 : 7[1e0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 00/1 : 15[1e0] -> 8[170] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 01/1 : 15[1e0] -> 8[170] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 00/1 : 7[1e0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 01/1 : 7[1e0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 00/1 : 15[1e0] -> 9[180] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 01/1 : 15[1e0] -> 9[180] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 00/1 : 6[1d0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 00/1 : 7[1e0] -> 2[190] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 01/1 : 6[1d0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 00/1 : 14[1d0] -> 9[180] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 01/1 : 7[1e0] -> 2[190] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 00/1 : 15[1e0] -> 10[190] via P2P/indirect/11[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 01/1 : 14[1d0] -> 9[180] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 01/1 : 15[1e0] -> 10[190] via P2P/indirect/11[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 00/1 : 5[1c0] -> 2[190] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 01/1 : 5[1c0] -> 2[190] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 00/1 : 6[1d0] -> 3[1a0] via P2P/indirect/2[190]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 00/1 : 13[1c0] -> 10[190] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 01/1 : 6[1d0] -> 3[1a0] via P2P/indirect/2[190]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 00/1 : 14[1d0] -> 11[1a0] via P2P/indirect/10[190]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 01/1 : 13[1c0] -> 10[190] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 01/1 : 14[1d0] -> 11[1a0] via P2P/indirect/10[190]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 00/1 : 4[1b0] -> 2[190] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 00/1 : 5[1c0] -> 3[1a0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 00/1 : 13[1c0] -> 11[1a0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 00/1 : 12[1b0] -> 10[190] via P2P/indirect/14[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 01/1 : 4[1b0] -> 2[190] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 00/1 : 4[1b0] -> 3[1a0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 01/1 : 12[1b0] -> 10[190] via P2P/indirect/14[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 01/1 : 5[1c0] -> 3[1a0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 01/1 : 4[1b0] -> 3[1a0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 01/1 : 13[1c0] -> 11[1a0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 00/1 : 12[1b0] -> 11[1a0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 01/1 : 12[1b0] -> 11[1a0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO comm 0x564daae52ce0 rank 4 nranks 16 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO comm 0x562d291e19b0 rank 6 nranks 16 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO comm 0x560d2c8bbeb0 rank 0 nranks 16 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO comm 0x55ab5031d6a0 rank 1 nranks 16 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO comm 0x55861bb410b0 rank 7 nranks 16 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO comm 0x558afe13a650 rank 5 nranks 16 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO comm 0x5596f3ec2650 rank 14 nranks 16 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO comm 0x55bd625e9f40 rank 8 nranks 16 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:NCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO comm 0x55f3ab496cf0 rank 12 nranks 16 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO comm 0x5577e7c03b40 rank 15 nranks 16 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO comm 0x55679bc636d0 rank 10 nranks 16 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO comm 0x55dbc5e01d10 rank 13 nranks 16 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO comm 0x55e04dae7720 rank 9 nranks 16 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO comm 0x55ddcfed3270 rank 11 nranks 16 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO comm 0x55e22ead3b50 rank 2 nranks 16 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO comm 0x55e5825511e0 rank 3 nranks 16 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Trees [0] 6/-1/-1->5->1 [1] 6/-1/-1->5->1 [2] 1/-1/-1->5->6 [3] 1/-1/-1->5->6 [4] 4/-1/-1->5->7 [5] 7/-1/-1->5->4 [6] 6/-1/-1->5->1 [7] 6/-1/-1->5->1 [8] 1/-1/-1->5->6 [9] 1/-1/-1->5->6 [10] 4/-1/-1->5->7 [11] 7/-1/-1->5->4\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Trees [0] 5/-1/-1->1->2 [1] 5/-1/-1->1->2 [2] 2/-1/-1->1->5 [3] 2/-1/-1->1->5 [4] 3/-1/-1->1->0 [5] -1/-1/-1->1->3 [6] 5/-1/-1->1->2 [7] 5/-1/-1->1->2 [8] 2/-1/-1->1->5 [9] 2/-1/-1->1->5 [10] 3/-1/-1->1->0 [11] -1/-1/-1->1->3\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1 [1] 3/-1/-1->0->-1 [2] 4/-1/-1->0->-1 [3] 4/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 2/-1/-1->0->-1 [6] 3/-1/-1->0->-1 [7] 3/-1/-1->0->-1 [8] 4/-1/-1->0->-1 [9] 4/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 2/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Trees [0] -1/-1/-1->4->7 [1] -1/-1/-1->4->7 [2] 7/-1/-1->4->0 [3] 7/-1/-1->4->0 [4] 6/-1/-1->4->5 [5] 5/-1/-1->4->6 [6] -1/-1/-1->4->7 [7] -1/-1/-1->4->7 [8] 7/-1/-1->4->0 [9] 7/-1/-1->4->0 [10] 6/-1/-1->4->5 [11] 5/-1/-1->4->6\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Trees [0] 1/-1/-1->2->3 [1] 1/-1/-1->2->3 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] -1/-1/-1->2->6 [5] 6/-1/-1->2->0 [6] 1/-1/-1->2->3 [7] 1/-1/-1->2->3 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] -1/-1/-1->2->6 [11] 6/-1/-1->2->0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 5/-1/-1->6->7 [3] 5/-1/-1->6->7 [4] 2/-1/-1->6->4 [5] 4/-1/-1->6->2 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 5/-1/-1->6->7 [9] 5/-1/-1->6->7 [10] 2/-1/-1->6->4 [11] 4/-1/-1->6->2\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Trees [0] 4/-1/-1->7->6 [1] 4/-1/-1->7->6 [2] 6/-1/-1->7->4 [3] 6/-1/-1->7->4 [4] 5/-1/-1->7->3 [5] 3/-1/-1->7->5 [6] 4/-1/-1->7->6 [7] 4/-1/-1->7->6 [8] 6/-1/-1->7->4 [9] 6/-1/-1->7->4 [10] 5/-1/-1->7->3 [11] 3/-1/-1->7->5\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Trees [0] 2/-1/-1->3->0 [1] 2/-1/-1->3->0 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] 7/-1/-1->3->1 [5] 1/-1/-1->3->7 [6] 2/-1/-1->3->0 [7] 2/-1/-1->3->0 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] 7/-1/-1->3->1 [11] 1/-1/-1->3->7\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 05/0 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 04/0 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 11/0 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 10/0 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 00/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 02/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 01/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 03/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 06/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 02/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 00/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 08/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 07/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 03/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 01/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 09/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 08/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 06/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 09/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 07/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 04/0 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 05/0 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 10/0 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 11/0 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 04/0 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 10/0 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 05/0 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 11/0 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 00/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 01/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 05/0 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 06/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 11/0 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 07/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 00/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 01/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 06/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 07/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Trees [0] 1/-1/-1->2->3 [1] 1/-1/-1->2->3 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] -1/-1/-1->2->6 [5] 6/-1/-1->2->0 [6] 1/-1/-1->2->3 [7] 1/-1/-1->2->3 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] -1/-1/-1->2->6 [11] 6/-1/-1->2->0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 5/-1/-1->6->7 [3] 5/-1/-1->6->7 [4] 2/-1/-1->6->4 [5] 4/-1/-1->6->2 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 5/-1/-1->6->7 [9] 5/-1/-1->6->7 [10] 2/-1/-1->6->4 [11] 4/-1/-1->6->2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Trees [0] 6/-1/-1->5->1 [1] 6/-1/-1->5->1 [2] 1/-1/-1->5->6 [3] 1/-1/-1->5->6 [4] 4/-1/-1->5->7 [5] 7/-1/-1->5->4 [6] 6/-1/-1->5->1 [7] 6/-1/-1->5->1 [8] 1/-1/-1->5->6 [9] 1/-1/-1->5->6 [10] 4/-1/-1->5->7 [11] 7/-1/-1->5->4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Trees [0] 4/-1/-1->7->6 [1] 4/-1/-1->7->6 [2] 6/-1/-1->7->4 [3] 6/-1/-1->7->4 [4] 5/-1/-1->7->3 [5] 3/-1/-1->7->5 [6] 4/-1/-1->7->6 [7] 4/-1/-1->7->6 [8] 6/-1/-1->7->4 [9] 6/-1/-1->7->4 [10] 5/-1/-1->7->3 [11] 3/-1/-1->7->5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Trees [0] -1/-1/-1->4->7 [1] -1/-1/-1->4->7 [2] 7/-1/-1->4->0 [3] 7/-1/-1->4->0 [4] 6/-1/-1->4->5 [5] 5/-1/-1->4->6 [6] -1/-1/-1->4->7 [7] -1/-1/-1->4->7 [8] 7/-1/-1->4->0 [9] 7/-1/-1->4->0 [10] 6/-1/-1->4->5 [11] 5/-1/-1->4->6\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Trees [0] 5/-1/-1->1->2 [1] 5/-1/-1->1->2 [2] 2/-1/-1->1->5 [3] 2/-1/-1->1->5 [4] 3/-1/-1->1->0 [5] -1/-1/-1->1->3 [6] 5/-1/-1->1->2 [7] 5/-1/-1->1->2 [8] 2/-1/-1->1->5 [9] 2/-1/-1->1->5 [10] 3/-1/-1->1->0 [11] -1/-1/-1->1->3\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Trees [0] 2/-1/-1->3->0 [1] 2/-1/-1->3->0 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] 7/-1/-1->3->1 [5] 1/-1/-1->3->7 [6] 2/-1/-1->3->0 [7] 2/-1/-1->3->0 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] 7/-1/-1->3->1 [11] 1/-1/-1->3->7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1 [1] 3/-1/-1->0->-1 [2] 4/-1/-1->0->-1 [3] 4/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 2/-1/-1->0->-1 [6] 3/-1/-1->0->-1 [7] 3/-1/-1->0->-1 [8] 4/-1/-1->0->-1 [9] 4/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 2/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 05/0 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 04/0 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 02/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 03/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 04/0 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 02/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 08/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 04/0 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 10/0 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 03/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 09/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 10/0 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 02/0 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 08/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 10/0 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 11/0 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 03/0 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 02/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 09/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 00/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 08/0 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 03/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 01/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 08/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 09/0 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 02/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 00/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 04/0 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 06/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 09/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 05/0 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 03/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 01/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 07/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 06/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 08/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 10/0 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 05/0 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 07/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 09/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 11/0 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 05/0 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 04/0 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 11/0 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 11/0 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 10/0 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 00/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 01/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 00/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 06/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 02/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 01/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 07/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 03/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 06/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 08/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 07/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 05/0 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 09/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 04/0 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 11/0 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 10/0 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 02/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 05/0 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 04/0 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 11/0 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 10/0 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 00/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 03/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 01/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 04/0 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 08/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 05/0 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 02/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 09/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 00/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 10/0 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 06/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 00/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 02/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 03/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 11/0 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 01/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 07/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 01/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 03/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 08/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 06/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 06/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 08/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 09/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 07/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 07/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 09/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 00/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 02/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 05/0 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 04/0 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 01/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 11/0 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 10/0 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 03/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 06/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 08/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 07/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 05/0 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 09/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 04/0 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 04/0 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 11/0 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 10/0 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 10/0 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 05/0 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 11/0 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 04/0 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 02/0 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 03/0 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 10/0 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 00/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 08/0 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 09/0 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 01/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 00/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 06/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 00/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 02/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 01/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 02/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 07/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 01/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 06/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 03/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 06/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 07/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 03/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 08/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 07/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 00/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 08/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 09/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 01/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 02/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 09/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 06/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 03/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 07/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 08/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 05/0 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 09/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 11/0 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 05/0 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 11/0 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 05/0 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 04/0 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 11/0 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 10/0 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 04/0 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 10/0 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 04/0 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 00/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 10/0 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 02/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 05/0 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 11/0 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 00/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 01/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 03/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 02/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 01/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 06/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 08/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 03/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 06/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 08/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 07/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 07/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 09/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 00/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 05/0 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 09/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 01/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 05/0 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 11/0 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 04/0 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 02/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 00/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 11/0 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 06/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 10/0 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 03/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 01/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 08/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 06/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 07/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 09/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 02/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 07/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 04/0 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 03/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 10/0 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 05/0 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 04/0 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 04/0 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 08/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 10/0 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 11/0 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 09/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 10/0 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 00/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 01/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 06/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 00/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 07/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 02/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 01/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 03/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 06/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 08/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 07/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 09/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 00/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 01/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 02/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 06/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 02/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 03/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 07/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 03/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 08/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 08/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 09/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 04/0 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 05/0 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 09/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 10/0 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 11/0 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 05/0 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 11/0 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 05/0 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 02/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 00/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 11/0 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 03/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 01/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 08/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 06/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 04/0 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 09/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 07/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 00/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 02/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 04/0 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 10/0 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 05/0 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 04/0 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 01/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 03/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 10/0 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 05/0 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 11/0 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 10/0 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 06/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 08/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 07/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 09/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 11/0 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 02/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 03/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 00/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 08/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 01/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 09/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 06/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 05/0 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 07/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 11/0 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 00/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 08/1 : 7[1e0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 01/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 08/1 : 3[1a0] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 06/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 02/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 07/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 04/0 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 04/1 : 2[190] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 03/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 04/1 : 6[1d0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 10/0 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 09/1 : 3[1a0] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 04/0 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 08/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 09/1 : 7[1e0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 10/0 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 09/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 05/0 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 04/1 : 3[1a0] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 05/1 : 2[190] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 11/0 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 05/1 : 6[1d0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 04/1 : 7[1e0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 00/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 02/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 05/1 : 3[1a0] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 03/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 05/1 : 7[1e0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 08/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 09/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 01/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 12/1 : 3[1a0] -> 6[1d0] via P2P/indirect/7[1e0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 06/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 07/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 12/1 : 7[1e0] -> 2[190] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO Channel 13/1 : 3[1a0] -> 6[1d0] via P2P/indirect/7[1e0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 12/1 : 5[1c0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 04/0 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 10/0 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 05/0 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 12/1 : 2[190] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO Channel 13/1 : 7[1e0] -> 2[190] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 11/0 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 12/1 : 6[1d0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 13/1 : 5[1c0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 12/1 : 1[180] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 02/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 10/1 : 0[170] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 13/1 : 2[190] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 11/1 : 0[170] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 13/1 : 6[1d0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 03/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 00/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 08/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 01/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 13/1 : 1[180] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 09/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 06/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 02/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 04/0 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 07/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 03/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 00/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 08/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 05/0 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 10/0 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 01/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 09/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 11/0 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 10/1 : 4[1b0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 06/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 07/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 10/1 : 5[1c0] -> 2[190] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 11/1 : 4[1b0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 11/1 : 5[1c0] -> 2[190] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 10/1 : 2[190] -> 7[1e0] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 08/1 : 3[1a0] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 10/1 : 1[180] -> 6[1d0] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 10/1 : 6[1d0] -> 3[1a0] via P2P/indirect/2[190]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO Channel 11/1 : 2[190] -> 7[1e0] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 11/1 : 1[180] -> 6[1d0] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 06/1 : 0[170] -> 6[1d0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO Channel 11/1 : 6[1d0] -> 3[1a0] via P2P/indirect/2[190]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 08/1 : 7[1e0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 07/1 : 0[170] -> 6[1d0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 04/1 : 2[190] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 09/1 : 3[1a0] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 04/1 : 6[1d0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 06/1 : 4[1b0] -> 2[190] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 09/1 : 7[1e0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 06/1 : 5[1c0] -> 3[1a0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 06/1 : 1[180] -> 7[1e0] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 05/1 : 2[190] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 04/1 : 3[1a0] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 04/1 : 7[1e0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 07/1 : 4[1b0] -> 2[190] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO Channel 07/1 : 5[1c0] -> 3[1a0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 05/1 : 6[1d0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO Channel 07/1 : 1[180] -> 7[1e0] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 05/1 : 3[1a0] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 05/1 : 7[1e0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 14/1 : 4[1b0] -> 3[1a0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 12/1 : 3[1a0] -> 6[1d0] via P2P/indirect/7[1e0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 14/1 : 0[170] -> 7[1e0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO Channel 15/1 : 4[1b0] -> 3[1a0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO Channel 15/1 : 0[170] -> 7[1e0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 12/1 : 7[1e0] -> 2[190] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO Channel 13/1 : 3[1a0] -> 6[1d0] via P2P/indirect/7[1e0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 12/1 : 6[1d0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 12/1 : 5[1c0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO Channel 13/1 : 7[1e0] -> 2[190] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 12/1 : 2[190] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 12/1 : 1[180] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 13/1 : 5[1c0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 13/1 : 6[1d0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 13/1 : 2[190] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 10/1 : 0[170] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 13/1 : 1[180] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 11/1 : 0[170] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 10/1 : 4[1b0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 11/1 : 4[1b0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 10/1 : 5[1c0] -> 2[190] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 10/1 : 2[190] -> 7[1e0] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 10/1 : 1[180] -> 6[1d0] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 10/1 : 6[1d0] -> 3[1a0] via P2P/indirect/2[190]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 11/1 : 5[1c0] -> 2[190] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO Channel 11/1 : 2[190] -> 7[1e0] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 11/1 : 1[180] -> 6[1d0] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO Channel 11/1 : 6[1d0] -> 3[1a0] via P2P/indirect/2[190]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 06/1 : 0[170] -> 6[1d0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 06/1 : 4[1b0] -> 2[190] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 07/1 : 0[170] -> 6[1d0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 06/1 : 5[1c0] -> 3[1a0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 06/1 : 1[180] -> 7[1e0] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 07/1 : 4[1b0] -> 2[190] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO Channel 07/1 : 5[1c0] -> 3[1a0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO Channel 07/1 : 1[180] -> 7[1e0] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:142:142 [4] NCCL INFO comm 0x55f3aba4dda0 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:144:144 [7] NCCL INFO comm 0x5577e93143e0 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:207:207 [2] NCCL INFO comm 0x55679cba1390 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:481:481 [0] NCCL INFO comm 0x55bd62ccd590 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:138:138 [1] NCCL INFO comm 0x55e04e2c4c30 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:147:147 [5] NCCL INFO comm 0x55dbc6674810 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:215:215 [6] NCCL INFO comm 0x5596f409c970 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:140:140 [3] NCCL INFO comm 0x55ddd00ad960 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 14/1 : 4[1b0] -> 3[1a0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 14/1 : 0[170] -> 7[1e0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO Channel 15/1 : 4[1b0] -> 3[1a0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO Channel 15/1 : 0[170] -> 7[1e0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:215:215 [6] NCCL INFO comm 0x562d29c7e490 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:204:204 [1] NCCL INFO comm 0x55ab50e00b20 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:202:202 [2] NCCL INFO comm 0x55e22ecae3a0 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:208:208 [5] NCCL INFO comm 0x558afe314d70 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:206:206 [3] NCCL INFO comm 0x55e584759420 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:210:210 [7] NCCL INFO comm 0x55861bd1b720 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:214:214 [4] NCCL INFO comm 0x564dac2f2d40 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:615:615 [0] NCCL INFO comm 0x560d2cc08bb0 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Running smdistributed.dataparallel v1.7.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:SMDDP: Multi node ENA mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:01/23/2024 03:37:27 - WARNING - __main__ - Process rank: 7, device: cuda:7, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:01/23/2024 03:37:27 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:01/23/2024 03:37:27 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:01/23/2024 03:37:27 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:01/23/2024 03:37:27 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:01/23/2024 03:37:27 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:01/23/2024 03:37:27 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:_n_gpu=1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:adafactor=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:adam_beta1=0.9,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:adam_beta2=0.999,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:adam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:auto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:bf16=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:bf16_full_eval=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:data_seed=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:dataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:dataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:dataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:ddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:ddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:ddp_timeout=1800,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:debug=[],\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:deepspeed=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:disable_tqdm=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:do_eval=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:do_predict=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:do_train=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:eval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:eval_delay=0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:eval_steps=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:evaluation_strategy=no,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:fp16=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:fp16_backend=auto,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:fp16_full_eval=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:fp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:fsdp=[],\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:fsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:fsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:full_determinism=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:gradient_accumulation_steps=1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:gradient_checkpointing=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:greater_is_better=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:group_by_length=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:half_precision_backend=auto,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:hub_model_id=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:hub_private_repo=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:hub_strategy=every_save,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:hub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:ignore_data_skip=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:include_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:jit_mode_eval=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:label_names=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:label_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:learning_rate=5e-05,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:length_column_name=length,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:load_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:local_rank=0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:log_level=passive,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:log_level_replica=passive,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:log_on_each_node=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:logging_dir=/opt/ml/model/runs/Jan23_03-37-19_algo-1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:logging_first_step=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:logging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:logging_steps=500,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:logging_strategy=steps,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:lr_scheduler_type=linear,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:max_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:max_steps=100,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:metric_for_best_model=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:mp_parameters=,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:no_cuda=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:num_train_epochs=2.0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:optim=adamw_hf,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:optim_args=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:output_dir=/opt/ml/model,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:overwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:past_index=-1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:per_device_eval_batch_size=4,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:per_device_train_batch_size=4,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:prediction_loss_only=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:push_to_hub=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:push_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:push_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:ray_scope=last,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:remove_unused_columns=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:report_to=[],\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:resume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:run_name=/opt/ml/model,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:save_on_each_node=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:save_steps=500,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:save_strategy=steps,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:save_total_limit=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:seed=42,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:sharded_ddp=[],\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:skip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:tf32=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:torch_compile=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:torch_compile_backend=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:torch_compile_mode=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:torchdynamo=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:tpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:tpu_num_cores=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:use_ipex=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:use_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:use_mps_device=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:warmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:warmup_steps=0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:weight_decay=0.0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:xpu_backend=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:)\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:01/23/2024 03:37:27 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:01/23/2024 03:37:27 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:01/23/2024 03:37:27 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:01/23/2024 03:37:27 - WARNING - __main__ - Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:01/23/2024 03:37:27 - WARNING - __main__ - Process rank: 7, device: cuda:7, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:01/23/2024 03:37:27 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:01/23/2024 03:37:27 - WARNING - __main__ - Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:01/23/2024 03:37:27 - WARNING - __main__ - Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:01/23/2024 03:37:27 - WARNING - __main__ - Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:01/23/2024 03:37:27 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading readme:   0%|          | 0.00/7.83k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading readme: 100%|██████████| 7.83k/7.83k [00:00<00:00, 9.96MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:01/23/2024 03:37:27 - WARNING - datasets.builder - Using custom data configuration plain_text-b38df7ca980d7b55\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading readme:   0%|          | 0.00/7.83k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:01/23/2024 03:37:27 - WARNING - datasets.builder - Using custom data configuration plain_text-b38df7ca980d7b55\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading readme: 100%|██████████| 7.83k/7.83k [00:00<00:00, 8.41MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:01/23/2024 03:37:28 - WARNING - datasets.builder - Using custom data configuration plain_text-b38df7ca980d7b55\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Downloading and preparing dataset None/plain_text to /root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:01/23/2024 03:37:28 - WARNING - datasets.builder - Using custom data configuration plain_text-b38df7ca980d7b55\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:01/23/2024 03:37:28 - WARNING - datasets.builder - Using custom data configuration plain_text-b38df7ca980d7b55\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:Downloading and preparing dataset None/plain_text to /root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:01/23/2024 03:37:28 - WARNING - datasets.builder - Using custom data configuration plain_text-b38df7ca980d7b55\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading readme:   0%|          | 0.00/7.83k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading readme: 100%|██████████| 7.83k/7.83k [00:00<00:00, 9.25MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Downloading readme:   0%|          | 0.00/7.83k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Downloading readme: 100%|██████████| 7.83k/7.83k [00:00<00:00, 7.48MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:01/23/2024 03:37:28 - WARNING - datasets.builder - Using custom data configuration plain_text-b38df7ca980d7b55\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:01/23/2024 03:37:28 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/squad/resolve/main/README.md not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpjoh0sn52\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:01/23/2024 03:37:28 - WARNING - datasets.builder - Using custom data configuration plain_text-b38df7ca980d7b55\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:01/23/2024 03:37:28 - WARNING - datasets.builder - Using custom data configuration plain_text-b38df7ca980d7b55\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s][1,mpirank:10,algo-2]<stderr>:#015Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Downloading readme:   0%|          | 0.00/7.83k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Downloading readme: 100%|██████████| 7.83k/7.83k [00:00<00:00, 10.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading readme:   0%|          | 0.00/7.83k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading readme: 100%|██████████| 7.83k/7.83k [00:00<00:00, 9.51MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:01/23/2024 03:37:28 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/squad/resolve/main/README.md in cache at /root/.cache/huggingface/datasets/downloads/94fe703883a1850055695505cee42dcb38fbdbecd11abd45ef317f8650ecd86e.e95954055d3d9b0b098ef6f88fca339fd035a2cd1befb983a1c0b41244b0b0cb\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:01/23/2024 03:37:28 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/94fe703883a1850055695505cee42dcb38fbdbecd11abd45ef317f8650ecd86e.e95954055d3d9b0b098ef6f88fca339fd035a2cd1befb983a1c0b41244b0b0cb\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:01/23/2024 03:37:28 - WARNING - datasets.builder - Using custom data configuration plain_text-b38df7ca980d7b55\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:01/23/2024 03:37:28 - WARNING - datasets.builder - Using custom data configuration plain_text-b38df7ca980d7b55\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:01/23/2024 03:37:28 - WARNING - datasets.builder - Using custom data configuration plain_text-b38df7ca980d7b55\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading readme:   0%|          | 0.00/7.83k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading readme: 100%|██████████| 7.83k/7.83k [00:00<00:00, 10.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading readme:   0%|          | 0.00/7.83k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading readme: 100%|██████████| 7.83k/7.83k [00:00<00:00, 6.85MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:01/23/2024 03:37:28 - WARNING - datasets.builder - Using custom data configuration plain_text-b38df7ca980d7b55\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:01/23/2024 03:37:28 - WARNING - datasets.builder - Using custom data configuration plain_text-b38df7ca980d7b55\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Downloading readme:   0%|          | 0.00/7.83k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Downloading readme: 100%|██████████| 7.83k/7.83k [00:00<00:00, 9.60MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:01/23/2024 03:37:28 - WARNING - datasets.builder - Using custom data configuration plain_text-b38df7ca980d7b55\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Downloading readme:   0%|          | 0.00/7.83k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Downloading readme: 100%|██████████| 7.83k/7.83k [00:00<00:00, 9.60MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:01/23/2024 03:37:28 - WARNING - datasets.builder - Using custom data configuration plain_text-b38df7ca980d7b55\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/14.5M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading data:   0%|          | 0.00/14.5M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading data:  47%|████▋     | 6.78M/14.5M [00:00<00:00, 67.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading data:  46%|████▌     | 6.63M/14.5M [00:00<00:00, 66.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading data: 100%|█████████▉| 14.4M/14.5M [00:00<00:00, 73.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading data: 100%|██████████| 14.5M/14.5M [00:00<00:00, 71.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading data files:  50%|█████     | 1/2 [00:00<00:00,  2.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading data:  97%|█████████▋| 14.1M/14.5M [00:00<00:00, 71.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading data: 100%|██████████| 14.5M/14.5M [00:00<00:00, 70.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading data files:  50%|█████     | 1/2 [00:00<00:00,  2.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/1.82M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading data:   0%|          | 0.00/1.82M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading data: 100%|██████████| 1.82M/1.82M [00:00<00:00, 67.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading data files: 100%|██████████| 2/2 [00:00<00:00,  3.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading data files: 100%|██████████| 2/2 [00:00<00:00,  2.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading data: 100%|██████████| 1.82M/1.82M [00:00<00:00, 64.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading data files: 100%|██████████| 2/2 [00:00<00:00,  3.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading data files: 100%|██████████| 2/2 [00:00<00:00,  2.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 1562.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 1529.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Generating train split:  80%|███████▉  | 70000/87599 [00:00<00:00, 584038.24 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Generating train split:  80%|███████▉  | 70000/87599 [00:00<00:00, 514744.09 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:01/23/2024 03:37:29 - WARNING - datasets.builder - Found cached dataset parquet (/root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 416.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:01/23/2024 03:37:29 - WARNING - datasets.builder - Found cached dataset parquet (/root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 429.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:01/23/2024 03:37:29 - WARNING - datasets.builder - Found cached dataset parquet (/root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:01/23/2024 03:37:29 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 378.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 407.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:01/23/2024 03:37:29 - WARNING - datasets.builder - Found cached dataset parquet (/root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:01/23/2024 03:37:29 - WARNING - datasets.builder - Found cached dataset parquet (/root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:01/23/2024 03:37:29 - WARNING - datasets.builder - Found cached dataset parquet (/root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 437.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 413.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 425.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 432.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:01/23/2024 03:37:29 - WARNING - datasets.builder - Found cached dataset parquet (/root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:01/23/2024 03:37:29 - WARNING - datasets.builder - Found cached dataset parquet (/root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s][1,mpirank:1,algo-1]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:01/23/2024 03:37:29 - WARNING - datasets.builder - Found cached dataset parquet (/root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 408.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 380.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:01/23/2024 03:37:29 - WARNING - datasets.builder - Found cached dataset parquet (/root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 401.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 419.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:01/23/2024 03:37:29 - WARNING - datasets.builder - Found cached dataset parquet (/root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:01/23/2024 03:37:29 - WARNING - datasets.builder - Found cached dataset parquet (/root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 418.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:01/23/2024 03:37:29 - WARNING - datasets.builder - Found cached dataset parquet (/root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 423.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 381.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:01/23/2024 03:37:29 - WARNING - datasets.builder - Found cached dataset parquet (/root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 404.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading (…)\"config.json\";:   0%|          | 0.00/434 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading (…)\"config.json\";: 100%|██████████| 434/434 [00:00<00:00, 70.5kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"config.json\";:   0%|          | 0.00/434 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"config.json\";: 100%|██████████| 434/434 [00:00<00:00, 59.9kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:660] 2024-01-23 03:37:29,191 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/8b35f05561d0d917087166b71d3c2c83a39104b1/config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:660] 2024-01-23 03:37:29,191 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/8b35f05561d0d917087166b71d3c2c83a39104b1/config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:712] 2024-01-23 03:37:29,195 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"_name_or_path\": \"bert-large-uncased-whole-word-masking\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"classifier_dropout\": null,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:712] 2024-01-23 03:37:29,195 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"_name_or_path\": \"bert-large-uncased-whole-word-masking\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"classifier_dropout\": null,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading (…)enizer_config.json\";:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading (…)enizer_config.json\";: 100%|██████████| 28.0/28.0 [00:00<00:00, 10.9kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)enizer_config.json\";:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)enizer_config.json\";: 100%|██████████| 28.0/28.0 [00:00<00:00, 12.9kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:660] 2024-01-23 03:37:29,326 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/8b35f05561d0d917087166b71d3c2c83a39104b1/config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:660] 2024-01-23 03:37:29,326 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/8b35f05561d0d917087166b71d3c2c83a39104b1/config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:712] 2024-01-23 03:37:29,326 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"_name_or_path\": \"bert-large-uncased-whole-word-masking\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"classifier_dropout\": null,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:712] 2024-01-23 03:37:29,326 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"_name_or_path\": \"bert-large-uncased-whole-word-masking\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"classifier_dropout\": null,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading (…)\"vocab.txt\";:   0%|          | 0.00/232k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading (…)\"vocab.txt\";: 100%|██████████| 232k/232k [00:00<00:00, 38.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"vocab.txt\";:   0%|          | 0.00/232k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"vocab.txt\";: 100%|██████████| 232k/232k [00:00<00:00, 39.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"tokenizer.json\";:   0%|          | 0.00/466k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"tokenizer.json\";: 100%|██████████| 466k/466k [00:00<00:00, 31.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/1.35G [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading (…)\"tokenizer.json\";:   0%|          | 0.00/466k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading (…)\"tokenizer.json\";: 100%|██████████| 466k/466k [00:00<00:00, 14.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|tokenization_utils_base.py:1802] 2024-01-23 03:37:30,287 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/8b35f05561d0d917087166b71d3c2c83a39104b1/vocab.txt\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|tokenization_utils_base.py:1802] 2024-01-23 03:37:30,287 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/8b35f05561d0d917087166b71d3c2c83a39104b1/tokenizer.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|tokenization_utils_base.py:1802] 2024-01-23 03:37:30,287 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|tokenization_utils_base.py:1802] 2024-01-23 03:37:30,287 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/8b35f05561d0d917087166b71d3c2c83a39104b1/vocab.txt\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|tokenization_utils_base.py:1802] 2024-01-23 03:37:30,287 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/8b35f05561d0d917087166b71d3c2c83a39104b1/tokenizer.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|tokenization_utils_base.py:1802] 2024-01-23 03:37:30,287 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|tokenization_utils_base.py:1802] 2024-01-23 03:37:30,287 >> loading file special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|tokenization_utils_base.py:1802] 2024-01-23 03:37:30,287 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/8b35f05561d0d917087166b71d3c2c83a39104b1/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|tokenization_utils_base.py:1802] 2024-01-23 03:37:30,287 >> loading file special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|tokenization_utils_base.py:1802] 2024-01-23 03:37:30,287 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/8b35f05561d0d917087166b71d3c2c83a39104b1/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:660] 2024-01-23 03:37:30,288 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/8b35f05561d0d917087166b71d3c2c83a39104b1/config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:660] 2024-01-23 03:37:30,288 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/8b35f05561d0d917087166b71d3c2c83a39104b1/config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:712] 2024-01-23 03:37:30,288 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"_name_or_path\": \"bert-large-uncased-whole-word-masking\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"classifier_dropout\": null,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:712] 2024-01-23 03:37:30,288 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"_name_or_path\": \"bert-large-uncased-whole-word-masking\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"classifier_dropout\": null,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   1%|          | 10.5M/1.35G [00:00<00:51, 25.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/1.35G [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   2%|▏         | 21.0M/1.35G [00:00<00:41, 31.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   2%|▏         | 21.0M/1.35G [00:00<00:11, 113MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   2%|▏         | 31.5M/1.35G [00:01<00:47, 27.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   3%|▎         | 41.9M/1.35G [00:00<00:31, 41.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   3%|▎         | 41.9M/1.35G [00:01<00:43, 29.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   4%|▍         | 52.4M/1.35G [00:01<00:37, 34.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   4%|▍         | 52.4M/1.35G [00:01<00:30, 42.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   5%|▍         | 62.9M/1.35G [00:01<00:35, 35.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   5%|▍         | 62.9M/1.35G [00:01<00:32, 39.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   5%|▌         | 73.4M/1.35G [00:02<00:33, 38.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   5%|▌         | 73.4M/1.35G [00:01<00:30, 41.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   6%|▌         | 83.9M/1.35G [00:02<00:41, 30.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   6%|▌         | 83.9M/1.35G [00:02<00:45, 27.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   7%|▋         | 94.4M/1.35G [00:02<00:42, 29.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   7%|▋         | 94.4M/1.35G [00:03<00:44, 27.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   8%|▊         | 105M/1.35G [00:03<00:44, 27.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   8%|▊         | 105M/1.35G [00:03<00:43, 28.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   9%|▊         | 115M/1.35G [00:03<00:35, 34.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   9%|▊         | 115M/1.35G [00:03<00:39, 31.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   9%|▉         | 126M/1.35G [00:03<00:39, 30.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   9%|▉         | 126M/1.35G [00:04<00:41, 29.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  10%|█         | 136M/1.35G [00:03<00:33, 35.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  10%|█         | 136M/1.35G [00:04<00:34, 34.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  11%|█         | 147M/1.35G [00:04<00:30, 38.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  11%|█         | 147M/1.35G [00:04<00:32, 37.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  12%|█▏        | 157M/1.35G [00:04<00:34, 34.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  12%|█▏        | 157M/1.35G [00:04<00:36, 32.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  12%|█▏        | 168M/1.35G [00:04<00:39, 29.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  12%|█▏        | 168M/1.35G [00:05<00:42, 27.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  13%|█▎        | 178M/1.35G [00:05<00:33, 34.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  13%|█▎        | 178M/1.35G [00:05<00:36, 32.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  14%|█▍        | 189M/1.35G [00:05<00:30, 37.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  14%|█▍        | 189M/1.35G [00:05<00:29, 39.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  15%|█▍        | 199M/1.35G [00:05<00:32, 35.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  15%|█▍        | 199M/1.35G [00:06<00:31, 36.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  16%|█▌        | 210M/1.35G [00:06<00:39, 28.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  16%|█▌        | 210M/1.35G [00:06<00:40, 28.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  16%|█▋        | 220M/1.35G [00:07<00:42, 26.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  16%|█▋        | 220M/1.35G [00:06<00:43, 25.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  17%|█▋        | 231M/1.35G [00:07<00:41, 26.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  17%|█▋        | 231M/1.35G [00:07<00:41, 26.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  18%|█▊        | 241M/1.35G [00:07<00:38, 28.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  18%|█▊        | 241M/1.35G [00:07<00:39, 27.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  19%|█▊        | 252M/1.35G [00:07<00:41, 26.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  19%|█▊        | 252M/1.35G [00:08<00:41, 26.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  19%|█▉        | 262M/1.35G [00:08<00:41, 26.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  19%|█▉        | 262M/1.35G [00:08<00:41, 26.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  20%|██        | 273M/1.35G [00:08<00:36, 29.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  20%|██        | 273M/1.35G [00:08<00:38, 28.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  21%|██        | 283M/1.35G [00:09<00:31, 33.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  21%|██        | 283M/1.35G [00:08<00:31, 33.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  22%|██▏       | 294M/1.35G [00:09<00:32, 32.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  22%|██▏       | 294M/1.35G [00:09<00:33, 31.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  23%|██▎       | 304M/1.35G [00:09<00:32, 31.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  23%|██▎       | 304M/1.35G [00:09<00:33, 31.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  23%|██▎       | 315M/1.35G [00:09<00:35, 29.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  23%|██▎       | 315M/1.35G [00:10<00:35, 28.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  24%|██▍       | 325M/1.35G [00:10<00:33, 30.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  24%|██▍       | 325M/1.35G [00:10<00:37, 27.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  25%|██▍       | 336M/1.35G [00:10<00:38, 26.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  25%|██▍       | 336M/1.35G [00:11<00:40, 25.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  26%|██▌       | 346M/1.35G [00:10<00:31, 31.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  26%|██▌       | 346M/1.35G [00:11<00:33, 30.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  27%|██▋       | 357M/1.35G [00:11<00:30, 32.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  27%|██▋       | 357M/1.35G [00:11<00:30, 31.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  27%|██▋       | 367M/1.35G [00:12<00:33, 29.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  27%|██▋       | 367M/1.35G [00:11<00:34, 28.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  28%|██▊       | 377M/1.35G [00:12<00:36, 26.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  28%|██▊       | 377M/1.35G [00:12<00:36, 26.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  29%|██▉       | 388M/1.35G [00:12<00:33, 28.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  29%|██▉       | 388M/1.35G [00:12<00:34, 27.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  30%|██▉       | 398M/1.35G [00:12<00:35, 26.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  30%|██▉       | 398M/1.35G [00:13<00:35, 26.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  30%|███       | 409M/1.35G [00:13<00:30, 30.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  30%|███       | 409M/1.35G [00:13<00:31, 30.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  31%|███       | 419M/1.35G [00:14<00:34, 26.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  31%|███       | 419M/1.35G [00:13<00:34, 26.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  32%|███▏      | 430M/1.35G [00:13<00:32, 28.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  32%|███▏      | 430M/1.35G [00:14<00:32, 28.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  33%|███▎      | 440M/1.35G [00:14<00:27, 33.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  33%|███▎      | 440M/1.35G [00:14<00:27, 33.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  34%|███▎      | 451M/1.35G [00:14<00:22, 39.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  34%|███▎      | 451M/1.35G [00:14<00:26, 33.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  34%|███▍      | 461M/1.35G [00:14<00:27, 31.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  34%|███▍      | 461M/1.35G [00:15<00:29, 29.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  35%|███▌      | 472M/1.35G [00:15<00:25, 33.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  35%|███▌      | 472M/1.35G [00:15<00:26, 32.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  36%|███▌      | 482M/1.35G [00:15<00:23, 35.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  36%|███▌      | 482M/1.35G [00:15<00:23, 36.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  37%|███▋      | 493M/1.35G [00:15<00:26, 32.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  37%|███▋      | 493M/1.35G [00:16<00:27, 31.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  37%|███▋      | 503M/1.35G [00:16<00:30, 27.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  37%|███▋      | 503M/1.35G [00:16<00:31, 26.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  38%|███▊      | 514M/1.35G [00:16<00:27, 30.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  38%|███▊      | 514M/1.35G [00:16<00:27, 30.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  39%|███▉      | 524M/1.35G [00:16<00:26, 31.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  39%|███▉      | 524M/1.35G [00:17<00:26, 30.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  40%|███▉      | 535M/1.35G [00:17<00:22, 36.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  40%|███▉      | 535M/1.35G [00:16<00:22, 35.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  41%|████      | 545M/1.35G [00:17<00:28, 27.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  41%|████      | 545M/1.35G [00:18<00:28, 28.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  41%|████▏     | 556M/1.35G [00:18<00:26, 30.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  41%|████▏     | 556M/1.35G [00:17<00:27, 28.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  42%|████▏     | 566M/1.35G [00:18<00:23, 32.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  42%|████▏     | 566M/1.35G [00:18<00:25, 31.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  43%|████▎     | 577M/1.35G [00:19<00:25, 30.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  43%|████▎     | 577M/1.35G [00:18<00:26, 29.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  44%|████▎     | 587M/1.35G [00:19<00:29, 26.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  44%|████▎     | 587M/1.35G [00:19<00:30, 24.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  44%|████▍     | 598M/1.35G [00:19<00:27, 26.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  44%|████▍     | 598M/1.35G [00:19<00:28, 26.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  45%|████▌     | 608M/1.35G [00:19<00:25, 28.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  45%|████▌     | 608M/1.35G [00:20<00:25, 29.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  46%|████▌     | 619M/1.35G [00:20<00:21, 34.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  46%|████▌     | 619M/1.35G [00:19<00:22, 32.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  47%|████▋     | 629M/1.35G [00:20<00:22, 31.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  47%|████▋     | 629M/1.35G [00:20<00:23, 30.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  48%|████▊     | 640M/1.35G [00:21<00:21, 32.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  48%|████▊     | 640M/1.35G [00:20<00:22, 31.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  48%|████▊     | 650M/1.35G [00:21<00:19, 35.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  48%|████▊     | 650M/1.35G [00:20<00:20, 33.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  49%|████▉     | 661M/1.35G [00:21<00:21, 32.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  49%|████▉     | 661M/1.35G [00:21<00:21, 31.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  50%|████▉     | 671M/1.35G [00:22<00:22, 29.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  50%|████▉     | 671M/1.35G [00:21<00:22, 29.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  51%|█████     | 682M/1.35G [00:22<00:19, 34.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  51%|█████     | 682M/1.35G [00:21<00:19, 33.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  51%|█████▏    | 692M/1.35G [00:22<00:18, 35.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  51%|█████▏    | 692M/1.35G [00:22<00:18, 35.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  52%|█████▏    | 703M/1.35G [00:22<00:17, 37.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  52%|█████▏    | 703M/1.35G [00:22<00:17, 36.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  53%|█████▎    | 713M/1.35G [00:22<00:21, 28.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  53%|█████▎    | 713M/1.35G [00:23<00:22, 28.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  54%|█████▍    | 724M/1.35G [00:23<00:19, 32.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  54%|█████▍    | 724M/1.35G [00:23<00:19, 32.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  55%|█████▍    | 734M/1.35G [00:23<00:17, 35.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  55%|█████▍    | 734M/1.35G [00:23<00:17, 35.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  55%|█████▌    | 744M/1.35G [00:24<00:16, 36.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  55%|█████▌    | 744M/1.35G [00:23<00:16, 36.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  56%|█████▌    | 755M/1.35G [00:24<00:21, 28.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  56%|█████▌    | 755M/1.35G [00:24<00:21, 27.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  57%|█████▋    | 765M/1.35G [00:25<00:19, 29.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  57%|█████▋    | 765M/1.35G [00:24<00:19, 29.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  58%|█████▊    | 776M/1.35G [00:25<00:17, 32.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  58%|█████▊    | 776M/1.35G [00:24<00:20, 28.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  58%|█████▊    | 786M/1.35G [00:25<00:18, 30.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  58%|█████▊    | 786M/1.35G [00:25<00:18, 30.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  59%|█████▉    | 797M/1.35G [00:26<00:21, 25.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  59%|█████▉    | 797M/1.35G [00:25<00:21, 26.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  60%|██████    | 807M/1.35G [00:25<00:17, 31.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  60%|██████    | 807M/1.35G [00:26<00:18, 29.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  61%|██████    | 818M/1.35G [00:26<00:16, 32.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  61%|██████    | 818M/1.35G [00:26<00:16, 31.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  62%|██████▏   | 828M/1.35G [00:27<00:14, 34.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  62%|██████▏   | 828M/1.35G [00:26<00:15, 33.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  62%|██████▏   | 839M/1.35G [00:27<00:17, 28.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  62%|██████▏   | 839M/1.35G [00:27<00:17, 28.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  63%|██████▎   | 849M/1.35G [00:27<00:17, 27.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  63%|██████▎   | 849M/1.35G [00:27<00:17, 28.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  64%|██████▍   | 860M/1.35G [00:28<00:16, 28.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  64%|██████▍   | 860M/1.35G [00:27<00:17, 28.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  65%|██████▍   | 870M/1.35G [00:28<00:15, 30.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  65%|██████▍   | 870M/1.35G [00:28<00:15, 30.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  65%|██████▌   | 881M/1.35G [00:28<00:17, 26.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  65%|██████▌   | 881M/1.35G [00:29<00:18, 25.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  66%|██████▋   | 891M/1.35G [00:28<00:14, 31.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  66%|██████▋   | 891M/1.35G [00:29<00:14, 31.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  67%|██████▋   | 902M/1.35G [00:29<00:12, 34.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  67%|██████▋   | 902M/1.35G [00:29<00:13, 33.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  68%|██████▊   | 912M/1.35G [00:29<00:13, 31.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  68%|██████▊   | 912M/1.35G [00:29<00:13, 31.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  69%|██████▊   | 923M/1.35G [00:29<00:15, 27.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  69%|██████▊   | 923M/1.35G [00:30<00:16, 24.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  69%|██████▉   | 933M/1.35G [00:30<00:13, 31.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  69%|██████▉   | 933M/1.35G [00:30<00:14, 29.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  70%|███████   | 944M/1.35G [00:31<00:12, 32.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  70%|███████   | 944M/1.35G [00:30<00:13, 30.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  71%|███████   | 954M/1.35G [00:31<00:11, 35.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  71%|███████   | 954M/1.35G [00:30<00:11, 33.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  72%|███████▏  | 965M/1.35G [00:31<00:14, 26.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  72%|███████▏  | 965M/1.35G [00:31<00:15, 24.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  73%|███████▎  | 975M/1.35G [00:31<00:12, 30.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  73%|███████▎  | 975M/1.35G [00:32<00:12, 29.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  73%|███████▎  | 986M/1.35G [00:31<00:10, 33.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  73%|███████▎  | 986M/1.35G [00:32<00:10, 34.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  74%|███████▍  | 996M/1.35G [00:32<00:10, 34.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  74%|███████▍  | 996M/1.35G [00:32<00:10, 34.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  75%|███████▍  | 1.01G/1.35G [00:32<00:11, 28.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  75%|███████▍  | 1.01G/1.35G [00:33<00:11, 28.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  76%|███████▌  | 1.02G/1.35G [00:32<00:10, 31.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  76%|███████▌  | 1.02G/1.35G [00:33<00:10, 31.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  76%|███████▋  | 1.03G/1.35G [00:33<00:10, 29.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  76%|███████▋  | 1.03G/1.35G [00:33<00:10, 29.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  77%|███████▋  | 1.04G/1.35G [00:34<00:09, 31.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  77%|███████▋  | 1.04G/1.35G [00:33<00:10, 30.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  78%|███████▊  | 1.05G/1.35G [00:34<00:11, 25.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  78%|███████▊  | 1.05G/1.35G [00:34<00:11, 25.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  79%|███████▊  | 1.06G/1.35G [00:34<00:11, 25.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  79%|███████▊  | 1.06G/1.35G [00:35<00:11, 25.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  80%|███████▉  | 1.07G/1.35G [00:35<00:09, 29.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  80%|███████▉  | 1.07G/1.35G [00:34<00:10, 26.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  80%|████████  | 1.08G/1.35G [00:35<00:07, 33.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  80%|████████  | 1.08G/1.35G [00:35<00:07, 34.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  81%|████████  | 1.09G/1.35G [00:36<00:09, 27.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  81%|████████  | 1.09G/1.35G [00:35<00:09, 27.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  82%|████████▏ | 1.10G/1.35G [00:36<00:08, 28.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  82%|████████▏ | 1.10G/1.35G [00:35<00:08, 28.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  83%|████████▎ | 1.11G/1.35G [00:36<00:07, 30.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  83%|████████▎ | 1.11G/1.35G [00:36<00:07, 29.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  83%|████████▎ | 1.12G/1.35G [00:36<00:07, 29.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  83%|████████▎ | 1.12G/1.35G [00:37<00:07, 29.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  84%|████████▍ | 1.13G/1.35G [00:37<00:10, 20.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  84%|████████▍ | 1.13G/1.35G [00:37<00:10, 20.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  85%|████████▍ | 1.14G/1.35G [00:38<00:08, 24.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  85%|████████▍ | 1.14G/1.35G [00:37<00:08, 24.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  86%|████████▌ | 1.15G/1.35G [00:38<00:07, 27.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  86%|████████▌ | 1.15G/1.35G [00:38<00:07, 26.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  87%|████████▋ | 1.16G/1.35G [00:38<00:05, 31.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  87%|████████▋ | 1.16G/1.35G [00:38<00:05, 30.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  87%|████████▋ | 1.17G/1.35G [00:39<00:07, 22.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  87%|████████▋ | 1.17G/1.35G [00:39<00:08, 21.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  88%|████████▊ | 1.18G/1.35G [00:39<00:06, 26.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  88%|████████▊ | 1.18G/1.35G [00:39<00:06, 25.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  89%|████████▉ | 1.20G/1.35G [00:39<00:05, 28.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  89%|████████▉ | 1.20G/1.35G [00:40<00:05, 29.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  90%|████████▉ | 1.21G/1.35G [00:40<00:04, 29.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  90%|████████▉ | 1.21G/1.35G [00:39<00:04, 28.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  90%|█████████ | 1.22G/1.35G [00:40<00:05, 24.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  90%|█████████ | 1.22G/1.35G [00:40<00:05, 24.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  91%|█████████ | 1.23G/1.35G [00:41<00:04, 27.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  91%|█████████ | 1.23G/1.35G [00:40<00:04, 26.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  92%|█████████▏| 1.24G/1.35G [00:41<00:03, 29.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  92%|█████████▏| 1.24G/1.35G [00:41<00:03, 29.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  93%|█████████▎| 1.25G/1.35G [00:41<00:03, 29.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  93%|█████████▎| 1.25G/1.35G [00:41<00:03, 28.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  94%|█████████▎| 1.26G/1.35G [00:42<00:03, 24.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  94%|█████████▎| 1.26G/1.35G [00:42<00:03, 24.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  94%|█████████▍| 1.27G/1.35G [00:42<00:02, 29.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  94%|█████████▍| 1.27G/1.35G [00:42<00:02, 29.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  95%|█████████▌| 1.28G/1.35G [00:42<00:02, 32.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  95%|█████████▌| 1.28G/1.35G [00:42<00:02, 31.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  96%|█████████▌| 1.29G/1.35G [00:43<00:01, 37.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  96%|█████████▌| 1.29G/1.35G [00:42<00:01, 36.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  97%|█████████▋| 1.30G/1.35G [00:43<00:01, 31.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  97%|█████████▋| 1.30G/1.35G [00:43<00:01, 30.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  97%|█████████▋| 1.31G/1.35G [00:43<00:01, 31.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  97%|█████████▋| 1.31G/1.35G [00:43<00:01, 30.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  98%|█████████▊| 1.32G/1.35G [00:44<00:00, 33.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  98%|█████████▊| 1.32G/1.35G [00:43<00:00, 30.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  99%|█████████▉| 1.33G/1.35G [00:44<00:00, 40.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  99%|█████████▉| 1.33G/1.35G [00:44<00:00, 34.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";: 100%|█████████▉| 1.34G/1.35G [00:44<00:00, 32.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";: 100%|█████████▉| 1.34G/1.35G [00:44<00:00, 34.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 1.35G/1.35G [00:44<00:00, 30.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 1.35G/1.35G [00:44<00:00, 30.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|modeling_utils.py:2275] 2024-01-23 03:38:14,961 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/8b35f05561d0d917087166b71d3c2c83a39104b1/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|modeling_utils.py:2275] 2024-01-23 03:38:14,961 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/8b35f05561d0d917087166b71d3c2c83a39104b1/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,434 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,434 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,434 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,434 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,484 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,485 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,484 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,485 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,561 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,561 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,561 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,561 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,572 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,572 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,573 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,573 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,575 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,575 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,575 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,575 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,579 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,579 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,580 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,580 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,581 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,581 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,581 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,581 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,608 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,608 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,608 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,608 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,611 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,611 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,611 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,611 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/88 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,619 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,619 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,619 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,619 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,635 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,635 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,635 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,635 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,639 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,639 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,639 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,639 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,674 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,674 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,674 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,674 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,699 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,699 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,699 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,699 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,728 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,728 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,728 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,728 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:01/23/2024 03:38:20 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-56590649d2be20b0.arrow\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,870 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[WARNING|modeling_utils.py:2847] 2024-01-23 03:38:20,870 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,870 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:[WARNING|modeling_utils.py:2859] 2024-01-23 03:38:20,870 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:   1%|          | 1/88 [00:00<00:32,  2.67ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:   2%|▏         | 2/88 [00:00<00:30,  2.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:   3%|▎         | 3/88 [00:01<00:30,  2.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:   5%|▍         | 4/88 [00:01<00:28,  2.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:   6%|▌         | 5/88 [00:01<00:27,  2.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 6/88 [00:02<00:30,  2.70ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:   8%|▊         | 7/88 [00:02<00:29,  2.74ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:   9%|▉         | 8/88 [00:02<00:27,  2.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  10%|█         | 9/88 [00:03<00:27,  2.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  11%|█▏        | 10/88 [00:03<00:26,  2.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  12%|█▎        | 11/88 [00:03<00:26,  2.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  14%|█▎        | 12/88 [00:04<00:28,  2.69ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  15%|█▍        | 13/88 [00:04<00:26,  2.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  16%|█▌        | 14/88 [00:04<00:26,  2.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  17%|█▋        | 15/88 [00:05<00:25,  2.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  18%|█▊        | 16/88 [00:05<00:23,  3.03ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  19%|█▉        | 17/88 [00:05<00:23,  3.04ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 18/88 [00:06<00:22,  3.10ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  22%|██▏       | 19/88 [00:06<00:24,  2.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  23%|██▎       | 20/88 [00:06<00:23,  2.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  24%|██▍       | 21/88 [00:07<00:22,  2.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  25%|██▌       | 22/88 [00:07<00:21,  3.04ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  26%|██▌       | 23/88 [00:07<00:21,  3.03ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 24/88 [00:08<00:20,  3.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  28%|██▊       | 25/88 [00:08<00:22,  2.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  30%|██▉       | 26/88 [00:09<00:21,  2.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  31%|███       | 27/88 [00:09<00:20,  2.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  32%|███▏      | 28/88 [00:09<00:20,  2.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 29/88 [00:10<00:20,  2.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  34%|███▍      | 30/88 [00:10<00:20,  2.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  35%|███▌      | 31/88 [00:10<00:22,  2.57ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  36%|███▋      | 32/88 [00:11<00:20,  2.69ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  38%|███▊      | 33/88 [00:11<00:19,  2.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  39%|███▊      | 34/88 [00:11<00:19,  2.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  40%|███▉      | 35/88 [00:12<00:18,  2.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  41%|████      | 36/88 [00:12<00:18,  2.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  42%|████▏     | 37/88 [00:13<00:19,  2.62ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  43%|████▎     | 38/88 [00:13<00:18,  2.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  44%|████▍     | 39/88 [00:13<00:17,  2.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  45%|████▌     | 40/88 [00:14<00:16,  2.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 41/88 [00:14<00:16,  2.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  48%|████▊     | 42/88 [00:14<00:15,  2.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  49%|████▉     | 43/88 [00:15<00:16,  2.68ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  50%|█████     | 44/88 [00:15<00:15,  2.75ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  51%|█████     | 45/88 [00:15<00:15,  2.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  52%|█████▏    | 46/88 [00:16<00:14,  2.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 47/88 [00:16<00:14,  2.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  55%|█████▍    | 48/88 [00:16<00:13,  2.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  56%|█████▌    | 49/88 [00:17<00:13,  2.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  57%|█████▋    | 50/88 [00:17<00:14,  2.64ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  58%|█████▊    | 51/88 [00:17<00:13,  2.75ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  59%|█████▉    | 52/88 [00:18<00:12,  2.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 53/88 [00:18<00:12,  2.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  61%|██████▏   | 54/88 [00:18<00:11,  2.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  62%|██████▎   | 55/88 [00:19<00:11,  2.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  64%|██████▎   | 56/88 [00:19<00:12,  2.65ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  65%|██████▍   | 57/88 [00:20<00:11,  2.74ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  66%|██████▌   | 58/88 [00:20<00:10,  2.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 59/88 [00:20<00:10,  2.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  68%|██████▊   | 60/88 [00:21<00:09,  2.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  69%|██████▉   | 61/88 [00:21<00:09,  2.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  70%|███████   | 62/88 [00:21<00:09,  2.70ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  72%|███████▏  | 63/88 [00:22<00:08,  2.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 64/88 [00:22<00:08,  2.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  74%|███████▍  | 65/88 [00:22<00:08,  2.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  75%|███████▌  | 66/88 [00:23<00:07,  2.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  76%|███████▌  | 67/88 [00:23<00:07,  2.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  77%|███████▋  | 68/88 [00:24<00:07,  2.54ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  78%|███████▊  | 69/88 [00:24<00:07,  2.65ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  80%|███████▉  | 70/88 [00:24<00:06,  2.75ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  81%|████████  | 71/88 [00:25<00:06,  2.74ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  82%|████████▏ | 72/88 [00:25<00:05,  2.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  83%|████████▎ | 73/88 [00:25<00:05,  2.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  84%|████████▍ | 74/88 [00:26<00:05,  2.64ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  85%|████████▌ | 75/88 [00:26<00:04,  2.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  86%|████████▋ | 76/88 [00:26<00:04,  2.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  88%|████████▊ | 77/88 [00:27<00:03,  2.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  89%|████████▊ | 78/88 [00:27<00:03,  2.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  90%|████████▉ | 79/88 [00:27<00:03,  2.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  91%|█████████ | 80/88 [00:28<00:02,  2.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  92%|█████████▏| 81/88 [00:28<00:02,  2.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 82/88 [00:28<00:02,  2.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  94%|█████████▍| 83/88 [00:29<00:01,  2.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  95%|█████████▌| 84/88 [00:29<00:01,  2.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  97%|█████████▋| 85/88 [00:30<00:01,  2.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  98%|█████████▊| 86/88 [00:30<00:00,  2.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  99%|█████████▉| 87/88 [00:30<00:00,  2.70ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:30<00:00,  3.14ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:30<00:00,  2.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:01/23/2024 03:38:51 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-56590649d2be20b0.arrow\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:01/23/2024 03:38:51 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-56590649d2be20b0.arrow\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:01/23/2024 03:38:51 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-56590649d2be20b0.arrow\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:01/23/2024 03:38:51 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-56590649d2be20b0.arrow\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:01/23/2024 03:38:51 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-56590649d2be20b0.arrow\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/88 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/88 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/88 [00:00<?, ?ba/s][1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/88 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:01/23/2024 03:38:51 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-56590649d2be20b0.arrow\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/88 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:01/23/2024 03:38:51 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-56590649d2be20b0.arrow\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/88 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/88 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/88 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:01/23/2024 03:38:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/plain_text-b38df7ca980d7b55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-3d46bed77060f58d.arrow\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:00<00:04,  2.48ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:   1%|          | 1/88 [00:00<00:50,  1.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:   1%|          | 1/88 [00:00<00:52,  1.65ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:   1%|          | 1/88 [00:00<01:01,  1.40ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:   1%|          | 1/88 [00:00<01:04,  1.36ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:00<00:03,  2.50ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:   1%|          | 1/88 [00:00<01:13,  1.19ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:   1%|          | 1/88 [00:00<01:15,  1.15ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:   1%|          | 1/88 [00:00<01:23,  1.04ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:   1%|          | 1/88 [00:00<01:23,  1.04ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:   2%|▏         | 2/88 [00:01<00:45,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:   2%|▏         | 2/88 [00:01<00:48,  1.76ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:01<00:03,  2.45ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:   2%|▏         | 2/88 [00:01<00:51,  1.66ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:   2%|▏         | 2/88 [00:01<00:54,  1.59ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:   2%|▏         | 2/88 [00:01<00:56,  1.53ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:   2%|▏         | 2/88 [00:01<00:58,  1.47ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:   2%|▏         | 2/88 [00:01<00:59,  1.45ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:   2%|▏         | 2/88 [00:01<01:02,  1.39ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:   3%|▎         | 3/88 [00:01<00:42,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:   3%|▎         | 3/88 [00:01<00:44,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:01<00:02,  2.43ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:   3%|▎         | 3/88 [00:01<00:45,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:   3%|▎         | 3/88 [00:01<00:47,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:   3%|▎         | 3/88 [00:01<00:49,  1.73ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:   3%|▎         | 3/88 [00:01<00:49,  1.73ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:   3%|▎         | 3/88 [00:01<00:49,  1.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:   3%|▎         | 3/88 [00:01<00:51,  1.65ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:   5%|▍         | 4/88 [00:02<00:40,  2.07ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:   5%|▍         | 4/88 [00:02<00:41,  2.04ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:02<00:02,  2.22ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:   5%|▍         | 4/88 [00:02<00:42,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:   5%|▍         | 4/88 [00:02<00:42,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:   5%|▍         | 4/88 [00:02<00:45,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:   5%|▍         | 4/88 [00:02<00:48,  1.74ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:   5%|▍         | 4/88 [00:02<00:47,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:   6%|▌         | 5/88 [00:02<00:38,  2.14ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:   5%|▍         | 4/88 [00:02<00:47,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:   6%|▌         | 5/88 [00:02<00:42,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:   6%|▌         | 5/88 [00:02<00:41,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:   6%|▌         | 5/88 [00:02<00:41,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:02<00:02,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:   6%|▌         | 5/88 [00:02<00:43,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:   6%|▌         | 5/88 [00:02<00:44,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:   6%|▌         | 5/88 [00:02<00:44,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:   6%|▌         | 5/88 [00:02<00:44,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 6/88 [00:03<00:43,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:03<00:01,  2.10ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 6/88 [00:03<00:45,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 6/88 [00:03<00:43,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 6/88 [00:03<00:44,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 6/88 [00:03<00:47,  1.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 6/88 [00:03<00:46,  1.76ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 6/88 [00:03<00:45,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:   8%|▊         | 7/88 [00:03<00:39,  2.04ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:03<00:01,  2.15ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 6/88 [00:03<00:47,  1.74ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:   8%|▊         | 7/88 [00:03<00:42,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:   8%|▊         | 7/88 [00:03<00:40,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:   8%|▊         | 7/88 [00:03<00:40,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:   8%|▊         | 7/88 [00:03<00:41,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:   8%|▊         | 7/88 [00:03<00:43,  1.84ba/s][1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:   8%|▊         | 7/88 [00:03<00:43,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:   9%|▉         | 8/88 [00:03<00:37,  2.11ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:04<00:00,  2.21ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:   9%|▉         | 8/88 [00:04<00:38,  2.07ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:   8%|▊         | 7/88 [00:04<00:43,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:   9%|▉         | 8/88 [00:04<00:38,  2.09ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:   9%|▉         | 8/88 [00:04<00:38,  2.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:   9%|▉         | 8/88 [00:04<00:40,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:   9%|▉         | 8/88 [00:04<00:39,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:   9%|▉         | 8/88 [00:04<00:41,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:04<00:00,  2.26ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  10%|█         | 9/88 [00:04<00:38,  2.04ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:   9%|▉         | 8/88 [00:04<00:40,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  10%|█         | 9/88 [00:04<00:39,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  10%|█         | 9/88 [00:04<00:37,  2.08ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  10%|█         | 9/88 [00:04<00:39,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:04<00:00,  2.62ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:04<00:00,  2.34ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  10%|█         | 9/88 [00:04<00:39,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  10%|█         | 9/88 [00:04<00:39,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  10%|█         | 9/88 [00:04<00:40,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  11%|█▏        | 10/88 [00:04<00:36,  2.12ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  10%|█         | 9/88 [00:05<00:39,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  11%|█▏        | 10/88 [00:05<00:37,  2.09ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  11%|█▏        | 10/88 [00:05<00:36,  2.14ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  11%|█▏        | 10/88 [00:05<00:38,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  11%|█▏        | 10/88 [00:05<00:38,  2.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  11%|█▏        | 10/88 [00:05<00:39,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  12%|█▎        | 11/88 [00:05<00:37,  2.08ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  12%|█▎        | 11/88 [00:05<00:35,  2.18ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  12%|█▎        | 11/88 [00:05<00:33,  2.30ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  11%|█▏        | 10/88 [00:05<00:37,  2.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  11%|█▏        | 10/88 [00:05<00:41,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  12%|█▎        | 11/88 [00:05<00:37,  2.08ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  12%|█▎        | 11/88 [00:05<00:36,  2.10ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  12%|█▎        | 11/88 [00:05<00:37,  2.03ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  12%|█▎        | 11/88 [00:05<00:36,  2.13ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  12%|█▎        | 11/88 [00:05<00:37,  2.06ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  14%|█▎        | 12/88 [00:06<00:37,  2.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  14%|█▎        | 12/88 [00:06<00:39,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  14%|█▎        | 12/88 [00:06<00:41,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  14%|█▎        | 12/88 [00:06<00:39,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  14%|█▎        | 12/88 [00:06<00:38,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  15%|█▍        | 13/88 [00:06<00:33,  2.22ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  14%|█▎        | 12/88 [00:06<00:39,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  14%|█▎        | 12/88 [00:06<00:37,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  14%|█▎        | 12/88 [00:06<00:39,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  15%|█▍        | 13/88 [00:06<00:37,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  15%|█▍        | 13/88 [00:06<00:38,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  15%|█▍        | 13/88 [00:06<00:36,  2.03ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  15%|█▍        | 13/88 [00:06<00:35,  2.10ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  15%|█▍        | 13/88 [00:06<00:36,  2.04ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  15%|█▍        | 13/88 [00:06<00:37,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  15%|█▍        | 13/88 [00:06<00:35,  2.10ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  16%|█▌        | 14/88 [00:06<00:35,  2.08ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  16%|█▌        | 14/88 [00:07<00:36,  2.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  16%|█▌        | 14/88 [00:07<00:35,  2.11ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  16%|█▌        | 14/88 [00:07<00:39,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  16%|█▌        | 14/88 [00:07<00:34,  2.13ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  16%|█▌        | 14/88 [00:07<00:35,  2.08ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  16%|█▌        | 14/88 [00:07<00:34,  2.16ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  16%|█▌        | 14/88 [00:07<00:34,  2.14ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  17%|█▋        | 15/88 [00:07<00:34,  2.14ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  17%|█▋        | 15/88 [00:07<00:34,  2.11ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  17%|█▋        | 15/88 [00:07<00:33,  2.20ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  17%|█▋        | 15/88 [00:07<00:38,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  17%|█▋        | 15/88 [00:07<00:33,  2.21ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  17%|█▋        | 15/88 [00:07<00:32,  2.22ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  17%|█▋        | 15/88 [00:07<00:33,  2.15ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  18%|█▊        | 16/88 [00:07<00:32,  2.23ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  18%|█▊        | 16/88 [00:07<00:32,  2.21ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  17%|█▋        | 15/88 [00:07<00:36,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  18%|█▊        | 16/88 [00:07<00:31,  2.30ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  18%|█▊        | 16/88 [00:08<00:35,  2.03ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  18%|█▊        | 16/88 [00:08<00:32,  2.23ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  18%|█▊        | 16/88 [00:08<00:32,  2.22ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  19%|█▉        | 17/88 [00:08<00:30,  2.30ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  18%|█▊        | 16/88 [00:08<00:34,  2.10ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  19%|█▉        | 17/88 [00:08<00:30,  2.35ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  19%|█▉        | 17/88 [00:08<00:32,  2.20ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  18%|█▊        | 16/88 [00:08<00:36,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  19%|█▉        | 17/88 [00:08<00:33,  2.10ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  19%|█▉        | 17/88 [00:08<00:31,  2.23ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 18/88 [00:08<00:30,  2.32ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  19%|█▉        | 17/88 [00:08<00:32,  2.20ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  19%|█▉        | 17/88 [00:08<00:33,  2.12ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 18/88 [00:08<00:29,  2.34ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 18/88 [00:08<00:31,  2.21ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  19%|█▉        | 17/88 [00:08<00:34,  2.04ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 18/88 [00:08<00:32,  2.18ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 18/88 [00:09<00:31,  2.25ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 18/88 [00:09<00:31,  2.23ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 18/88 [00:09<00:31,  2.19ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  22%|██▏       | 19/88 [00:09<00:32,  2.10ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 18/88 [00:09<00:32,  2.18ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  22%|██▏       | 19/88 [00:09<00:33,  2.09ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  22%|██▏       | 19/88 [00:09<00:34,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  22%|██▏       | 19/88 [00:09<00:34,  2.03ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  22%|██▏       | 19/88 [00:09<00:33,  2.08ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  22%|██▏       | 19/88 [00:09<00:33,  2.07ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  23%|██▎       | 20/88 [00:09<00:31,  2.16ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  22%|██▏       | 19/88 [00:09<00:33,  2.06ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  23%|██▎       | 20/88 [00:09<00:31,  2.18ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  23%|██▎       | 20/88 [00:09<00:33,  2.06ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  22%|██▏       | 19/88 [00:09<00:34,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  23%|██▎       | 20/88 [00:09<00:32,  2.11ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  23%|██▎       | 20/88 [00:10<00:32,  2.12ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  23%|██▎       | 20/88 [00:10<00:32,  2.08ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  23%|██▎       | 20/88 [00:10<00:31,  2.14ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  24%|██▍       | 21/88 [00:10<00:29,  2.24ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  24%|██▍       | 21/88 [00:10<00:34,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  24%|██▍       | 21/88 [00:10<00:31,  2.10ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  23%|██▎       | 20/88 [00:10<00:33,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  24%|██▍       | 21/88 [00:10<00:33,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  24%|██▍       | 21/88 [00:10<00:30,  2.18ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  25%|██▌       | 22/88 [00:10<00:28,  2.28ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  24%|██▍       | 21/88 [00:10<00:32,  2.09ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  24%|██▍       | 21/88 [00:10<00:31,  2.14ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  25%|██▌       | 22/88 [00:10<00:31,  2.08ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  25%|██▌       | 22/88 [00:10<00:30,  2.16ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  24%|██▍       | 21/88 [00:10<00:32,  2.03ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  25%|██▌       | 22/88 [00:10<00:29,  2.20ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  26%|██▌       | 23/88 [00:11<00:28,  2.31ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  25%|██▌       | 22/88 [00:11<00:29,  2.23ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  25%|██▌       | 22/88 [00:11<00:34,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  26%|██▌       | 23/88 [00:11<00:30,  2.15ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  26%|██▌       | 23/88 [00:11<00:29,  2.19ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  25%|██▌       | 22/88 [00:11<00:34,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  25%|██▌       | 22/88 [00:11<00:31,  2.12ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  26%|██▌       | 23/88 [00:11<00:29,  2.21ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  26%|██▌       | 23/88 [00:11<00:32,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 24/88 [00:11<00:28,  2.23ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 24/88 [00:11<00:29,  2.16ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  26%|██▌       | 23/88 [00:11<00:32,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 24/88 [00:11<00:29,  2.15ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  26%|██▌       | 23/88 [00:11<00:32,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  26%|██▌       | 23/88 [00:11<00:29,  2.19ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 24/88 [00:11<00:29,  2.14ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 24/88 [00:12<00:32,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  28%|██▊       | 25/88 [00:12<00:30,  2.09ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 24/88 [00:12<00:28,  2.23ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 24/88 [00:12<00:30,  2.07ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 24/88 [00:12<00:32,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  28%|██▊       | 25/88 [00:12<00:33,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  28%|██▊       | 25/88 [00:12<00:32,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  28%|██▊       | 25/88 [00:12<00:31,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  30%|██▉       | 26/88 [00:12<00:29,  2.07ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  28%|██▊       | 25/88 [00:12<00:33,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  28%|██▊       | 25/88 [00:12<00:32,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  28%|██▊       | 25/88 [00:12<00:31,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  30%|██▉       | 26/88 [00:12<00:31,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  30%|██▉       | 26/88 [00:12<00:30,  2.03ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  28%|██▊       | 25/88 [00:12<00:33,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  30%|██▉       | 26/88 [00:12<00:30,  2.03ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  31%|███       | 27/88 [00:13<00:29,  2.07ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  30%|██▉       | 26/88 [00:13<00:31,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  30%|██▉       | 26/88 [00:13<00:31,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  31%|███       | 27/88 [00:13<00:30,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  30%|██▉       | 26/88 [00:13<00:32,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  30%|██▉       | 26/88 [00:13<00:32,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  31%|███       | 27/88 [00:13<00:32,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  31%|███       | 27/88 [00:13<00:29,  2.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  32%|███▏      | 28/88 [00:13<00:29,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  31%|███       | 27/88 [00:13<00:30,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  31%|███       | 27/88 [00:13<00:30,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  31%|███       | 27/88 [00:13<00:30,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  32%|███▏      | 28/88 [00:13<00:30,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  31%|███       | 27/88 [00:13<00:32,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  32%|███▏      | 28/88 [00:13<00:31,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  32%|███▏      | 28/88 [00:13<00:29,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 29/88 [00:14<00:28,  2.04ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  32%|███▏      | 28/88 [00:14<00:30,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  32%|███▏      | 28/88 [00:14<00:29,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 29/88 [00:14<00:29,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  32%|███▏      | 28/88 [00:14<00:29,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  32%|███▏      | 28/88 [00:14<00:31,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 29/88 [00:14<00:29,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 29/88 [00:14<00:29,  2.03ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  34%|███▍      | 30/88 [00:14<00:28,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 29/88 [00:14<00:29,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 29/88 [00:14<00:28,  2.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 29/88 [00:14<00:29,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  34%|███▍      | 30/88 [00:14<00:29,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 29/88 [00:14<00:30,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  34%|███▍      | 30/88 [00:14<00:31,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  34%|███▍      | 30/88 [00:14<00:29,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  34%|███▍      | 30/88 [00:15<00:29,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  34%|███▍      | 30/88 [00:15<00:27,  2.08ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  34%|███▍      | 30/88 [00:15<00:28,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  34%|███▍      | 30/88 [00:15<00:29,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  35%|███▌      | 31/88 [00:15<00:33,  1.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  35%|███▌      | 31/88 [00:15<00:31,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  35%|███▌      | 31/88 [00:15<00:32,  1.75ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  35%|███▌      | 31/88 [00:15<00:31,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  35%|███▌      | 31/88 [00:15<00:29,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  35%|███▌      | 31/88 [00:15<00:31,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  36%|███▋      | 32/88 [00:15<00:30,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  36%|███▋      | 32/88 [00:15<00:29,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  35%|███▌      | 31/88 [00:15<00:32,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  35%|███▌      | 31/88 [00:15<00:31,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  36%|███▋      | 32/88 [00:16<00:30,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  36%|███▋      | 32/88 [00:16<00:29,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  36%|███▋      | 32/88 [00:16<00:28,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  36%|███▋      | 32/88 [00:16<00:29,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  38%|███▊      | 33/88 [00:16<00:29,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  38%|███▊      | 33/88 [00:16<00:28,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  36%|███▋      | 32/88 [00:16<00:30,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  36%|███▋      | 32/88 [00:16<00:30,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  38%|███▊      | 33/88 [00:16<00:28,  1.95ba/s][1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  38%|███▊      | 33/88 [00:16<00:29,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  38%|███▊      | 33/88 [00:16<00:28,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  38%|███▊      | 33/88 [00:16<00:27,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  39%|███▊      | 34/88 [00:16<00:28,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  39%|███▊      | 34/88 [00:16<00:26,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  38%|███▊      | 33/88 [00:16<00:28,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  38%|███▊      | 33/88 [00:16<00:29,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  39%|███▊      | 34/88 [00:17<00:27,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  39%|███▊      | 34/88 [00:17<00:28,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  39%|███▊      | 34/88 [00:17<00:26,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  39%|███▊      | 34/88 [00:17<00:27,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  40%|███▉      | 35/88 [00:17<00:26,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  40%|███▉      | 35/88 [00:17<00:28,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  39%|███▊      | 34/88 [00:17<00:27,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  39%|███▊      | 34/88 [00:17<00:28,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  40%|███▉      | 35/88 [00:17<00:25,  2.07ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  40%|███▉      | 35/88 [00:17<00:27,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  40%|███▉      | 35/88 [00:17<00:26,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  40%|███▉      | 35/88 [00:17<00:26,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  41%|████      | 36/88 [00:17<00:27,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  41%|████      | 36/88 [00:17<00:28,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  41%|████      | 36/88 [00:17<00:25,  2.06ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  40%|███▉      | 35/88 [00:17<00:27,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  40%|███▉      | 35/88 [00:18<00:28,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  41%|████      | 36/88 [00:18<00:26,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  41%|████      | 36/88 [00:18<00:25,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  41%|████      | 36/88 [00:18<00:26,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  41%|████      | 36/88 [00:18<00:26,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  41%|████      | 36/88 [00:18<00:27,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  42%|████▏     | 37/88 [00:18<00:27,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  42%|████▏     | 37/88 [00:18<00:30,  1.69ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  42%|████▏     | 37/88 [00:18<00:27,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  42%|████▏     | 37/88 [00:18<00:28,  1.76ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  42%|████▏     | 37/88 [00:18<00:27,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  42%|████▏     | 37/88 [00:18<00:28,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  43%|████▎     | 38/88 [00:18<00:26,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  43%|████▎     | 38/88 [00:19<00:27,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  42%|████▏     | 37/88 [00:19<00:27,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  43%|████▎     | 38/88 [00:19<00:26,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  42%|████▏     | 37/88 [00:19<00:29,  1.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  43%|████▎     | 38/88 [00:19<00:26,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  43%|████▎     | 38/88 [00:19<00:25,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  43%|████▎     | 38/88 [00:19<00:27,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  44%|████▍     | 39/88 [00:19<00:25,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  43%|████▎     | 38/88 [00:19<00:26,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  44%|████▍     | 39/88 [00:19<00:26,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  44%|████▍     | 39/88 [00:19<00:25,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  44%|████▍     | 39/88 [00:19<00:25,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  43%|████▎     | 38/88 [00:19<00:28,  1.75ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  44%|████▍     | 39/88 [00:19<00:25,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  44%|████▍     | 39/88 [00:19<00:25,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  45%|████▌     | 40/88 [00:19<00:24,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  45%|████▌     | 40/88 [00:20<00:24,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  45%|████▌     | 40/88 [00:20<00:25,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  44%|████▍     | 39/88 [00:20<00:27,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  45%|████▌     | 40/88 [00:20<00:23,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  45%|████▌     | 40/88 [00:20<00:24,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  45%|████▌     | 40/88 [00:20<00:23,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  44%|████▍     | 39/88 [00:20<00:27,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 41/88 [00:20<00:23,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 41/88 [00:20<00:23,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 41/88 [00:20<00:23,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 41/88 [00:20<00:22,  2.07ba/s][1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 41/88 [00:20<00:23,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  45%|████▌     | 40/88 [00:20<00:25,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  45%|████▌     | 40/88 [00:20<00:25,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 41/88 [00:20<00:24,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  48%|████▊     | 42/88 [00:20<00:22,  2.03ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  48%|████▊     | 42/88 [00:21<00:22,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  48%|████▊     | 42/88 [00:21<00:24,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  48%|████▊     | 42/88 [00:21<00:21,  2.10ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  48%|████▊     | 42/88 [00:21<00:22,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 41/88 [00:21<00:23,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  48%|████▊     | 42/88 [00:21<00:22,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 41/88 [00:21<00:24,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  49%|████▉     | 43/88 [00:21<00:23,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  49%|████▉     | 43/88 [00:21<00:23,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  48%|████▊     | 42/88 [00:21<00:22,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  48%|████▊     | 42/88 [00:21<00:22,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  49%|████▉     | 43/88 [00:21<00:25,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  49%|████▉     | 43/88 [00:21<00:24,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  49%|████▉     | 43/88 [00:21<00:24,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  49%|████▉     | 43/88 [00:21<00:24,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  50%|█████     | 44/88 [00:21<00:22,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  50%|█████     | 44/88 [00:22<00:21,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  50%|█████     | 44/88 [00:22<00:23,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  49%|████▉     | 43/88 [00:22<00:23,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  50%|█████     | 44/88 [00:22<00:22,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  50%|█████     | 44/88 [00:22<00:22,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  49%|████▉     | 43/88 [00:22<00:24,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  50%|█████     | 44/88 [00:22<00:26,  1.69ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  51%|█████     | 45/88 [00:22<00:20,  2.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  51%|█████     | 45/88 [00:22<00:20,  2.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  50%|█████     | 44/88 [00:22<00:22,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  51%|█████     | 45/88 [00:22<00:22,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  50%|█████     | 44/88 [00:22<00:22,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  51%|█████     | 45/88 [00:22<00:21,  2.04ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  51%|█████     | 45/88 [00:22<00:22,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  51%|█████     | 45/88 [00:22<00:24,  1.75ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  52%|█████▏    | 46/88 [00:22<00:20,  2.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  51%|█████     | 45/88 [00:23<00:21,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  52%|█████▏    | 46/88 [00:23<00:21,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  52%|█████▏    | 46/88 [00:23<00:21,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  52%|█████▏    | 46/88 [00:23<00:21,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  51%|█████     | 45/88 [00:23<00:22,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  52%|█████▏    | 46/88 [00:23<00:21,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  52%|█████▏    | 46/88 [00:23<00:22,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 47/88 [00:23<00:19,  2.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 47/88 [00:23<00:20,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  52%|█████▏    | 46/88 [00:23<00:21,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 47/88 [00:23<00:20,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  52%|█████▏    | 46/88 [00:23<00:21,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 47/88 [00:23<00:20,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 47/88 [00:23<00:21,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 47/88 [00:23<00:21,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  55%|█████▍    | 48/88 [00:23<00:19,  2.10ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 47/88 [00:24<00:20,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  55%|█████▍    | 48/88 [00:24<00:19,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  55%|█████▍    | 48/88 [00:24<00:19,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  55%|█████▍    | 48/88 [00:24<00:19,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 47/88 [00:24<00:20,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  56%|█████▌    | 49/88 [00:24<00:18,  2.17ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  55%|█████▍    | 48/88 [00:24<00:21,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  55%|█████▍    | 48/88 [00:24<00:21,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  55%|█████▍    | 48/88 [00:24<00:19,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  55%|█████▍    | 48/88 [00:24<00:20,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  56%|█████▌    | 49/88 [00:24<00:19,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  56%|█████▌    | 49/88 [00:24<00:19,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  56%|█████▌    | 49/88 [00:24<00:19,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  56%|█████▌    | 49/88 [00:24<00:19,  2.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  57%|█████▋    | 50/88 [00:24<00:20,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  56%|█████▌    | 49/88 [00:25<00:22,  1.75ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  56%|█████▌    | 49/88 [00:25<00:19,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  56%|█████▌    | 49/88 [00:25<00:19,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  57%|█████▋    | 50/88 [00:25<00:19,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  58%|█████▊    | 51/88 [00:25<00:18,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  57%|█████▋    | 50/88 [00:25<00:20,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  57%|█████▋    | 50/88 [00:25<00:21,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  57%|█████▋    | 50/88 [00:25<00:20,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  57%|█████▋    | 50/88 [00:25<00:21,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  57%|█████▋    | 50/88 [00:25<00:20,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  58%|█████▊    | 51/88 [00:25<00:18,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  59%|█████▉    | 52/88 [00:25<00:17,  2.06ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  57%|█████▋    | 50/88 [00:25<00:20,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  58%|█████▊    | 51/88 [00:25<00:19,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  58%|█████▊    | 51/88 [00:25<00:19,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  58%|█████▊    | 51/88 [00:25<00:19,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  58%|█████▊    | 51/88 [00:26<00:19,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 53/88 [00:26<00:17,  2.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  59%|█████▉    | 52/88 [00:26<00:18,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  58%|█████▊    | 51/88 [00:26<00:19,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  58%|█████▊    | 51/88 [00:26<00:19,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  59%|█████▉    | 52/88 [00:26<00:18,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  59%|█████▉    | 52/88 [00:26<00:18,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  59%|█████▉    | 52/88 [00:26<00:17,  2.06ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  59%|█████▉    | 52/88 [00:26<00:19,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 53/88 [00:26<00:17,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  61%|██████▏   | 54/88 [00:26<00:16,  2.04ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 53/88 [00:26<00:17,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 53/88 [00:26<00:18,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 53/88 [00:26<00:17,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 53/88 [00:26<00:17,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  59%|█████▉    | 52/88 [00:26<00:19,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  59%|█████▉    | 52/88 [00:27<00:20,  1.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  61%|██████▏   | 54/88 [00:27<00:16,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  61%|██████▏   | 54/88 [00:27<00:16,  2.10ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  61%|██████▏   | 54/88 [00:27<00:17,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  61%|██████▏   | 54/88 [00:27<00:16,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 53/88 [00:27<00:17,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  62%|██████▎   | 55/88 [00:27<00:16,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  61%|██████▏   | 54/88 [00:27<00:17,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 53/88 [00:27<00:19,  1.75ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  62%|██████▎   | 55/88 [00:27<00:16,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  62%|██████▎   | 55/88 [00:27<00:16,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  62%|██████▎   | 55/88 [00:27<00:16,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  62%|██████▎   | 55/88 [00:27<00:16,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  61%|██████▏   | 54/88 [00:27<00:17,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  62%|██████▎   | 55/88 [00:27<00:17,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  64%|██████▎   | 56/88 [00:28<00:17,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  61%|██████▏   | 54/88 [00:28<00:18,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  65%|██████▍   | 57/88 [00:28<00:15,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  62%|██████▎   | 55/88 [00:28<00:17,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  64%|██████▎   | 56/88 [00:28<00:17,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  62%|██████▎   | 55/88 [00:28<00:16,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  64%|██████▎   | 56/88 [00:28<00:17,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  64%|██████▎   | 56/88 [00:28<00:18,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  64%|██████▎   | 56/88 [00:28<00:18,  1.76ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  64%|██████▎   | 56/88 [00:28<00:17,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  66%|██████▌   | 58/88 [00:28<00:14,  2.04ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  65%|██████▍   | 57/88 [00:29<00:16,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  65%|██████▍   | 57/88 [00:29<00:16,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  65%|██████▍   | 57/88 [00:29<00:16,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  65%|██████▍   | 57/88 [00:29<00:16,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  65%|██████▍   | 57/88 [00:29<00:17,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  64%|██████▎   | 56/88 [00:29<00:18,  1.74ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  64%|██████▎   | 56/88 [00:29<00:18,  1.75ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 59/88 [00:29<00:14,  2.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  66%|██████▌   | 58/88 [00:29<00:15,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  66%|██████▌   | 58/88 [00:29<00:15,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  65%|██████▍   | 57/88 [00:29<00:16,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  66%|██████▌   | 58/88 [00:29<00:15,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  66%|██████▌   | 58/88 [00:29<00:15,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  66%|██████▌   | 58/88 [00:29<00:17,  1.70ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  68%|██████▊   | 60/88 [00:29<00:12,  2.16ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  65%|██████▍   | 57/88 [00:29<00:18,  1.68ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 59/88 [00:30<00:14,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 59/88 [00:30<00:14,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 59/88 [00:30<00:14,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  66%|██████▌   | 58/88 [00:30<00:16,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 59/88 [00:30<00:16,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 59/88 [00:30<00:15,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  66%|██████▌   | 58/88 [00:30<00:17,  1.75ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  69%|██████▉   | 61/88 [00:30<00:14,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  68%|██████▊   | 60/88 [00:30<00:13,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  68%|██████▊   | 60/88 [00:30<00:13,  2.03ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  68%|██████▊   | 60/88 [00:30<00:14,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 59/88 [00:30<00:14,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  68%|██████▊   | 60/88 [00:30<00:14,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  68%|██████▊   | 60/88 [00:30<00:15,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 59/88 [00:30<00:15,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  69%|██████▉   | 61/88 [00:30<00:13,  2.03ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  70%|███████   | 62/88 [00:31<00:14,  1.81ba/s][1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  69%|██████▉   | 61/88 [00:31<00:13,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  69%|██████▉   | 61/88 [00:31<00:13,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  68%|██████▊   | 60/88 [00:31<00:14,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  69%|██████▉   | 61/88 [00:31<00:13,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  69%|██████▉   | 61/88 [00:31<00:13,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  68%|██████▊   | 60/88 [00:31<00:14,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  72%|███████▏  | 63/88 [00:31<00:13,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  69%|██████▉   | 61/88 [00:31<00:13,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  70%|███████   | 62/88 [00:31<00:14,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  70%|███████   | 62/88 [00:31<00:14,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  70%|███████   | 62/88 [00:31<00:14,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  70%|███████   | 62/88 [00:31<00:14,  1.76ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  70%|███████   | 62/88 [00:31<00:14,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  69%|██████▉   | 61/88 [00:31<00:14,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 64/88 [00:31<00:12,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  72%|███████▏  | 63/88 [00:32<00:13,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  70%|███████   | 62/88 [00:32<00:14,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  72%|███████▏  | 63/88 [00:32<00:12,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  72%|███████▏  | 63/88 [00:32<00:13,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  72%|███████▏  | 63/88 [00:32<00:13,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  72%|███████▏  | 63/88 [00:32<00:13,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  74%|███████▍  | 65/88 [00:32<00:11,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  70%|███████   | 62/88 [00:32<00:14,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  72%|███████▏  | 63/88 [00:32<00:13,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 64/88 [00:32<00:12,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 64/88 [00:32<00:12,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 64/88 [00:32<00:12,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  75%|███████▌  | 66/88 [00:32<00:10,  2.04ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  72%|███████▏  | 63/88 [00:32<00:12,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 64/88 [00:32<00:14,  1.68ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 64/88 [00:33<00:14,  1.64ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 64/88 [00:33<00:12,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  74%|███████▍  | 65/88 [00:33<00:11,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  74%|███████▍  | 65/88 [00:33<00:11,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  74%|███████▍  | 65/88 [00:33<00:11,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  74%|███████▍  | 65/88 [00:33<00:13,  1.76ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  76%|███████▌  | 67/88 [00:33<00:10,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  74%|███████▍  | 65/88 [00:33<00:13,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 64/88 [00:33<00:12,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  74%|███████▍  | 65/88 [00:33<00:11,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  75%|███████▌  | 66/88 [00:33<00:11,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  75%|███████▌  | 66/88 [00:33<00:11,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  75%|███████▌  | 66/88 [00:33<00:11,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  75%|███████▌  | 66/88 [00:33<00:12,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  74%|███████▍  | 65/88 [00:34<00:11,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  75%|███████▌  | 66/88 [00:34<00:12,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  76%|███████▌  | 67/88 [00:34<00:10,  2.06ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  76%|███████▌  | 67/88 [00:34<00:10,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  76%|███████▌  | 67/88 [00:34<00:10,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  77%|███████▋  | 68/88 [00:34<00:11,  1.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  75%|███████▌  | 66/88 [00:34<00:11,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  75%|███████▌  | 66/88 [00:34<00:11,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  76%|███████▌  | 67/88 [00:34<00:11,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  76%|███████▌  | 67/88 [00:34<00:11,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  78%|███████▊  | 69/88 [00:34<00:10,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  76%|███████▌  | 67/88 [00:34<00:10,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  77%|███████▋  | 68/88 [00:34<00:10,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  77%|███████▋  | 68/88 [00:34<00:11,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  77%|███████▋  | 68/88 [00:34<00:11,  1.70ba/s][1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  76%|███████▌  | 67/88 [00:34<00:10,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  80%|███████▉  | 70/88 [00:35<00:09,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  77%|███████▋  | 68/88 [00:35<00:11,  1.73ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  77%|███████▋  | 68/88 [00:35<00:11,  1.73ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  78%|███████▊  | 69/88 [00:35<00:10,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  77%|███████▋  | 68/88 [00:35<00:10,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  78%|███████▊  | 69/88 [00:35<00:10,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  78%|███████▊  | 69/88 [00:35<00:10,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  81%|████████  | 71/88 [00:35<00:08,  2.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  78%|███████▊  | 69/88 [00:35<00:10,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  78%|███████▊  | 69/88 [00:35<00:10,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  77%|███████▋  | 68/88 [00:35<00:11,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  80%|███████▉  | 70/88 [00:35<00:09,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  80%|███████▉  | 70/88 [00:35<00:09,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  80%|███████▉  | 70/88 [00:35<00:09,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  78%|███████▊  | 69/88 [00:36<00:11,  1.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  82%|████████▏ | 72/88 [00:36<00:07,  2.10ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  80%|███████▉  | 70/88 [00:36<00:09,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  78%|███████▊  | 69/88 [00:36<00:10,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  80%|███████▉  | 70/88 [00:36<00:09,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  81%|████████  | 71/88 [00:36<00:08,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  81%|████████  | 71/88 [00:36<00:08,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  81%|████████  | 71/88 [00:36<00:08,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  80%|███████▉  | 70/88 [00:36<00:09,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  83%|████████▎ | 73/88 [00:36<00:07,  2.04ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  80%|███████▉  | 70/88 [00:36<00:09,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  81%|████████  | 71/88 [00:36<00:09,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  81%|████████  | 71/88 [00:36<00:09,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  82%|████████▏ | 72/88 [00:36<00:07,  2.04ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  82%|████████▏ | 72/88 [00:36<00:08,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  82%|████████▏ | 72/88 [00:36<00:08,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  81%|████████  | 71/88 [00:37<00:09,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  82%|████████▏ | 72/88 [00:37<00:08,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  82%|████████▏ | 72/88 [00:37<00:08,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  83%|████████▎ | 73/88 [00:37<00:06,  2.15ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  84%|████████▍ | 74/88 [00:37<00:07,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  81%|████████  | 71/88 [00:37<00:09,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  83%|████████▎ | 73/88 [00:37<00:07,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  83%|████████▎ | 73/88 [00:37<00:07,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  82%|████████▏ | 72/88 [00:37<00:08,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  83%|████████▎ | 73/88 [00:37<00:07,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  85%|████████▌ | 75/88 [00:37<00:06,  1.87ba/s][1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  82%|████████▏ | 72/88 [00:37<00:08,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  83%|████████▎ | 73/88 [00:37<00:07,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  83%|████████▎ | 73/88 [00:37<00:07,  2.03ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  84%|████████▍ | 74/88 [00:37<00:07,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  84%|████████▍ | 74/88 [00:38<00:07,  1.75ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  84%|████████▍ | 74/88 [00:38<00:08,  1.73ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  86%|████████▋ | 76/88 [00:38<00:06,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  83%|████████▎ | 73/88 [00:38<00:07,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  84%|████████▍ | 74/88 [00:38<00:07,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  85%|████████▌ | 75/88 [00:38<00:06,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  84%|████████▍ | 74/88 [00:38<00:08,  1.69ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  85%|████████▌ | 75/88 [00:38<00:06,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  85%|████████▌ | 75/88 [00:38<00:07,  1.76ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  84%|████████▍ | 74/88 [00:38<00:08,  1.69ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  85%|████████▌ | 75/88 [00:38<00:06,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  88%|████████▊ | 77/88 [00:38<00:05,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  86%|████████▋ | 76/88 [00:38<00:05,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  84%|████████▍ | 74/88 [00:38<00:07,  1.76ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  85%|████████▌ | 75/88 [00:38<00:07,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  86%|████████▋ | 76/88 [00:39<00:06,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  86%|████████▋ | 76/88 [00:39<00:06,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  89%|████████▊ | 78/88 [00:39<00:05,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  86%|████████▋ | 76/88 [00:39<00:06,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  85%|████████▌ | 75/88 [00:39<00:07,  1.76ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  85%|████████▌ | 75/88 [00:39<00:07,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  88%|████████▊ | 77/88 [00:39<00:05,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  86%|████████▋ | 76/88 [00:39<00:06,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  88%|████████▊ | 77/88 [00:39<00:05,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  88%|████████▊ | 77/88 [00:39<00:05,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  86%|████████▋ | 76/88 [00:39<00:06,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  90%|████████▉ | 79/88 [00:39<00:04,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  88%|████████▊ | 77/88 [00:39<00:05,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  86%|████████▋ | 76/88 [00:39<00:06,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  89%|████████▊ | 78/88 [00:39<00:05,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  88%|████████▊ | 77/88 [00:39<00:05,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  89%|████████▊ | 78/88 [00:39<00:05,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  89%|████████▊ | 78/88 [00:40<00:05,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  91%|█████████ | 80/88 [00:40<00:04,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  88%|████████▊ | 77/88 [00:40<00:06,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  90%|████████▉ | 79/88 [00:40<00:04,  2.06ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  88%|████████▊ | 77/88 [00:40<00:05,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  89%|████████▊ | 78/88 [00:40<00:05,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  89%|████████▊ | 78/88 [00:40<00:05,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  90%|████████▉ | 79/88 [00:40<00:04,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  90%|████████▉ | 79/88 [00:40<00:04,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  89%|████████▊ | 78/88 [00:40<00:05,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  91%|█████████ | 80/88 [00:40<00:03,  2.06ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  90%|████████▉ | 79/88 [00:40<00:04,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  92%|█████████▏| 81/88 [00:40<00:03,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  89%|████████▊ | 78/88 [00:40<00:05,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  91%|█████████ | 80/88 [00:40<00:04,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  90%|████████▉ | 79/88 [00:41<00:04,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  91%|█████████ | 80/88 [00:41<00:04,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  90%|████████▉ | 79/88 [00:41<00:04,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  91%|█████████ | 80/88 [00:41<00:04,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 82/88 [00:41<00:03,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  90%|████████▉ | 79/88 [00:41<00:04,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  91%|█████████ | 80/88 [00:41<00:04,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  92%|█████████▏| 81/88 [00:41<00:03,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  92%|█████████▏| 81/88 [00:41<00:03,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  92%|█████████▏| 81/88 [00:41<00:03,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  91%|█████████ | 80/88 [00:41<00:04,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  94%|█████████▍| 83/88 [00:41<00:02,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  91%|█████████ | 80/88 [00:41<00:04,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  92%|█████████▏| 81/88 [00:41<00:03,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 82/88 [00:42<00:03,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 82/88 [00:42<00:03,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  92%|█████████▏| 81/88 [00:42<00:03,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 82/88 [00:42<00:03,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  95%|█████████▌| 84/88 [00:42<00:01,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  92%|█████████▏| 81/88 [00:42<00:03,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  92%|█████████▏| 81/88 [00:42<00:03,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 82/88 [00:42<00:03,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  94%|█████████▍| 83/88 [00:42<00:02,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 82/88 [00:42<00:03,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  94%|█████████▍| 83/88 [00:42<00:02,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  94%|█████████▍| 83/88 [00:42<00:02,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  97%|█████████▋| 85/88 [00:42<00:01,  2.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 82/88 [00:42<00:03,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  95%|█████████▌| 84/88 [00:43<00:02,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 82/88 [00:43<00:03,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  95%|█████████▌| 84/88 [00:43<00:02,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  95%|█████████▌| 84/88 [00:43<00:01,  2.12ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  94%|█████████▍| 83/88 [00:43<00:02,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  94%|█████████▍| 83/88 [00:43<00:02,  1.73ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  98%|█████████▊| 86/88 [00:43<00:01,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  94%|█████████▍| 83/88 [00:43<00:02,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  97%|█████████▋| 85/88 [00:43<00:01,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  94%|█████████▍| 83/88 [00:43<00:02,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  97%|█████████▋| 85/88 [00:43<00:01,  2.08ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  97%|█████████▋| 85/88 [00:43<00:01,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  95%|█████████▌| 84/88 [00:43<00:02,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  95%|█████████▌| 84/88 [00:43<00:02,  1.76ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  95%|█████████▌| 84/88 [00:43<00:02,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  99%|█████████▉| 87/88 [00:43<00:00,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  98%|█████████▊| 86/88 [00:44<00:00,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  98%|█████████▊| 86/88 [00:44<00:00,  2.08ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  95%|█████████▌| 84/88 [00:44<00:02,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  97%|█████████▋| 85/88 [00:44<00:01,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  98%|█████████▊| 86/88 [00:44<00:01,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:44<00:00,  2.16ba/s][1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:44<00:00,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  97%|█████████▋| 85/88 [00:44<00:01,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  97%|█████████▋| 85/88 [00:44<00:01,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  98%|█████████▊| 86/88 [00:44<00:01,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  98%|█████████▊| 86/88 [00:44<00:00,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  99%|█████████▉| 87/88 [00:44<00:00,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  99%|█████████▉| 87/88 [00:44<00:00,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  99%|█████████▉| 87/88 [00:44<00:00,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  98%|█████████▊| 86/88 [00:44<00:00,  2.11ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  97%|█████████▋| 85/88 [00:44<00:01,  1.67ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:45<00:00,  2.18ba/s][1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:45<00:00,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:45<00:00,  2.14ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:45<00:00,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:45<00:00,  2.03ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:45<00:00,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  99%|█████████▉| 87/88 [00:45<00:00,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  98%|█████████▊| 86/88 [00:45<00:01,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  99%|█████████▉| 87/88 [00:45<00:00,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  99%|█████████▉| 87/88 [00:45<00:00,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:45<00:00,  2.14ba/s][1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:45<00:00,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:45<00:00,  2.24ba/s][1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:45<00:00,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:45<00:00,  2.37ba/s][1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:45<00:00,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  99%|█████████▉| 87/88 [00:45<00:00,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:45<00:00,  2.30ba/s][1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:45<00:00,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s][1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s][1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s][1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s][1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading builder script: 100%|██████████| 4.53k/4.53k [00:00<00:00, 4.71MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading extra modules:   0%|          | 0.00/3.32k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading extra modules: 100%|██████████| 3.32k/3.32k [00:00<00:00, 2.94MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:00<00:05,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:00<00:05,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:00<00:05,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:00<00:05,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:00<00:05,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:00<00:05,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:00<00:05,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:00<00:05,  1.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:00<00:05,  1.70ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:00<00:06,  1.56ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:511] 2024-01-23 03:39:38,308 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:511] 2024-01-23 03:39:38,308 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:565] 2024-01-23 03:39:38,309 >> Using cuda_amp half precision backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:565] 2024-01-23 03:39:38,309 >> Using cuda_amp half precision backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:00<00:06,  1.50ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:00<00:07,  1.32ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:00<00:08,  1.23ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:00<00:09,  1.09ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:00<00:09,  1.08ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:01<00:04,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:01<00:04,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:01<00:04,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:01<00:04,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:01<00:04,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:01<00:04,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:01<00:04,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:01<00:05,  1.73ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:01<00:05,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:01<00:05,  1.75ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:01<00:05,  1.62ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:01<00:05,  1.57ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:01<00:06,  1.47ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:01<00:06,  1.49ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:01<00:06,  1.42ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:01<00:04,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:01<00:04,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:01<00:04,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:01<00:04,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:01<00:04,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:01<00:04,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:01<00:04,  1.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:01<00:04,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:01<00:04,  1.69ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:01<00:04,  1.62ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:01<00:05,  1.56ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:01<00:04,  1.71ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:01<00:04,  1.66ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:01<00:04,  1.60ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:02<00:04,  1.60ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:02<00:03,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:02<00:03,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:02<00:03,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:02<00:03,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:02<00:03,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:02<00:03,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:02<00:03,  1.75ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:02<00:03,  1.76ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:02<00:04,  1.71ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:02<00:04,  1.73ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:02<00:04,  1.59ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:02<00:04,  1.71ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:02<00:04,  1.67ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:02<00:04,  1.73ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:02<00:04,  1.49ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:02<00:03,  1.62ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:02<00:03,  1.63ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:02<00:03,  1.61ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:02<00:03,  1.64ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:02<00:03,  1.62ba/s][1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:02<00:03,  1.61ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:02<00:03,  1.62ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:03<00:03,  1.62ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:03<00:03,  1.55ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:03<00:03,  1.59ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:03<00:03,  1.66ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:03<00:03,  1.62ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:03<00:03,  1.52ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:03<00:03,  1.53ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:03<00:04,  1.46ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:03<00:03,  1.55ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:03<00:03,  1.51ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:03<00:03,  1.49ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:03<00:03,  1.54ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:03<00:03,  1.50ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:03<00:03,  1.47ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:03<00:03,  1.50ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:03<00:03,  1.47ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:03<00:03,  1.45ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:03<00:03,  1.53ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:03<00:03,  1.49ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:03<00:03,  1.55ba/s][1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:03<00:03,  1.53ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:03<00:03,  1.43ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:04<00:03,  1.36ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:04<00:02,  1.60ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:04<00:02,  1.57ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:04<00:02,  1.55ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:04<00:02,  1.57ba/s][1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:04<00:02,  1.54ba/s][1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:04<00:02,  1.54ba/s][1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:04<00:02,  1.56ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:04<00:02,  1.53ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:04<00:02,  1.54ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:04<00:02,  1.61ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:04<00:02,  1.57ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:04<00:02,  1.58ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:04<00:02,  1.62ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:04<00:02,  1.52ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:04<00:02,  1.44ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:04<00:01,  1.63ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:04<00:01,  1.61ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:04<00:01,  1.59ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:04<00:01,  1.58ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:04<00:01,  1.59ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:04<00:01,  1.59ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:04<00:01,  1.56ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:04<00:01,  1.57ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:04<00:01,  1.55ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:05<00:01,  1.66ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:05<00:01,  1.61ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:05<00:01,  1.61ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:05<00:01,  1.59ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:05<00:01,  1.56ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:05<00:01,  1.50ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:05<00:01,  1.68ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:05<00:01,  1.65ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:05<00:01,  1.62ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:05<00:01,  1.62ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:05<00:01,  1.60ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:05<00:01,  1.60ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:05<00:01,  1.59ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:05<00:01,  1.59ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:05<00:01,  1.59ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:05<00:01,  1.67ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:05<00:01,  1.65ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:05<00:01,  1.66ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:05<00:01,  1.61ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:05<00:01,  1.55ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:05<00:01,  1.55ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:05<00:00,  1.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:06<00:00,  1.71ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:06<00:00,  1.65ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:06<00:00,  1.65ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:06<00:00,  1.65ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:06<00:00,  1.63ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:06<00:00,  1.63ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:06<00:00,  1.62ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:06<00:00,  1.71ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:06<00:00,  1.70ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:06<00:00,  1.54ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:06<00:00,  1.66ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  2.01ba/s][1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  1.75ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  2.02ba/s][1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  1.74ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:06<00:00,  1.62ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:06<00:00,  1.57ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:06<00:00,  1.66ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  1.91ba/s][1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  1.70ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  1.70ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  1.70ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  1.70ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  1.70ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  1.70ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  1.86ba/s][1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  1.69ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  1.69ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  1.69ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  1.68ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  1.94ba/s][1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  1.66ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  1.66ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  2.00ba/s][1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:06<00:00,  1.64ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Downloading builder script: 100%|██████████| 4.53k/4.53k [00:00<00:00, 5.17MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading builder script: 100%|██████████| 4.53k/4.53k [00:00<00:00, 6.23MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Downloading extra modules:   0%|          | 0.00/3.32k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Downloading extra modules: 100%|██████████| 3.32k/3.32k [00:00<00:00, 4.29MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Downloading builder script: 100%|██████████| 4.53k/4.53k [00:00<00:00, 4.58MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading builder script: 100%|██████████| 4.53k/4.53k [00:00<00:00, 3.10MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Downloading builder script: 100%|██████████| 4.53k/4.53k [00:00<00:00, 2.77MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading builder script: 100%|██████████| 4.53k/4.53k [00:00<00:00, 1.87MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:1650] 2024-01-23 03:39:45,791 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:1651] 2024-01-23 03:39:45,791 >>   Num examples = 88524\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:1650] 2024-01-23 03:39:45,791 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:1651] 2024-01-23 03:39:45,791 >>   Num examples = 88524\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:1652] 2024-01-23 03:39:45,791 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:1652] 2024-01-23 03:39:45,791 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:1653] 2024-01-23 03:39:45,791 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:1654] 2024-01-23 03:39:45,791 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:1653] 2024-01-23 03:39:45,791 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:1654] 2024-01-23 03:39:45,791 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:1655] 2024-01-23 03:39:45,791 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:1656] 2024-01-23 03:39:45,791 >>   Total optimization steps = 100\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:1655] 2024-01-23 03:39:45,791 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:1656] 2024-01-23 03:39:45,791 >>   Total optimization steps = 100\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:1657] 2024-01-23 03:39:45,793 >>   Number of trainable parameters = 334094338\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:1657] 2024-01-23 03:39:45,793 >>   Number of trainable parameters = 334094338\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/100 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 03:39:45.891: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2024-01-23 03:39:45.893: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2024-01-23 03:39:45.893: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2024-01-23 03:39:45.902: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2024-01-23 03:39:45.907: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2024-01-23 03:39:45.907: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2024-01-23 03:39:45.911: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2024-01-23 03:39:45.921: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2024-01-23 03:39:45.933: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2024-01-23 03:39:45.933: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2024-01-23 03:39:45.933: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2024-01-23 03:39:45.933: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2024-01-23 03:39:45.933: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2024-01-23 03:39:45.933: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2024-01-23 03:39:45.934: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2024-01-23 03:39:45.935 algo-1:202 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2024-01-23 03:39:45.936 algo-1:206 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2024-01-23 03:39:45.940: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2024-01-23 03:39:45.945 algo-1:208 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2024-01-23 03:39:45.949 algo-1:210 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2024-01-23 03:39:45.950 algo-1:215 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2024-01-23 03:39:45.957 algo-1:204 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2024-01-23 03:39:45.967 algo-1:214 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2024-01-23 03:39:45.978 algo-2:138 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2024-01-23 03:39:45.978 algo-2:140 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2024-01-23 03:39:45.978 algo-2:147 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2024-01-23 03:39:45.978 algo-2:142 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2024-01-23 03:39:45.978 algo-2:207 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2024-01-23 03:39:45.978 algo-2:215 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2024-01-23 03:39:45.982 algo-1:202 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2024-01-23 03:39:45.982 algo-1:206 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2024-01-23 03:39:45.983 algo-1:202 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2024-01-23 03:39:45.983 algo-1:206 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2024-01-23 03:39:45.983 algo-1:202 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2024-01-23 03:39:45.983 algo-1:206 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2024-01-23 03:39:45.984 algo-1:202 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2024-01-23 03:39:45.984 algo-1:202 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2024-01-23 03:39:45.984 algo-1:206 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2024-01-23 03:39:45.984 algo-1:206 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2024-01-23 03:39:45.987 algo-2:481 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2024-01-23 03:39:45.989 algo-1:208 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2024-01-23 03:39:45.989 algo-1:208 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2024-01-23 03:39:45.990 algo-1:208 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2024-01-23 03:39:45.990 algo-1:208 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2024-01-23 03:39:45.991 algo-1:208 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2024-01-23 03:39:45.993 algo-1:210 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2024-01-23 03:39:45.993 algo-1:215 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2024-01-23 03:39:45.993 algo-1:210 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2024-01-23 03:39:45.994 algo-1:210 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2024-01-23 03:39:45.994 algo-1:215 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2024-01-23 03:39:45.994 algo-1:215 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2024-01-23 03:39:45.994 algo-1:210 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2024-01-23 03:39:45.994 algo-1:210 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2024-01-23 03:39:45.995 algo-1:215 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2024-01-23 03:39:45.995 algo-1:215 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2024-01-23 03:39:46.004 algo-1:204 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2024-01-23 03:39:46.004 algo-1:204 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2024-01-23 03:39:46.005 algo-1:204 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2024-01-23 03:39:46.005 algo-1:204 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2024-01-23 03:39:46.005 algo-1:204 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2024-01-23 03:39:46.016 algo-1:214 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2024-01-23 03:39:46.017 algo-1:214 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2024-01-23 03:39:46.018 algo-1:214 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2024-01-23 03:39:46.018 algo-1:214 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2024-01-23 03:39:46.018 algo-1:214 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 03:39:46.083 algo-1:615 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2024-01-23 03:39:46.122 algo-2:144 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 03:39:46.129 algo-1:615 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 03:39:46.130 algo-1:615 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 03:39:46.131 algo-1:615 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 03:39:46.131 algo-1:615 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 03:39:46.131 algo-1:615 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2024-01-23 03:39:46.149 algo-2:138 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2024-01-23 03:39:46.149 algo-2:138 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2024-01-23 03:39:46.150 algo-2:138 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2024-01-23 03:39:46.150 algo-2:138 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2024-01-23 03:39:46.150 algo-2:138 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2024-01-23 03:39:46.156 algo-2:147 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2024-01-23 03:39:46.157 algo-2:147 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2024-01-23 03:39:46.157 algo-2:147 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2024-01-23 03:39:46.158 algo-2:147 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2024-01-23 03:39:46.158 algo-2:147 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2024-01-23 03:39:46.169 algo-2:144 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2024-01-23 03:39:46.170 algo-2:144 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2024-01-23 03:39:46.170 algo-2:144 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2024-01-23 03:39:46.171 algo-2:144 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2024-01-23 03:39:46.171 algo-2:144 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2024-01-23 03:39:46.172 algo-2:140 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2024-01-23 03:39:46.173 algo-2:140 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2024-01-23 03:39:46.173 algo-2:140 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2024-01-23 03:39:46.173 algo-2:140 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2024-01-23 03:39:46.174 algo-2:140 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2024-01-23 03:39:46.175 algo-2:207 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2024-01-23 03:39:46.175 algo-2:215 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2024-01-23 03:39:46.175 algo-2:207 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2024-01-23 03:39:46.175 algo-2:215 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2024-01-23 03:39:46.176 algo-2:207 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2024-01-23 03:39:46.176 algo-2:215 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2024-01-23 03:39:46.176 algo-2:207 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2024-01-23 03:39:46.176 algo-2:207 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2024-01-23 03:39:46.177 algo-2:215 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2024-01-23 03:39:46.177 algo-2:215 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2024-01-23 03:39:46.183 algo-2:142 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2024-01-23 03:39:46.184 algo-2:142 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2024-01-23 03:39:46.184 algo-2:142 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2024-01-23 03:39:46.185 algo-2:142 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2024-01-23 03:39:46.185 algo-2:142 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2024-01-23 03:39:46.201 algo-2:481 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2024-01-23 03:39:46.201 algo-2:481 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2024-01-23 03:39:46.202 algo-2:481 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2024-01-23 03:39:46.203 algo-2:481 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2024-01-23 03:39:46.203 algo-2:481 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  1%|          | 1/100 [00:02<03:36,  2.19s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  2%|▏         | 2/100 [00:03<02:25,  1.48s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  3%|▎         | 3/100 [00:04<01:58,  1.22s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  4%|▍         | 4/100 [00:05<01:51,  1.16s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  5%|▌         | 5/100 [00:06<01:41,  1.07s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  6%|▌         | 6/100 [00:06<01:35,  1.01s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  7%|▋         | 7/100 [00:07<01:30,  1.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 8/100 [00:08<01:29,  1.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  9%|▉         | 9/100 [00:09<01:27,  1.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 10%|█         | 10/100 [00:10<01:25,  1.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 11%|█         | 11/100 [00:11<01:22,  1.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▏        | 12/100 [00:12<01:20,  1.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 13/100 [00:13<01:19,  1.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 14%|█▍        | 14/100 [00:14<01:18,  1.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▌        | 15/100 [00:15<01:19,  1.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 16%|█▌        | 16/100 [00:16<01:17,  1.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 17/100 [00:17<01:17,  1.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 18/100 [00:18<01:16,  1.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 19%|█▉        | 19/100 [00:18<01:15,  1.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|██        | 20/100 [00:19<01:14,  1.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 21%|██        | 21/100 [00:20<01:13,  1.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▏       | 22/100 [00:21<01:12,  1.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 23/100 [00:22<01:11,  1.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▍       | 24/100 [00:23<01:10,  1.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 25%|██▌       | 25/100 [00:24<01:08,  1.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▌       | 26/100 [00:25<01:07,  1.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 27/100 [00:26<01:08,  1.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 28/100 [00:27<01:07,  1.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 29%|██▉       | 29/100 [00:28<01:05,  1.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|███       | 30/100 [00:29<01:04,  1.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 31%|███       | 31/100 [00:30<01:03,  1.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 32%|███▏      | 32/100 [00:30<01:02,  1.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 33/100 [00:31<01:01,  1.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▍      | 34/100 [00:32<01:00,  1.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 35%|███▌      | 35/100 [00:33<00:59,  1.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▌      | 36/100 [00:34<00:57,  1.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 37%|███▋      | 37/100 [00:35<00:56,  1.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 38/100 [00:36<00:56,  1.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 39%|███▉      | 39/100 [00:37<00:55,  1.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|████      | 40/100 [00:38<00:56,  1.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 41%|████      | 41/100 [00:39<00:54,  1.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 42%|████▏     | 42/100 [00:40<00:54,  1.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 43/100 [00:41<00:52,  1.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▍     | 44/100 [00:41<00:51,  1.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 45%|████▌     | 45/100 [00:42<00:51,  1.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▌     | 46/100 [00:43<00:51,  1.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 47/100 [00:44<00:49,  1.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 48/100 [00:45<00:48,  1.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 49%|████▉     | 49/100 [00:46<00:47,  1.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|█████     | 50/100 [00:47<00:45,  1.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████     | 51/100 [00:48<00:44,  1.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▏    | 52/100 [00:49<00:43,  1.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 53/100 [00:50<00:43,  1.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▍    | 54/100 [00:51<00:41,  1.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▌    | 55/100 [00:52<00:41,  1.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▌    | 56/100 [00:53<00:41,  1.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▋    | 57/100 [00:54<00:40,  1.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 58%|█████▊    | 58/100 [00:54<00:39,  1.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▉    | 59/100 [00:55<00:38,  1.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|██████    | 60/100 [00:56<00:36,  1.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████    | 61/100 [00:57<00:35,  1.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 62/100 [00:58<00:34,  1.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 63%|██████▎   | 63/100 [00:59<00:33,  1.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 64%|██████▍   | 64/100 [01:00<00:32,  1.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 65%|██████▌   | 65/100 [01:01<00:31,  1.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▌   | 66/100 [01:02<00:31,  1.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 67/100 [01:03<00:30,  1.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 68%|██████▊   | 68/100 [01:04<00:30,  1.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▉   | 69/100 [01:05<00:29,  1.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|███████   | 70/100 [01:06<00:28,  1.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 71%|███████   | 71/100 [01:07<00:27,  1.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 72/100 [01:07<00:26,  1.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 73/100 [01:08<00:25,  1.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▍  | 74/100 [01:09<00:24,  1.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 75%|███████▌  | 75/100 [01:10<00:23,  1.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▌  | 76/100 [01:11<00:22,  1.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 77/100 [01:12<00:21,  1.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 78/100 [01:13<00:20,  1.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 79%|███████▉  | 79/100 [01:14<00:19,  1.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|████████  | 80/100 [01:15<00:18,  1.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████  | 81/100 [01:16<00:18,  1.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▏ | 82/100 [01:17<00:17,  1.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 83/100 [01:18<00:16,  1.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 84%|████████▍ | 84/100 [01:19<00:15,  1.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▌ | 85/100 [01:20<00:14,  1.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▌ | 86/100 [01:21<00:13,  1.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 87/100 [01:22<00:12,  1.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 88/100 [01:23<00:11,  1.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 89%|████████▉ | 89/100 [01:24<00:10,  1.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 90%|█████████ | 90/100 [01:25<00:09,  1.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 91%|█████████ | 91/100 [01:25<00:08,  1.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 92/100 [01:27<00:08,  1.00s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 93/100 [01:28<00:06,  1.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▍| 94/100 [01:28<00:05,  1.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 95%|█████████▌| 95/100 [01:30<00:05,  1.00s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▌| 96/100 [01:31<00:03,  1.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 97/100 [01:31<00:02,  1.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 98/100 [01:32<00:01,  1.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▉| 99/100 [01:33<00:00,  1.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 100/100 [01:34<00:00,  1.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:1901] 2024-01-23 03:41:20,499 >> \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:1901] 2024-01-23 03:41:20,499 >> \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:{'train_runtime': 94.7066, 'train_samples_per_second': 67.577, 'train_steps_per_second': 1.056, 'train_loss': 2.125393524169922, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 100/100 [01:34<00:00,  1.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 100/100 [01:34<00:00,  1.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:2709] 2024-01-23 03:41:20,505 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:2709] 2024-01-23 03:41:20,505 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:453] 2024-01-23 03:41:20,506 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:453] 2024-01-23 03:41:20,506 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|modeling_utils.py:1704] 2024-01-23 03:41:22,189 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|modeling_utils.py:1704] 2024-01-23 03:41:22,189 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|tokenization_utils_base.py:2160] 2024-01-23 03:41:22,190 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|tokenization_utils_base.py:2160] 2024-01-23 03:41:22,190 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|tokenization_utils_base.py:2167] 2024-01-23 03:41:22,191 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|tokenization_utils_base.py:2167] 2024-01-23 03:41:22,191 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:***** train metrics *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  epoch                    =       0.07\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  train_loss               =     2.1254\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  train_runtime            = 0:01:34.70\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  train_samples            =      88524[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  train_samples_per_second =     67.577\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  train_steps_per_second   =      1.056\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:01/23/2024 03:41:22 - INFO - __main__ - *** Evaluate ***\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:710] 2024-01-23 03:41:22,235 >> The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:710] 2024-01-23 03:41:22,235 >> The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:2964] 2024-01-23 03:41:22,237 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:2964] 2024-01-23 03:41:22,237 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:2966] 2024-01-23 03:41:22,238 >>   Num examples = 10784\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:2969] 2024-01-23 03:41:22,238 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:2966] 2024-01-23 03:41:22,238 >>   Num examples = 10784\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:2969] 2024-01-23 03:41:22,238 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/169 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  2%|▏         | 4/169 [00:00<00:05, 28.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  4%|▍         | 7/169 [00:00<00:07, 23.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  6%|▌         | 10/169 [00:00<00:07, 20.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 13/169 [00:00<00:07, 20.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  9%|▉         | 16/169 [00:00<00:07, 20.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 11%|█         | 19/169 [00:00<00:07, 20.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 22/169 [00:01<00:07, 20.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▍        | 25/169 [00:01<00:07, 19.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 16%|█▌        | 27/169 [00:01<00:07, 18.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 30/169 [00:01<00:07, 19.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|█▉        | 33/169 [00:01<00:06, 19.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 21%|██▏       | 36/169 [00:01<00:06, 20.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 39/169 [00:01<00:06, 20.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 25%|██▍       | 42/169 [00:02<00:06, 20.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 45/169 [00:02<00:06, 20.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 48/169 [00:02<00:06, 20.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|███       | 51/169 [00:02<00:05, 20.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 32%|███▏      | 54/169 [00:02<00:05, 20.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▎      | 57/169 [00:02<00:05, 20.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▌      | 60/169 [00:02<00:05, 20.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 37%|███▋      | 63/169 [00:03<00:05, 20.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 39%|███▉      | 66/169 [00:03<00:05, 19.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 41%|████      | 69/169 [00:03<00:05, 19.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 72/169 [00:03<00:04, 19.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▍     | 75/169 [00:03<00:04, 20.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▌     | 78/169 [00:03<00:04, 18.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 81/169 [00:04<00:04, 19.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|████▉     | 84/169 [00:04<00:04, 19.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████     | 86/169 [00:04<00:04, 19.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▏    | 88/169 [00:04<00:04, 18.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 90/169 [00:04<00:04, 18.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▌    | 93/169 [00:04<00:03, 19.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▌    | 95/169 [00:04<00:04, 17.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▋    | 97/169 [00:04<00:04, 16.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▊    | 99/169 [00:05<00:04, 17.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|██████    | 102/169 [00:05<00:03, 18.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 105/169 [00:05<00:03, 18.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 64%|██████▍   | 108/169 [00:05<00:03, 19.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▌   | 111/169 [00:05<00:02, 20.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 114/169 [00:05<00:02, 19.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▊   | 116/169 [00:05<00:02, 19.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|███████   | 119/169 [00:06<00:02, 19.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 122/169 [00:06<00:02, 20.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▍  | 125/169 [00:06<00:02, 20.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▌  | 128/169 [00:06<00:02, 20.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 131/169 [00:06<00:01, 20.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 79%|███████▉  | 134/169 [00:06<00:01, 20.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████  | 137/169 [00:06<00:01, 21.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 140/169 [00:07<00:01, 21.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▍ | 143/169 [00:07<00:01, 21.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▋ | 146/169 [00:07<00:01, 21.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 149/169 [00:07<00:00, 21.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 90%|████████▉ | 152/169 [00:07<00:00, 21.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 155/169 [00:07<00:00, 21.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 158/169 [00:07<00:00, 21.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 95%|█████████▌| 161/169 [00:08<00:00, 21.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 164/169 [00:08<00:00, 21.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▉| 167/169 [00:08<00:00, 21.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:01/23/2024 03:41:45 - INFO - utils_qa - Post-processing 10570 example predictions split into 10784 features.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 30/10570 [00:00<00:36, 290.68it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  1%|          | 61/10570 [00:00<00:35, 299.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  1%|          | 93/10570 [00:00<00:34, 305.53it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  1%|          | 125/10570 [00:00<00:33, 310.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  1%|▏         | 157/10570 [00:00<00:33, 309.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  2%|▏         | 188/10570 [00:00<00:33, 308.86it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  2%|▏         | 219/10570 [00:00<00:34, 301.85it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  2%|▏         | 250/10570 [00:00<00:35, 291.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  3%|▎         | 280/10570 [00:01<00:43, 239.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  3%|▎         | 310/10570 [00:01<00:40, 254.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  3%|▎         | 342/10570 [00:01<00:37, 270.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  4%|▎         | 374/10570 [00:01<00:36, 281.66it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  4%|▍         | 404/10570 [00:01<00:35, 286.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  4%|▍         | 435/10570 [00:01<00:34, 291.26it/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  4%|▍         | 466/10570 [00:01<00:34, 293.94it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  5%|▍         | 497/10570 [00:01<00:33, 297.85it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  5%|▌         | 529/10570 [00:01<00:33, 302.56it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  5%|▌         | 560/10570 [00:01<00:33, 301.14it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  6%|▌         | 591/10570 [00:02<00:33, 301.74it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  6%|▌         | 622/10570 [00:02<00:32, 303.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  6%|▌         | 653/10570 [00:02<00:32, 303.92it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  6%|▋         | 685/10570 [00:02<00:32, 306.54it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  7%|▋         | 716/10570 [00:02<00:32, 303.73it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  7%|▋         | 747/10570 [00:02<00:32, 299.89it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  7%|▋         | 778/10570 [00:02<00:32, 296.86it/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 169/169 [00:25<00:00, 21.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 808/10570 [00:02<00:33, 288.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 837/10570 [00:02<00:33, 287.64it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 867/10570 [00:02<00:33, 290.89it/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 898/10570 [00:03<00:32, 293.91it/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  9%|▉         | 928/10570 [00:03<00:33, 289.80it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  9%|▉         | 958/10570 [00:03<00:32, 292.44it/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  9%|▉         | 988/10570 [00:03<00:33, 289.70it/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 10%|▉         | 1018/10570 [00:03<00:32, 289.85it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 10%|▉         | 1048/10570 [00:03<00:32, 288.66it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 10%|█         | 1078/10570 [00:03<00:32, 289.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 10%|█         | 1107/10570 [00:03<00:33, 281.66it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 11%|█         | 1137/10570 [00:03<00:32, 286.73it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 11%|█         | 1168/10570 [00:04<00:32, 292.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 11%|█▏        | 1199/10570 [00:04<00:31, 295.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▏        | 1230/10570 [00:04<00:31, 297.35it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▏        | 1260/10570 [00:04<00:31, 295.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▏        | 1291/10570 [00:04<00:31, 299.13it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 1322/10570 [00:04<00:30, 301.53it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 1353/10570 [00:04<00:30, 300.20it/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 1384/10570 [00:04<00:30, 298.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 1414/10570 [00:04<00:30, 296.68it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 14%|█▎        | 1444/10570 [00:04<00:30, 296.52it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 14%|█▍        | 1475/10570 [00:05<00:30, 297.96it/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 14%|█▍        | 1506/10570 [00:05<00:30, 300.38it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▍        | 1537/10570 [00:05<00:30, 300.74it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▍        | 1568/10570 [00:05<00:29, 301.88it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▌        | 1599/10570 [00:05<00:29, 302.72it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▌        | 1630/10570 [00:05<00:29, 302.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 16%|█▌        | 1662/10570 [00:05<00:29, 304.98it/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 16%|█▌        | 1694/10570 [00:05<00:28, 306.39it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 16%|█▋        | 1725/10570 [00:05<00:29, 304.77it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 1756/10570 [00:05<00:29, 302.76it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 1787/10570 [00:06<00:28, 304.33it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 1818/10570 [00:06<00:28, 303.77it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 1849/10570 [00:06<00:28, 302.92it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 1880/10570 [00:06<00:28, 302.63it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 1912/10570 [00:06<00:28, 306.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 1943/10570 [00:06<00:28, 305.67it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 19%|█▊        | 1975/10570 [00:06<00:27, 307.56it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 19%|█▉        | 2006/10570 [00:06<00:28, 304.87it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 19%|█▉        | 2037/10570 [00:06<00:28, 303.38it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|█▉        | 2069/10570 [00:06<00:27, 305.77it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|█▉        | 2100/10570 [00:07<00:27, 306.73it/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|██        | 2131/10570 [00:07<00:30, 275.80it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|██        | 2161/10570 [00:07<00:29, 282.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 21%|██        | 2191/10570 [00:07<00:29, 285.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 21%|██        | 2222/10570 [00:07<00:28, 291.74it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 21%|██▏       | 2252/10570 [00:07<00:28, 293.72it/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▏       | 2282/10570 [00:07<00:28, 295.19it/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▏       | 2312/10570 [00:07<00:27, 295.92it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▏       | 2342/10570 [00:07<00:27, 297.03it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▏       | 2372/10570 [00:08<00:27, 297.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 2402/10570 [00:08<00:27, 294.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 2432/10570 [00:08<00:27, 294.53it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 2462/10570 [00:08<00:27, 292.75it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▎       | 2492/10570 [00:08<00:27, 294.13it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▍       | 2522/10570 [00:08<00:29, 277.15it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▍       | 2551/10570 [00:08<00:28, 278.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▍       | 2580/10570 [00:08<00:28, 279.83it/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 25%|██▍       | 2610/10570 [00:08<00:27, 285.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 25%|██▍       | 2641/10570 [00:08<00:27, 291.03it/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 25%|██▌       | 2672/10570 [00:09<00:26, 295.41it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▌       | 2702/10570 [00:09<00:26, 295.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▌       | 2733/10570 [00:09<00:26, 298.84it/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▌       | 2765/10570 [00:09<00:25, 303.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▋       | 2797/10570 [00:09<00:25, 305.45it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 2828/10570 [00:09<00:25, 304.13it/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 2859/10570 [00:09<00:25, 301.45it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 2890/10570 [00:09<00:25, 297.98it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 2920/10570 [00:09<00:25, 296.80it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 2950/10570 [00:09<00:25, 296.92it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 2980/10570 [00:10<00:25, 296.05it/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 3010/10570 [00:10<00:25, 296.62it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 29%|██▉       | 3041/10570 [00:10<00:25, 298.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 29%|██▉       | 3071/10570 [00:10<00:25, 293.94it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 29%|██▉       | 3102/10570 [00:10<00:25, 296.05it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|██▉       | 3132/10570 [00:10<00:25, 295.21it/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|██▉       | 3162/10570 [00:10<00:25, 293.05it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|███       | 3192/10570 [00:10<00:25, 291.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|███       | 3222/10570 [00:10<00:25, 292.34it/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 31%|███       | 3252/10570 [00:11<00:25, 290.29it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 31%|███       | 3282/10570 [00:11<00:25, 291.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 31%|███▏      | 3312/10570 [00:11<00:24, 293.35it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 32%|███▏      | 3342/10570 [00:11<00:24, 294.61it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 32%|███▏      | 3372/10570 [00:11<00:24, 294.60it/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 32%|███▏      | 3402/10570 [00:11<00:24, 294.61it/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 32%|███▏      | 3432/10570 [00:11<00:24, 293.29it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 3462/10570 [00:11<00:24, 294.78it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 3492/10570 [00:11<00:24, 291.61it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 3522/10570 [00:11<00:24, 292.68it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▎      | 3552/10570 [00:12<00:23, 294.53it/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▍      | 3582/10570 [00:12<00:24, 290.50it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▍      | 3612/10570 [00:12<00:23, 290.73it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▍      | 3642/10570 [00:12<00:23, 292.38it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 35%|███▍      | 3672/10570 [00:12<00:23, 293.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 35%|███▌      | 3702/10570 [00:12<00:23, 292.53it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 35%|███▌      | 3732/10570 [00:12<00:23, 293.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▌      | 3762/10570 [00:12<00:23, 293.50it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▌      | 3792/10570 [00:12<00:22, 294.92it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▌      | 3822/10570 [00:12<00:23, 292.36it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▋      | 3852/10570 [00:13<00:22, 293.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 37%|███▋      | 3882/10570 [00:13<00:22, 294.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 37%|███▋      | 3912/10570 [00:13<00:22, 293.69it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 37%|███▋      | 3942/10570 [00:13<00:22, 293.60it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 3972/10570 [00:13<00:22, 294.66it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 4002/10570 [00:13<00:22, 294.06it/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 4032/10570 [00:13<00:22, 294.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 4063/10570 [00:13<00:21, 296.45it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 39%|███▊      | 4093/10570 [00:13<00:21, 295.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 39%|███▉      | 4123/10570 [00:13<00:22, 288.34it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 39%|███▉      | 4152/10570 [00:14<00:26, 238.31it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|███▉      | 4178/10570 [00:14<00:26, 243.42it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|███▉      | 4204/10570 [00:14<00:29, 216.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|████      | 4228/10570 [00:14<00:28, 221.97it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|████      | 4256/10570 [00:14<00:26, 236.14it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 41%|████      | 4281/10570 [00:14<00:34, 180.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 41%|████      | 4302/10570 [00:14<00:37, 168.36it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 41%|████      | 4326/10570 [00:15<00:33, 184.14it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 41%|████      | 4356/10570 [00:15<00:29, 211.98it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 41%|████▏     | 4386/10570 [00:15<00:26, 233.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 42%|████▏     | 4416/10570 [00:15<00:24, 249.30it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 42%|████▏     | 4446/10570 [00:15<00:23, 262.90it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 42%|████▏     | 4477/10570 [00:15<00:22, 274.39it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 4506/10570 [00:15<00:21, 276.35it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 4535/10570 [00:15<00:21, 275.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 4564/10570 [00:15<00:21, 278.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 4593/10570 [00:16<00:21, 279.39it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▎     | 4622/10570 [00:16<00:22, 267.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▍     | 4649/10570 [00:16<00:22, 260.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▍     | 4677/10570 [00:16<00:22, 264.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 45%|████▍     | 4708/10570 [00:16<00:21, 274.89it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 45%|████▍     | 4738/10570 [00:16<00:20, 279.96it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 45%|████▌     | 4767/10570 [00:16<00:21, 267.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 45%|████▌     | 4796/10570 [00:16<00:21, 271.36it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▌     | 4826/10570 [00:16<00:20, 278.99it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▌     | 4855/10570 [00:17<00:22, 259.49it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▌     | 4885/10570 [00:17<00:21, 269.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▋     | 4914/10570 [00:17<00:20, 274.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 4943/10570 [00:17<00:20, 277.64it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 4971/10570 [00:17<00:20, 277.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 4999/10570 [00:17<00:20, 276.72it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 5028/10570 [00:17<00:19, 278.16it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 5058/10570 [00:17<00:19, 282.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 5087/10570 [00:17<00:19, 281.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 5117/10570 [00:17<00:19, 285.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 49%|████▊     | 5146/10570 [00:18<00:18, 285.91it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 49%|████▉     | 5175/10570 [00:18<00:18, 285.60it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 49%|████▉     | 5204/10570 [00:18<00:18, 285.84it/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|████▉     | 5233/10570 [00:18<00:18, 286.66it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|████▉     | 5262/10570 [00:18<00:18, 285.17it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|█████     | 5292/10570 [00:18<00:18, 288.29it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|█████     | 5322/10570 [00:18<00:18, 289.93it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████     | 5351/10570 [00:18<00:18, 287.70it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████     | 5380/10570 [00:18<00:17, 288.34it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████     | 5410/10570 [00:18<00:17, 289.68it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████▏    | 5439/10570 [00:19<00:17, 289.55it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▏    | 5468/10570 [00:19<00:18, 269.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▏    | 5496/10570 [00:19<00:19, 260.67it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▏    | 5524/10570 [00:19<00:18, 266.05it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 5553/10570 [00:19<00:18, 272.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 5582/10570 [00:19<00:18, 276.11it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 5612/10570 [00:19<00:17, 280.16it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 5641/10570 [00:19<00:17, 280.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▎    | 5670/10570 [00:19<00:18, 269.81it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▍    | 5701/10570 [00:20<00:17, 278.73it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▍    | 5730/10570 [00:20<00:17, 281.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▍    | 5759/10570 [00:20<00:16, 283.80it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▍    | 5788/10570 [00:20<00:16, 282.82it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▌    | 5817/10570 [00:20<00:16, 284.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▌    | 5846/10570 [00:20<00:16, 285.42it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▌    | 5875/10570 [00:20<00:16, 286.29it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▌    | 5905/10570 [00:20<00:16, 288.00it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▌    | 5934/10570 [00:20<00:16, 287.51it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▋    | 5964/10570 [00:20<00:15, 289.77it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▋    | 5994/10570 [00:21<00:15, 290.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▋    | 6024/10570 [00:21<00:15, 289.13it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▋    | 6053/10570 [00:21<00:15, 286.43it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 58%|█████▊    | 6082/10570 [00:21<00:15, 282.70it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 58%|█████▊    | 6111/10570 [00:21<00:15, 284.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 58%|█████▊    | 6141/10570 [00:21<00:15, 287.13it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 58%|█████▊    | 6171/10570 [00:21<00:15, 288.56it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▊    | 6200/10570 [00:21<00:15, 288.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▉    | 6229/10570 [00:21<00:15, 274.39it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▉    | 6258/10570 [00:21<00:15, 278.17it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▉    | 6288/10570 [00:22<00:15, 283.55it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|█████▉    | 6318/10570 [00:22<00:14, 286.52it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|██████    | 6347/10570 [00:22<00:14, 286.52it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|██████    | 6376/10570 [00:22<00:15, 278.42it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████    | 6404/10570 [00:22<00:15, 277.38it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████    | 6434/10570 [00:22<00:14, 283.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████    | 6465/10570 [00:22<00:14, 288.59it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████▏   | 6494/10570 [00:22<00:14, 288.51it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 6523/10570 [00:22<00:14, 288.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 6553/10570 [00:22<00:13, 290.12it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 6583/10570 [00:23<00:13, 289.88it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 63%|██████▎   | 6612/10570 [00:23<00:13, 287.53it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 63%|██████▎   | 6642/10570 [00:23<00:13, 290.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 63%|██████▎   | 6672/10570 [00:23<00:13, 290.29it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 63%|██████▎   | 6702/10570 [00:23<00:13, 287.77it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 64%|██████▎   | 6731/10570 [00:23<00:13, 286.77it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 64%|██████▍   | 6761/10570 [00:23<00:13, 289.96it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 64%|██████▍   | 6791/10570 [00:23<00:12, 290.96it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 65%|██████▍   | 6821/10570 [00:23<00:13, 287.99it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 65%|██████▍   | 6850/10570 [00:24<00:12, 287.36it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 65%|██████▌   | 6879/10570 [00:24<00:12, 285.68it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 65%|██████▌   | 6908/10570 [00:24<00:12, 284.99it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▌   | 6939/10570 [00:24<00:12, 289.91it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▌   | 6969/10570 [00:24<00:12, 291.33it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▌   | 6999/10570 [00:24<00:12, 293.42it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▋   | 7029/10570 [00:24<00:12, 294.54it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 7059/10570 [00:24<00:11, 293.33it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 7089/10570 [00:24<00:11, 293.65it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 7119/10570 [00:24<00:11, 295.36it/s][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 68%|██████▊   | 7149/10570 [00:25<00:11, 296.41it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 68%|██████▊   | 7179/10570 [00:25<00:11, 294.88it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 68%|██████▊   | 7209/10570 [00:25<00:11, 291.86it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 68%|██████▊   | 7239/10570 [00:25<00:11, 291.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▉   | 7269/10570 [00:25<00:11, 290.56it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▉   | 7299/10570 [00:25<00:11, 291.89it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▉   | 7329/10570 [00:25<00:11, 291.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|██████▉   | 7359/10570 [00:25<00:12, 264.93it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|██████▉   | 7388/10570 [00:25<00:11, 270.05it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|███████   | 7418/10570 [00:26<00:11, 277.78it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|███████   | 7448/10570 [00:26<00:11, 281.53it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 71%|███████   | 7477/10570 [00:26<00:10, 283.94it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 71%|███████   | 7507/10570 [00:26<00:10, 286.16it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 71%|███████▏  | 7537/10570 [00:26<00:10, 288.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 7566/10570 [00:26<00:10, 288.98it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 7596/10570 [00:26<00:10, 291.00it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 7626/10570 [00:26<00:10, 291.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 7656/10570 [00:26<00:10, 282.89it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 7685/10570 [00:26<00:10, 281.92it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 7715/10570 [00:27<00:10, 284.60it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 7744/10570 [00:27<00:09, 285.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▎  | 7773/10570 [00:27<00:10, 273.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▍  | 7802/10570 [00:27<00:10, 276.75it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▍  | 7831/10570 [00:27<00:09, 278.65it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▍  | 7861/10570 [00:27<00:09, 282.78it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 75%|███████▍  | 7891/10570 [00:27<00:09, 285.65it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 75%|███████▍  | 7921/10570 [00:27<00:09, 289.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 75%|███████▌  | 7951/10570 [00:27<00:09, 290.65it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▌  | 7981/10570 [00:27<00:08, 288.78it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▌  | 8010/10570 [00:28<00:08, 288.49it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▌  | 8039/10570 [00:28<00:08, 288.67it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▋  | 8068/10570 [00:28<00:08, 286.29it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 8098/10570 [00:28<00:08, 289.92it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 8128/10570 [00:28<00:08, 291.66it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 8158/10570 [00:28<00:08, 286.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 8187/10570 [00:28<00:08, 286.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 8216/10570 [00:28<00:08, 283.02it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 8245/10570 [00:28<00:08, 283.30it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 8274/10570 [00:29<00:08, 278.31it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 79%|███████▊  | 8302/10570 [00:29<00:08, 269.73it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 79%|███████▉  | 8331/10570 [00:29<00:08, 273.70it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 79%|███████▉  | 8361/10570 [00:29<00:07, 279.55it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 79%|███████▉  | 8391/10570 [00:29<00:07, 284.00it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|███████▉  | 8421/10570 [00:29<00:07, 287.66it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|███████▉  | 8451/10570 [00:29<00:07, 291.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|████████  | 8481/10570 [00:29<00:07, 292.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████  | 8511/10570 [00:29<00:07, 291.64it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████  | 8541/10570 [00:29<00:06, 293.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████  | 8571/10570 [00:30<00:06, 291.68it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████▏ | 8601/10570 [00:30<00:06, 291.70it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▏ | 8631/10570 [00:30<00:06, 292.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▏ | 8661/10570 [00:30<00:06, 289.86it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▏ | 8691/10570 [00:30<00:06, 291.70it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 8721/10570 [00:30<00:06, 289.95it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 8751/10570 [00:30<00:06, 288.50it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 8780/10570 [00:30<00:06, 285.86it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 8809/10570 [00:30<00:06, 286.59it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 84%|████████▎ | 8838/10570 [00:30<00:06, 287.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 84%|████████▍ | 8868/10570 [00:31<00:05, 288.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 84%|████████▍ | 8897/10570 [00:31<00:05, 287.98it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 84%|████████▍ | 8927/10570 [00:31<00:05, 288.52it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▍ | 8956/10570 [00:31<00:05, 288.67it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▌ | 8986/10570 [00:31<00:05, 289.11it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▌ | 9016/10570 [00:31<00:05, 290.75it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▌ | 9046/10570 [00:31<00:05, 289.54it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▌ | 9076/10570 [00:31<00:05, 290.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▌ | 9106/10570 [00:31<00:05, 289.76it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▋ | 9136/10570 [00:31<00:04, 291.33it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 9166/10570 [00:32<00:04, 289.92it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 9196/10570 [00:32<00:04, 290.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 9226/10570 [00:32<00:04, 290.82it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 9256/10570 [00:32<00:04, 292.50it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 9286/10570 [00:32<00:04, 291.78it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 9316/10570 [00:32<00:04, 290.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 9346/10570 [00:32<00:04, 290.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 89%|████████▊ | 9376/10570 [00:32<00:04, 290.79it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 89%|████████▉ | 9406/10570 [00:32<00:04, 290.34it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 89%|████████▉ | 9436/10570 [00:33<00:03, 290.55it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 90%|████████▉ | 9466/10570 [00:33<00:03, 293.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 90%|████████▉ | 9496/10570 [00:33<00:03, 292.29it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 90%|█████████ | 9526/10570 [00:33<00:03, 293.63it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 90%|█████████ | 9556/10570 [00:33<00:03, 294.60it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 91%|█████████ | 9586/10570 [00:33<00:03, 293.56it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 91%|█████████ | 9616/10570 [00:33<00:03, 294.55it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 91%|█████████▏| 9646/10570 [00:33<00:03, 293.54it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 9676/10570 [00:33<00:03, 293.42it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 9706/10570 [00:33<00:02, 295.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 9736/10570 [00:34<00:02, 296.39it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 9766/10570 [00:34<00:02, 295.85it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 9796/10570 [00:34<00:02, 293.15it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 9826/10570 [00:34<00:02, 293.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 9856/10570 [00:34<00:02, 289.67it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▎| 9886/10570 [00:34<00:02, 290.63it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▍| 9916/10570 [00:34<00:02, 289.96it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▍| 9946/10570 [00:34<00:02, 292.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▍| 9976/10570 [00:34<00:02, 290.00it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 95%|█████████▍| 10006/10570 [00:34<00:01, 290.54it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 95%|█████████▍| 10036/10570 [00:35<00:01, 292.40it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 95%|█████████▌| 10066/10570 [00:35<00:01, 291.67it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▌| 10096/10570 [00:35<00:01, 291.59it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▌| 10126/10570 [00:35<00:01, 293.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▌| 10156/10570 [00:35<00:01, 292.91it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▋| 10186/10570 [00:35<00:01, 282.03it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 10216/10570 [00:35<00:01, 284.79it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 10246/10570 [00:35<00:01, 287.71it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 10275/10570 [00:35<00:01, 287.61it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 10305/10570 [00:36<00:00, 288.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 10335/10570 [00:36<00:00, 289.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 10364/10570 [00:36<00:00, 289.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 10393/10570 [00:36<00:00, 287.83it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▊| 10422/10570 [00:36<00:00, 287.97it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▉| 10451/10570 [00:36<00:00, 287.79it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▉| 10480/10570 [00:36<00:00, 285.11it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▉| 10510/10570 [00:36<00:00, 287.92it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|█████████▉| 10540/10570 [00:36<00:00, 290.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 10570/10570 [00:36<00:00, 292.61it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 10570/10570 [00:36<00:00, 286.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:01/23/2024 03:42:22 - INFO - utils_qa - Saving predictions to /opt/ml/model/eval_predictions.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:01/23/2024 03:42:22 - INFO - utils_qa - Saving nbest_preds to /opt/ml/model/eval_nbest_predictions.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 169/169 [01:05<00:00,  2.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:***** eval metrics *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  epoch                   =       0.07\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  eval_exact_match        =    76.0265\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  eval_f1                 =    85.2132\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  eval_runtime            = 0:00:08.49\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  eval_samples            =      10784\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  eval_samples_per_second =   1270.019\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  eval_steps_per_second   =     19.903\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|modelcard.py:449] 2024-01-23 03:42:27,538 >> Dropping the following result as it does not have all the necessary fields:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad', 'type': 'squad', 'config': 'plain_text', 'split': 'validation', 'args': 'plain_text'}}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|modelcard.py:449] 2024-01-23 03:42:27,538 >> Dropping the following result as it does not have all the necessary fields:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad', 'type': 'squad', 'config': 'plain_text', 'split': 'validation', 'args': 'plain_text'}}\u001b[0m\n",
      "\u001b[34m2024-01-23 03:42:36,254 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-01-23 03:42:36,254 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-01-23 03:42:36,255 sagemaker-training-toolkit INFO     Begin writing status file from leader node to worker nodes\u001b[0m\n",
      "\u001b[34m2024-01-23 03:42:36,255 sagemaker-training-toolkit INFO     Start writing mpirun finished status to algo-2\u001b[0m\n",
      "\u001b[34m2024-01-23 03:42:36,412 sagemaker-training-toolkit INFO     output from subprocess run CompletedProcess(args=['ssh', 'algo-2', 'touch', '/tmp/done.algo-1'], returncode=0, stdout='', stderr='')\u001b[0m\n",
      "\u001b[34m2024-01-23 03:42:36,412 sagemaker-training-toolkit INFO     Finished writing status file\u001b[0m\n",
      "\u001b[35m2024-01-23 03:42:36,287 sagemaker-training-toolkit INFO     Invoked on_terminate from psutil.wait_for_procs\u001b[0m\n",
      "\u001b[35m2024-01-23 03:42:36,287 sagemaker-training-toolkit INFO     process psutil.Process(pid=64, name='orted', status='terminated', started='03:37:14') terminated with exit code None\u001b[0m\n",
      "\u001b[35m2024-01-23 03:42:36,287 sagemaker-training-toolkit INFO     Reporting status for ORTEd process. gone: [psutil.Process(pid=64, name='orted', status='terminated', started='03:37:14')] alive: []\u001b[0m\n",
      "\u001b[35m2024-01-23 03:42:36,287 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\n",
      "2024-01-23 03:43:15 Uploading - Uploading generated training model\u001b[34m2024-01-23 03:43:06,442 sagemaker-training-toolkit INFO     Finished writing status file from leader node to worker nodes\u001b[0m\n",
      "\u001b[34m2024-01-23 03:43:06,443 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[35m2024-01-23 03:43:06,312 sagemaker-training-toolkit INFO     Begin looking for status file on algo-2\u001b[0m\n",
      "\u001b[35m2024-01-23 03:43:06,312 sagemaker-training-toolkit INFO     MPI training job status file found. Exit gracefully\u001b[0m\n",
      "\u001b[35m2024-01-23 03:43:06,312 sagemaker-training-toolkit INFO     End looking for status file\u001b[0m\n",
      "\u001b[35m2024-01-23 03:43:06,312 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[35m2024-01-23 03:43:06,312 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-01-23 03:43:31 Completed - Training job completed\n",
      "Training seconds: 1342\n",
      "Billable seconds: 1342\n"
     ]
    }
   ],
   "source": [
    "# estimator\n",
    "huggingface_estimator = HuggingFace(entry_point='run_qa.py',\n",
    "                                    source_dir='./examples/pytorch/question-answering',\n",
    "                                    git_config=git_config,\n",
    "                                    metric_definitions=metric_definitions,\n",
    "                                    instance_type=instance_type,\n",
    "                                    instance_count=instance_count,\n",
    "                                    volume_size=volume_size,\n",
    "                                    role=role,\n",
    "                                    transformers_version='4.26.0',\n",
    "                                    pytorch_version='1.13.1',\n",
    "                                    py_version='py39',\n",
    "                                    distribution= distribution,\n",
    "                                    hyperparameters = hyperparameters)\n",
    "\n",
    "# starting the train job\n",
    "huggingface_estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01d3e3b-9552-48e5-a534-179444934148",
   "metadata": {},
   "source": [
    "## Deploying the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8011d4f-003c-4fc2-bd82-20df2107aeb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-training-2024-01-23-03-45-29-539\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-pytorch-training-2024-01-23-03-45-29-539\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-pytorch-training-2024-01-23-03-45-29-539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "predictor = huggingface_estimator.deploy(1,\"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "862d0dea-6bb0-4c30-be97-41b42d1763c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9702168703079224, 'start': 68, 'end': 77, 'answer': 'sagemaker'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "\"inputs\": {\n",
    "\t\"question\": \"What is used for inference?\",\n",
    "\t\"context\": \"My Name is Philipp and I live in Nuremberg. This model is used with sagemaker for inference.\"\n",
    "\t}\n",
    "}\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97fa37c4-cf98-4224-8213-e6f9c74ac3be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: huggingface-pytorch-training-2024-01-23-03-45-29-539\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: huggingface-pytorch-training-2024-01-23-03-45-29-539\n",
      "INFO:sagemaker:Deleting endpoint with name: huggingface-pytorch-training-2024-01-23-03-45-29-539\n"
     ]
    }
   ],
   "source": [
    "# Delete the endpoints\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f585cde3-3473-49e0-863b-2514b7a3589f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d6962-a71a-4bf1-968b-15f5372d7e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ed38b2-c6e0-4665-a6a3-72a684933692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b257f705-b679-4a1a-a9fc-a525cf73a943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6673f03-923e-4fff-8261-ffdad108fb5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
