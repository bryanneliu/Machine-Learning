{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a74eed27-4462-4460-8ed9-45ff7ce89314",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Distributed Seq2Seq-transformer model on summarization\n",
    "As distributed training strategy we are going to use SageMaker Data Parallelism, which has been built into the Trainer API. To use data-parallelism we only have to define the distribution parameter in our HuggingFace estimator.\n",
    "\n",
    "```python\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}\n",
    "```\n",
    "\n",
    "## Model and Dataset\n",
    "We are going to fine-tune facebook/bart-base (https://huggingface.co/facebook/bart-base) on the samsum dataset. \"BART is sequence-to-sequence model trained with denoising as pretraining objective.\"\n",
    "\n",
    "The samsum dataset contains about 16k messenger-like conversations with summaries.\n",
    "```python\n",
    "{'id': '13818513',\n",
    " 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.',\n",
    " 'dialogue': \"Amanda: I baked cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\"}\n",
    "```\n",
    "\n",
    "## Download the model from S3 and unzip it\n",
    "```python\n",
    "import os\n",
    "import tarfile\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "local_path = 'my_bart_model'\n",
    "\n",
    "os.makedirs(local_path, exist_ok = True)\n",
    "\n",
    "# download model from S3\n",
    "S3Downloader.download(\n",
    "    s3_uri=huggingface_estimator.model_data, # s3 uri where the trained model is located\n",
    "    local_path=local_path, # local path where *.targ.gz is saved\n",
    "    sagemaker_session=sess # sagemaker session used for training the model\n",
    ")\n",
    "\n",
    "# unzip model\n",
    "tar = tarfile.open(f\"{local_path}/model.tar.gz\", \"r:gz\")\n",
    "tar.extractall(path=local_path)\n",
    "tar.close()\n",
    "os.remove(f\"{local_path}/model.tar.gz\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7aaa135-86a2-45c6-8201-44118ba32e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker>=2.48.0 in /opt/conda/lib/python3.10/site-packages (2.204.0)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.33.3 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (1.33.9)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (1.26.2)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (4.25.1)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (4.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (21.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (2.1.3)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (0.3.0)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from sagemaker>=2.48.0) (6.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (4.20.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (2.5.2)\n",
      "Requirement already satisfied: tblib<3,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (1.7.0)\n",
      "Requirement already satisfied: urllib3<1.27 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (1.26.18)\n",
      "Requirement already satisfied: uvicorn==0.22.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (0.22.0)\n",
      "Requirement already satisfied: fastapi==0.95.2 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (0.95.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (2.31.0)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (6.1.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (4.64.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.48.0) (5.9.0)\n",
      "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from fastapi==0.95.2->sagemaker>=2.48.0) (1.10.13)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi==0.95.2->sagemaker>=2.48.0) (0.27.0)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn==0.22.0->sagemaker>=2.48.0) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn==0.22.0->sagemaker>=2.48.0) (0.14.0)\n",
      "Requirement already satisfied: botocore<1.34.0,>=1.33.9 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker>=2.48.0) (1.33.9)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker>=2.48.0) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.9.0,>=0.8.2 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker>=2.48.0) (0.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker>=2.48.0) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->sagemaker>=2.48.0) (3.0.9)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from docker->sagemaker>=2.48.0) (0.58.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker>=2.48.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker>=2.48.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker>=2.48.0) (2023.11.17)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-pasta->sagemaker>=2.48.0) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker>=2.48.0) (2023.11.2)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker>=2.48.0) (0.31.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker>=2.48.0) (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker>=2.48.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker>=2.48.0) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker>=2.48.0) (2023.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2.48.0) (1.7.6.7)\n",
      "Requirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2.48.0) (0.3.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2.48.0) (0.3.3)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2.48.0) (0.70.14)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker>=2.48.0) (21.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi==0.95.2->sagemaker>=2.48.0) (4.3.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker>=2.48.0) (3.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker>=2.48.0) (1.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "/bin/bash: line 1: sudo: command not found\n",
      "/bin/bash: line 1: sudo: command not found\n",
      "git: 'lfs' is not a git command. See 'git --help'.\n",
      "\n",
      "The most similar command is\n",
      "\tlog\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.48.0\"  --upgrade\n",
    "\n",
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash\n",
    "!sudo yum install git-lfs -y\n",
    "!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b4dadf2-faf2-41ce-b939-467ed4ec7da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::802575742115:role/service-role/AmazonSageMaker-ExecutionRole-20230929T143152\n",
      "sagemaker bucket: sagemaker-us-east-1-802575742115\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5babab-f867-4a71-8589-bbd20d0e5d4d",
   "metadata": {},
   "source": [
    "## Configure distributed training and hyperparameters\n",
    "\n",
    "Since the HuggingFace Estimator has git support built-in, we can specify a training script that is stored in a GitHub repository as entry_point and source_dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f3739f3-4694-474c-95ba-c13ef8227f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'per_device_train_batch_size': 4,\n",
    "                 'per_device_eval_batch_size': 4,\n",
    "                 'model_name_or_path': 'facebook/bart-large-cnn',\n",
    "                 'dataset_name': 'samsum',\n",
    "                 'do_train': True,\n",
    "                 'do_eval': True,\n",
    "                 'do_predict': True,\n",
    "                 'predict_with_generate': True,\n",
    "                 'output_dir': '/opt/ml/model',\n",
    "                 'num_train_epochs': 3,\n",
    "                 'learning_rate': 5e-5,\n",
    "                 'seed': 7,\n",
    "                 'fp16': True,\n",
    "                 }\n",
    "\n",
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.26.0'} \n",
    "\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deea62e-d818-4109-aa62-e96ede2d24d5",
   "metadata": {},
   "source": [
    "## Create a HuggingFace estimator and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "809d3fe0-8234-41ca-916d-0d95ec0c23cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "      entry_point='run_summarization.py', # script\n",
    "      source_dir='./examples/pytorch/summarization', # relative path to example\n",
    "      git_config=git_config,\n",
    "      instance_type='ml.p3dn.24xlarge',\n",
    "      instance_count=2,\n",
    "      transformers_version='4.26.0',\n",
    "      pytorch_version='1.13.1',\n",
    "      py_version='py39',\n",
    "      role=role,\n",
    "      hyperparameters = hyperparameters,\n",
    "      distribution = distribution\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c571c95-e927-4437-b2db-b3ccaed383e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '/tmp/tmpws5m1iz3'...\n",
      "Note: switching to 'v4.26.0'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n",
      "HEAD is now at 820c46a70 Hotifx remove tuple for git config image processor. (#21278)\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2024-01-24-01-02-41-566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-24 01:03:05 Starting - Starting the training job...\n",
      "2024-01-24 01:03:21 Starting - Preparing the instances for training...............\n",
      "2024-01-24 01:05:52 Downloading - Downloading input data...\n",
      "2024-01-24 01:06:12 Downloading - Downloading the training image.....................\n",
      "2024-01-24 01:09:53 Training - Training image download completed. Training in progress.....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-01-24 01:10:33,384 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-01-24 01:10:33,444 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-24 01:10:33,454 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-01-24 01:10:33,457 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[34m2024-01-24 01:10:33,457 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-01-24 01:10:33,780 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate>=0.12.0 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (0.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=1.8.0 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (0.1.97)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (3.20.2)\u001b[0m\n",
      "\u001b[34mCollecting rouge-score\u001b[0m\n",
      "\u001b[34mDownloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:33,270 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:33,329 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:33,340 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:33,342 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:33,342 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:33,590 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: accelerate>=0.12.0 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (0.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: datasets>=1.8.0 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (2.9.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (0.1.97)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (3.20.2)\u001b[0m\n",
      "\u001b[35mCollecting rouge-score\u001b[0m\n",
      "\u001b[35mDownloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting nltk\u001b[0m\n",
      "\u001b[35mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 35.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting py7zr\u001b[0m\n",
      "\u001b[35mDownloading py7zr-0.20.8-py3-none-any.whl (67 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.0/67.0 kB 21.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[35mCollecting evaluate\u001b[0m\n",
      "\u001b[35mDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 26.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (23.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.9.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.3.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2023.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.12.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.18.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2.28.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (1.5.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.70.14)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (11.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.8.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (4.64.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.2.0)\u001b[0m\n",
      "\u001b[35mCollecting absl-py\u001b[0m\n",
      "\u001b[35mDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.7/133.7 kB 39.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge-score->-r requirements.txt (line 5)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting nltk\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 32.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting py7zr\u001b[0m\n",
      "\u001b[34mDownloading py7zr-0.20.8-py3-none-any.whl (67 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.0/67.0 kB 18.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mCollecting evaluate\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 28.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2023.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (1.5.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (11.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2.28.2)\u001b[0m\n",
      "\u001b[34mCollecting absl-py\u001b[0m\n",
      "\u001b[34mDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.7/133.7 kB 40.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge-score->-r requirements.txt (line 5)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 6)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 6)) (2022.10.31)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 6)) (8.1.2)\u001b[0m\n",
      "\u001b[34mCollecting pyppmd<1.2.0,>=1.1.0\u001b[0m\n",
      "\u001b[34mDownloading pyppmd-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.8/138.8 kB 40.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pybcj<1.1.0,>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading pybcj-1.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.5/49.5 kB 16.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting inflate64<1.1.0,>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading inflate64-1.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (92 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.0/93.0 kB 30.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting texttable\u001b[0m\n",
      "\u001b[34mDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyzstd>=0.15.9\u001b[0m\n",
      "\u001b[34mDownloading pyzstd-0.15.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 412.3/412.3 kB 70.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 6)) (8.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 6)) (2022.10.31)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 6)) (1.2.0)\u001b[0m\n",
      "\u001b[35mCollecting pybcj<1.1.0,>=1.0.0\u001b[0m\n",
      "\u001b[35mDownloading pybcj-1.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.5/49.5 kB 16.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting pyzstd>=0.15.9\u001b[0m\n",
      "\u001b[35mDownloading pyzstd-0.15.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 412.3/412.3 kB 73.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting multivolumefile>=0.2.3\u001b[0m\n",
      "\u001b[35mDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[35mCollecting brotli>=1.1.0\u001b[0m\n",
      "\u001b[35mDownloading Brotli-1.1.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.8 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.8/2.8 MB 106.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting texttable\u001b[0m\n",
      "\u001b[35mDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[35mCollecting pyppmd<1.2.0,>=1.1.0\u001b[0m\n",
      "\u001b[35mDownloading pyppmd-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.8/138.8 kB 43.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting pycryptodomex>=3.16.0\u001b[0m\n",
      "\u001b[35mDownloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 88.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting inflate64<1.1.0,>=1.0.0\u001b[0m\n",
      "\u001b[35mDownloading inflate64-1.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (92 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.0/93.0 kB 31.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.3->-r requirements.txt (line 8)) (4.4.0)\u001b[0m\n",
      "\u001b[34mCollecting pycryptodomex>=3.16.0\u001b[0m\n",
      "\u001b[34mDownloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 92.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting brotli>=1.1.0\u001b[0m\n",
      "\u001b[34mDownloading Brotli-1.1.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.8/2.8 MB 100.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting multivolumefile>=0.2.3\u001b[0m\n",
      "\u001b[34mDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.3->-r requirements.txt (line 8)) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2022.7.1)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: rouge-score\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge-score (setup.py): started\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (22.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (6.0.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (2.1.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (4.0.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.3.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.9.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2022.12.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (1.26.14)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2022.7.1)\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: rouge-score\u001b[0m\n",
      "\u001b[35mBuilding wheel for rouge-score (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for rouge-score (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=41a269eaa9d678ecb3beb28f98d6ca5ec90108565bed34d8c0efc505abca9a8d\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\u001b[0m\n",
      "\u001b[35mSuccessfully built rouge-score\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge-score (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=41a269eaa9d678ecb3beb28f98d6ca5ec90108565bed34d8c0efc505abca9a8d\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\u001b[0m\n",
      "\u001b[34mSuccessfully built rouge-score\u001b[0m\n",
      "\u001b[34mInstalling collected packages: texttable, brotli, pyzstd, pyppmd, pycryptodomex, pybcj, nltk, multivolumefile, inflate64, absl-py, rouge-score, py7zr, evaluate\u001b[0m\n",
      "\u001b[35mInstalling collected packages: texttable, brotli, pyzstd, pyppmd, pycryptodomex, pybcj, nltk, multivolumefile, inflate64, absl-py, rouge-score, py7zr, evaluate\u001b[0m\n",
      "\u001b[35mSuccessfully installed absl-py-2.1.0 brotli-1.1.0 evaluate-0.4.1 inflate64-1.0.0 multivolumefile-0.2.3 nltk-3.8.1 py7zr-0.20.8 pybcj-1.0.2 pycryptodomex-3.20.0 pyppmd-1.1.0 pyzstd-0.15.9 rouge-score-0.1.2 texttable-1.7.0\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m[notice] A new release of pip is available: 23.0 -> 23.3.2\u001b[0m\n",
      "\u001b[35m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-2.1.0 brotli-1.1.0 evaluate-0.4.1 inflate64-1.0.0 multivolumefile-0.2.3 nltk-3.8.1 py7zr-0.20.8 pybcj-1.0.2 pycryptodomex-3.20.0 pyppmd-1.1.0 pyzstd-0.15.9 rouge-score-0.1.2 texttable-1.7.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.3.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-01-24 01:10:40,162 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-01-24 01:10:40,162 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-01-24 01:10:40,250 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-24 01:10:40,322 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-24 01:10:40,334 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2024-01-24 01:10:40,334 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[34m2024-01-24 01:10:40,359 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[34m2024-01-24 01:10:40,505 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2024-01-24 01:10:40,506 sagemaker-training-toolkit INFO     Can connect to host algo-2\u001b[0m\n",
      "\u001b[34m2024-01-24 01:10:40,506 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2024-01-24 01:10:40,506 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[34m2024-01-24 01:10:40,510 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:39,897 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:39,898 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:39,984 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:40,056 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:40,067 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:40,067 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:40,069 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:40,082 sagemaker-training-toolkit INFO     Cannot connect to host algo-1 at port 22. Retrying...\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:40,082 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:41,122 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:41,268 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:41,268 sagemaker-training-toolkit INFO     Can connect to host algo-1 at port 22\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:41,269 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:41,269 sagemaker-training-toolkit INFO     Worker algo-1 available for communication\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:41,269 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:41,269 sagemaker-training-toolkit INFO     Host: ['algo-2', 'algo-1']\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:41,270 sagemaker-training-toolkit INFO     sagemaker_communication_backend: None\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:41,270 sagemaker-training-toolkit WARNING  Missing library /opt/conda/lib/libsmddp.so for SMDDP collective\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:41,270 sagemaker-training-toolkit WARNING  The system is not configured to run SMDDP collectives optimizedfor AWS infrastructure.Please use the latest SageMaker Deep Learning Container (DLC) to enable SMDDP Collectives support.\u001b[0m\n",
      "\u001b[35mContinuing model training with default NCCL communication backend.\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:41,270 sagemaker-training-toolkit INFO     instance type: ml.p3dn.24xlarge\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:41,270 sagemaker-training-toolkit INFO     Env Hosts: ['algo-2', 'algo-1'] Hosts: ['algo-2:8', 'algo-1:8'] process_per_hosts: 8 num_processes: 16\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:41,335 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:41,346 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": true,\n",
      "        \"sagemaker_instance_type\": \"ml.p3dn.24xlarge\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3dn.24xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dataset_name\": \"samsum\",\n",
      "        \"do_eval\": true,\n",
      "        \"do_predict\": true,\n",
      "        \"do_train\": true,\n",
      "        \"fp16\": true,\n",
      "        \"learning_rate\": 5e-05,\n",
      "        \"model_name_or_path\": \"facebook/bart-large-cnn\",\n",
      "        \"num_train_epochs\": 3,\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"per_device_eval_batch_size\": 4,\n",
      "        \"per_device_train_batch_size\": 4,\n",
      "        \"predict_with_generate\": true,\n",
      "        \"seed\": 7\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3dn.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2024-01-24-01-02-41-566\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-2\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-802575742115/huggingface-pytorch-training-2024-01-24-01-02-41-566/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_summarization\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.p3dn.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3dn.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_summarization.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"dataset_name\":\"samsum\",\"do_eval\":true,\"do_predict\":true,\"do_train\":true,\"fp16\":true,\"learning_rate\":5e-05,\"model_name_or_path\":\"facebook/bart-large-cnn\",\"num_train_epochs\":3,\"output_dir\":\"/opt/ml/model\",\"per_device_eval_batch_size\":4,\"per_device_train_batch_size\":4,\"predict_with_generate\":true,\"seed\":7}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=run_summarization.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3dn.24xlarge\"}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p3dn.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3dn.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.p3dn.24xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-1\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3dn.24xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=run_summarization\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-802575742115/huggingface-pytorch-training-2024-01-24-01-02-41-566/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3dn.24xlarge\"},\"channel_input_dirs\":{},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.p3dn.24xlarge\",\"distribution_hosts\":[\"algo-2\",\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"dataset_name\":\"samsum\",\"do_eval\":true,\"do_predict\":true,\"do_train\":true,\"fp16\":true,\"learning_rate\":5e-05,\"model_name_or_path\":\"facebook/bart-large-cnn\",\"num_train_epochs\":3,\"output_dir\":\"/opt/ml/model\",\"per_device_eval_batch_size\":4,\"per_device_train_batch_size\":4,\"predict_with_generate\":true,\"seed\":7},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3dn.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2024-01-24-01-02-41-566\",\"log_level\":20,\"master_hostname\":\"algo-2\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-802575742115/huggingface-pytorch-training-2024-01-24-01-02-41-566/source/sourcedir.tar.gz\",\"module_name\":\"run_summarization\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p3dn.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3dn.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_summarization.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--dataset_name\",\"samsum\",\"--do_eval\",\"True\",\"--do_predict\",\"True\",\"--do_train\",\"True\",\"--fp16\",\"True\",\"--learning_rate\",\"5e-05\",\"--model_name_or_path\",\"facebook/bart-large-cnn\",\"--num_train_epochs\",\"3\",\"--output_dir\",\"/opt/ml/model\",\"--per_device_eval_batch_size\",\"4\",\"--per_device_train_batch_size\",\"4\",\"--predict_with_generate\",\"True\",\"--seed\",\"7\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_HP_DATASET_NAME=samsum\u001b[0m\n",
      "\u001b[35mSM_HP_DO_EVAL=true\u001b[0m\n",
      "\u001b[35mSM_HP_DO_PREDICT=true\u001b[0m\n",
      "\u001b[35mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[35mSM_HP_FP16=true\u001b[0m\n",
      "\u001b[35mSM_HP_LEARNING_RATE=5e-05\u001b[0m\n",
      "\u001b[35mSM_HP_MODEL_NAME_OR_PATH=facebook/bart-large-cnn\u001b[0m\n",
      "\u001b[35mSM_HP_NUM_TRAIN_EPOCHS=3\u001b[0m\n",
      "\u001b[35mSM_HP_OUTPUT_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[35mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[35mSM_HP_PREDICT_WITH_GENERATE=true\u001b[0m\n",
      "\u001b[35mSM_HP_SEED=7\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35mmpirun --host algo-2:8,algo-1:8 -np 16 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 2 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_HOMOGENEOUS=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.9/site-packages/gethostname.cpython-39-x86_64-linux-gnu.so -x NCCL_PROTO=simple -x SMDATAPARALLEL_SERVER_ADDR=algo-2 -x SMDATAPARALLEL_SERVER_PORT=7592 -x SAGEMAKER_INSTANCE_TYPE=ml.p3dn.24xlarge -x USE_SMDDP_COLLECTIVES=0 smddprun /opt/conda/bin/python3.9 -m mpi4py run_summarization.py --dataset_name samsum --do_eval True --do_predict True --do_train True --fp16 True --learning_rate 5e-05 --model_name_or_path facebook/bart-large-cnn --num_train_epochs 3 --output_dir /opt/ml/model --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --predict_with_generate True --seed 7\u001b[0m\n",
      "\u001b[34m2024-01-24 01:10:43,520 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=75, name='orted', status='sleeping', started='01:10:42')]\u001b[0m\n",
      "\u001b[34m2024-01-24 01:10:43,520 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=75, name='orted', status='sleeping', started='01:10:42')]\u001b[0m\n",
      "\u001b[34m2024-01-24 01:10:43,520 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=75, name='orted', status='sleeping', started='01:10:42')]\u001b[0m\n",
      "\u001b[35m[2024-01-24 01:10:43.104: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m2024-01-24 01:10:43,109 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[35mWarning: Permanently added 'algo-1,10.2.99.166' (ECDSA) to the list of known hosts.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Bootstrap : Using eth0:10.2.110.88<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:NCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Bootstrap : Using eth0:10.2.110.88<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Bootstrap : Using eth0:10.2.99.166<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Bootstrap : Using eth0:10.2.110.88<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Bootstrap : Using eth0:10.2.110.88<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Bootstrap : Using eth0:10.2.110.88<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Bootstrap : Using eth0:10.2.110.88<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Bootstrap : Using eth0:10.2.99.166<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Bootstrap : Using eth0:10.2.110.88<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Bootstrap : Using eth0:10.2.99.166<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Bootstrap : Using eth0:10.2.99.166<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Bootstrap : Using eth0:10.2.99.166<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Bootstrap : Using eth0:10.2.110.88<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Bootstrap : Using eth0:10.2.99.166<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Bootstrap : Using eth0:10.2.99.166<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Bootstrap : Using eth0:10.2.99.166<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 00/08 :    0   3   2   1   5   6   7   4   8  11  10   9  13  14  15  12\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 01/08 :    0   4   7   6   5   9  10  11   8  12  15  14  13   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Trees [0] 5/-1/-1->1->2 [1] 2/9/-1->1->-1 [2] 2/-1/-1->1->5 [3] -1/-1/-1->1->2 [4] 5/-1/-1->1->2 [5] 2/-1/-1->1->9 [6] 2/-1/-1->1->5 [7] -1/-1/-1->1->2\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Trees [0] 1/-1/-1->2->3 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 1/-1/-1->2->3 [4] 1/-1/-1->2->3 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 1/-1/-1->2->3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 02/08 :    0   1   5   6  10  11  15  12   8   9  13  14   2   3   7   4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 03/08 :    0   1   2   6   5   4   7  11   8   9  10  14  13  12  15   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 04/08 :    0   3   2   1   5   6   7   4   8  11  10   9  13  14  15  12\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 05/08 :    0   4   7   6   5   9  10  11   8  12  15  14  13   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 06/08 :    0   1   5   6  10  11  15  12   8   9  13  14   2   3   7   4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 07/08 :    0   1   2   6   5   4   7  11   8   9  10  14  13  12  15   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Trees [0] 4/-1/-1->7->6 [1] 6/-1/-1->7->4 [2] 6/-1/-1->7->4 [3] 4/-1/-1->7->6 [4] 4/-1/-1->7->6 [5] 6/-1/-1->7->4 [6] 6/-1/-1->7->4 [7] 4/-1/-1->7->6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Trees [0] 6/-1/-1->5->1 [1] -1/-1/-1->5->6 [2] 1/-1/-1->5->6 [3] 6/13/-1->5->-1 [4] 6/-1/-1->5->1 [5] -1/-1/-1->5->6 [6] 1/-1/-1->5->6 [7] 6/-1/-1->5->13\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Trees [0] -1/-1/-1->4->7 [1] 7/-1/-1->4->0 [2] 7/12/-1->4->-1 [3] 0/-1/-1->4->7 [4] -1/-1/-1->4->7 [5] 7/-1/-1->4->0 [6] 7/-1/-1->4->12 [7] 0/-1/-1->4->7\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Trees [0] 3/8/-1->0->-1 [1] 4/-1/-1->0->3 [2] -1/-1/-1->0->3 [3] 3/-1/-1->0->4 [4] 3/-1/-1->0->8 [5] 4/-1/-1->0->3 [6] -1/-1/-1->0->3 [7] 3/-1/-1->0->4\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Trees [0] 2/-1/-1->3->0 [1] 0/-1/-1->3->2 [2] 0/-1/-1->3->2 [3] 2/-1/-1->3->0 [4] 2/-1/-1->3->0 [5] 0/-1/-1->3->2 [6] 0/-1/-1->3->2 [7] 2/-1/-1->3->0\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 5/-1/-1->6->7 [2] 5/-1/-1->6->7 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 5/-1/-1->6->7 [6] 5/-1/-1->6->7 [7] 7/-1/-1->6->5\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 13/-1/-1->14->15 [2] 13/-1/-1->14->15 [3] 15/-1/-1->14->13 [4] 15/-1/-1->14->13 [5] 13/-1/-1->14->15 [6] 13/-1/-1->14->15 [7] 15/-1/-1->14->13\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Trees [0] 12/-1/-1->15->14 [1] 14/-1/-1->15->12 [2] 14/-1/-1->15->12 [3] 12/-1/-1->15->14 [4] 12/-1/-1->15->14 [5] 14/-1/-1->15->12 [6] 14/-1/-1->15->12 [7] 12/-1/-1->15->14\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Trees [0] 10/-1/-1->11->8 [1] 8/-1/-1->11->10 [2] 8/-1/-1->11->10 [3] 10/-1/-1->11->8 [4] 10/-1/-1->11->8 [5] 8/-1/-1->11->10 [6] 8/-1/-1->11->10 [7] 10/-1/-1->11->8\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Trees [0] 9/-1/-1->10->11 [1] 11/-1/-1->10->9 [2] 11/-1/-1->10->9 [3] 9/-1/-1->10->11 [4] 9/-1/-1->10->11 [5] 11/-1/-1->10->9 [6] 11/-1/-1->10->9 [7] 9/-1/-1->10->11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Trees [0] -1/-1/-1->12->15 [1] 15/-1/-1->12->8 [2] 15/-1/-1->12->4 [3] 8/-1/-1->12->15 [4] -1/-1/-1->12->15 [5] 15/-1/-1->12->8 [6] 15/4/-1->12->-1 [7] 8/-1/-1->12->15\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Trees [0] 11/-1/-1->8->0 [1] 12/-1/-1->8->11 [2] -1/-1/-1->8->11 [3] 11/-1/-1->8->12 [4] 11/0/-1->8->-1 [5] 12/-1/-1->8->11 [6] -1/-1/-1->8->11 [7] 11/-1/-1->8->12\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Trees [0] 14/-1/-1->13->9 [1] -1/-1/-1->13->14 [2] 9/-1/-1->13->14 [3] 14/-1/-1->13->5 [4] 14/-1/-1->13->9 [5] -1/-1/-1->13->14 [6] 9/-1/-1->13->14 [7] 14/5/-1->13->-1\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Trees [0] 13/-1/-1->9->10 [1] 10/-1/-1->9->1 [2] 10/-1/-1->9->13 [3] -1/-1/-1->9->10 [4] 13/-1/-1->9->10 [5] 10/1/-1->9->-1 [6] 10/-1/-1->9->13 [7] -1/-1/-1->9->10\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 02/0 : 0[160] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 00/0 : 13[1b0] -> 14[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 01/0 : 12[1a0] -> 15[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 00/0 : 5[1b0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 02/0 : 8[160] -> 9[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 01/0 : 4[1a0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 03/0 : 0[160] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 02/0 : 13[1b0] -> 14[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 03/0 : 12[1a0] -> 15[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 02/0 : 5[1b0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 03/0 : 8[160] -> 9[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 03/0 : 4[1a0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 06/0 : 0[160] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 04/0 : 13[1b0] -> 14[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 06/0 : 8[160] -> 9[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 05/0 : 12[1a0] -> 15[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 04/0 : 5[1b0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 05/0 : 4[1a0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 07/0 : 0[160] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 07/0 : 8[160] -> 9[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 06/0 : 13[1b0] -> 14[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 07/0 : 12[1a0] -> 15[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 06/0 : 5[1b0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 07/0 : 4[1a0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 00/0 : 14[1c0] -> 15[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 01/0 : 10[180] -> 11[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 01/0 : 2[180] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 01/0 : 1[170] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 00/0 : 6[1c0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 01/0 : 9[170] -> 10[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 04/0 : 14[1c0] -> 15[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 02/0 : 10[180] -> 11[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 03/0 : 1[170] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 02/0 : 2[180] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 03/0 : 9[170] -> 10[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 04/0 : 6[1c0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 05/0 : 10[180] -> 11[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 05/0 : 2[180] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 05/0 : 1[170] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 05/0 : 9[170] -> 10[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 06/0 : 2[180] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 07/0 : 1[170] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 06/0 : 10[180] -> 11[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 07/0 : 9[170] -> 10[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 00/0 : 0[160] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 00/0 : 8[160] -> 11[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 04/0 : 0[160] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 04/0 : 8[160] -> 11[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 02/0 : 14[1c0] -> 2[180] [send] via NET/AWS Libfabric/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:505 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 02/0 : 6[1c0] -> 10[180] [send] via NET/AWS Libfabric/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:571 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 01/0 : 13[1b0] -> 1[170] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:507 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:501 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:504 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:500 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 03/0 : 7[1d0] -> 11[190] [send] via NET/AWS Libfabric/3\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 01/0 : 5[1b0] -> 9[170] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:566 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:569 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:573 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:570 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:567 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:572 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 00/0 : 4[1a0] -> 8[160] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:568 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 03/0 : 15[1d0] -> 3[190] [send] via NET/AWS Libfabric/3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 00/0 : 12[1a0] -> 0[160] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:506 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:502 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:503 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 03/0 : 7[1d0] -> 11[190] [receive] via NET/AWS Libfabric/3\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 03/0 : 15[1d0] -> 3[190] [receive] via NET/AWS Libfabric/3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 01/0 : 13[1b0] -> 1[170] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 02/0 : 6[1c0] -> 10[180] [receive] via NET/AWS Libfabric/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 01/0 : 5[1b0] -> 9[170] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 02/0 : 14[1c0] -> 2[180] [receive] via NET/AWS Libfabric/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 06/0 : 6[1c0] -> 10[180] [send] via NET/AWS Libfabric/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:571 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 06/0 : 14[1c0] -> 2[180] [send] via NET/AWS Libfabric/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 05/0 : 13[1b0] -> 1[170] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:504 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:505 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 07/0 : 7[1d0] -> 11[190] [send] via NET/AWS Libfabric/3\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:572 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 04/0 : 12[1a0] -> 0[160] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 07/0 : 15[1d0] -> 3[190] [send] via NET/AWS Libfabric/3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:502 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:506 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 04/0 : 4[1a0] -> 8[160] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 05/0 : 5[1b0] -> 9[170] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:568 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:566 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 00/0 : 4[1a0] -> 8[160] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 00/0 : 12[1a0] -> 0[160] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:503 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:507 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 07/0 : 7[1d0] -> 11[190] [receive] via NET/AWS Libfabric/3\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 06/0 : 6[1c0] -> 10[180] [receive] via NET/AWS Libfabric/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 03/0 : 10[180] -> 14[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 02/0 : 11[190] -> 15[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 06/0 : 11[190] -> 15[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 07/0 : 10[180] -> 14[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:567 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:570 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 07/0 : 15[1d0] -> 3[190] [receive] via NET/AWS Libfabric/3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 05/0 : 13[1b0] -> 1[170] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 02/0 : 3[190] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 00/0 : 1[170] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 02/0 : 1[170] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 06/0 : 3[190] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 04/0 : 1[170] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 06/0 : 1[170] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:501 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 05/0 : 5[1b0] -> 9[170] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 00/0 : 9[170] -> 13[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 02/0 : 9[170] -> 13[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 04/0 : 9[170] -> 13[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 06/0 : 9[170] -> 13[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:569 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 06/0 : 14[1c0] -> 2[180] [receive] via NET/AWS Libfabric/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 03/0 : 2[180] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 07/0 : 2[180] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:500 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 04/0 : 4[1a0] -> 8[160] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 01/0 : 8[160] -> 12[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 05/0 : 8[160] -> 12[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:573 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 04/0 : 12[1a0] -> 0[160] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 01/0 : 0[160] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 05/0 : 0[160] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 01/0 : 3[190] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 03/0 : 3[190] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 05/0 : 3[190] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 07/0 : 3[190] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 01/0 : 11[190] -> 8[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 03/0 : 11[190] -> 8[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 00/0 : 7[1d0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 05/0 : 11[190] -> 8[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 00/0 : 15[1d0] -> 12[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 02/0 : 7[1d0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 07/0 : 11[190] -> 8[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 04/0 : 7[1d0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 02/0 : 15[1d0] -> 12[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 06/0 : 7[1d0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 04/0 : 15[1d0] -> 12[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 06/0 : 15[1d0] -> 12[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 03/0 : 13[1b0] -> 12[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 07/0 : 13[1b0] -> 12[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 03/0 : 5[1b0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 07/0 : 5[1b0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 00/0 : 2[180] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 04/0 : 2[180] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 01/0 : 14[1c0] -> 13[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 03/0 : 14[1c0] -> 13[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 05/0 : 14[1c0] -> 13[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 07/0 : 14[1c0] -> 13[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 00/0 : 1[170] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 00/0 : 10[180] -> 9[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 02/0 : 1[170] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 04/0 : 10[180] -> 9[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 04/0 : 1[170] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 01/0 : 6[1c0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 06/0 : 1[170] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 03/0 : 6[1c0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 05/0 : 6[1c0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 07/0 : 6[1c0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 02/0 : 4[1a0] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 06/0 : 4[1a0] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 02/0 : 12[1a0] -> 8[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 06/0 : 12[1a0] -> 8[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 00/0 : 11[190] -> 10[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 01/0 : 8[160] -> 11[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 01/0 : 15[1d0] -> 14[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 04/0 : 11[190] -> 10[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 02/0 : 8[160] -> 11[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 05/0 : 15[1d0] -> 14[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 03/0 : 8[160] -> 11[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 05/0 : 8[160] -> 11[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 00/0 : 12[1a0] -> 15[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 00/0 : 9[170] -> 10[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 06/0 : 8[160] -> 11[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 02/0 : 12[1a0] -> 15[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 02/0 : 9[170] -> 10[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 07/0 : 8[160] -> 11[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 04/0 : 12[1a0] -> 15[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 04/0 : 9[170] -> 10[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 01/0 : 13[1b0] -> 14[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 06/0 : 12[1a0] -> 15[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 06/0 : 9[170] -> 10[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 03/0 : 13[1b0] -> 14[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 05/0 : 13[1b0] -> 14[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 07/0 : 13[1b0] -> 14[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 01/0 : 14[1c0] -> 15[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 02/0 : 14[1c0] -> 15[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 03/0 : 14[1c0] -> 15[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 05/0 : 14[1c0] -> 15[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 06/0 : 14[1c0] -> 15[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 00/0 : 10[180] -> 11[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 07/0 : 14[1c0] -> 15[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 03/0 : 10[180] -> 11[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 04/0 : 10[180] -> 11[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 07/0 : 10[180] -> 11[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 01/0 : 7[1d0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 01/0 : 0[160] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 00/0 : 3[190] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 05/0 : 7[1d0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 02/0 : 0[160] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 04/0 : 3[190] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 03/0 : 0[160] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 05/0 : 0[160] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 00/0 : 4[1a0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 06/0 : 0[160] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 02/0 : 4[1a0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 07/0 : 0[160] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 04/0 : 4[1a0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 01/0 : 15[1d0] -> 12[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 01/0 : 5[1b0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 03/0 : 15[1d0] -> 12[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 06/0 : 4[1a0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 03/0 : 5[1b0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 00/0 : 14[1c0] -> 13[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 05/0 : 5[1b0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 05/0 : 15[1d0] -> 12[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 07/0 : 5[1b0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 00/0 : 2[180] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 01/0 : 10[180] -> 9[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 01/0 : 6[1c0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 02/0 : 14[1c0] -> 13[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 07/0 : 15[1d0] -> 12[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 03/0 : 2[180] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 02/0 : 6[1c0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 04/0 : 2[180] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 03/0 : 6[1c0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 07/0 : 2[180] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 05/0 : 6[1c0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 04/0 : 14[1c0] -> 13[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 02/0 : 10[180] -> 9[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 00/0 : 11[190] -> 8[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 03/0 : 8[160] -> 12[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 06/0 : 6[1c0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 03/0 : 10[180] -> 9[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 02/0 : 11[190] -> 8[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 06/0 : 14[1c0] -> 13[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 07/0 : 8[160] -> 12[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 05/0 : 10[180] -> 9[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 07/0 : 6[1c0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 04/0 : 11[190] -> 8[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 06/0 : 10[180] -> 9[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 06/0 : 11[190] -> 8[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:567 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 01/0 : 9[170] -> 1[170] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 07/0 : 10[180] -> 9[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 01/0 : 2[180] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 03/0 : 0[160] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 00/0 : 3[190] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 02/0 : 2[180] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 07/0 : 0[160] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 02/0 : 3[190] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 03/0 : 2[180] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 04/0 : 3[190] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 05/0 : 2[180] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 00/0 : 6[1c0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 06/0 : 3[190] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 06/0 : 2[180] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 02/0 : 6[1c0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 07/0 : 2[180] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 04/0 : 6[1c0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 06/0 : 6[1c0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 01/0 : 7[1d0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 03/0 : 7[1d0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 05/0 : 7[1d0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 07/0 : 7[1d0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:504 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:501 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 01/0 : 1[170] -> 9[170] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:502 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:500 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 00/0 : 0[160] -> 8[160] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:567 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 05/0 : 9[170] -> 1[170] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:566 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:568 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:573 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 00/0 : 8[160] -> 0[160] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 03/0 : 5[1b0] -> 13[1b0] [receive] via NET/AWS Libfabric/3\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:501 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 05/0 : 1[170] -> 9[170] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 02/0 : 4[1a0] -> 12[1a0] [receive] via NET/AWS Libfabric/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 03/0 : 13[1b0] -> 5[1b0] [receive] via NET/AWS Libfabric/3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:500 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 04/0 : 0[160] -> 8[160] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 01/0 : 1[170] -> 9[170] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:567 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 02/0 : 12[1a0] -> 4[1a0] [receive] via NET/AWS Libfabric/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:504 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 07/0 : 5[1b0] -> 13[1b0] [receive] via NET/AWS Libfabric/3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:573 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 04/0 : 8[160] -> 0[160] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:502 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 06/0 : 4[1a0] -> 12[1a0] [receive] via NET/AWS Libfabric/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 01/0 : 9[170] -> 1[170] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:501 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:566 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 07/0 : 13[1b0] -> 5[1b0] [receive] via NET/AWS Libfabric/3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 00/0 : 8[160] -> 0[160] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:500 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 05/0 : 1[170] -> 9[170] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:567 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:568 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 06/0 : 12[1a0] -> 4[1a0] [receive] via NET/AWS Libfabric/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 03/0 : 13[1b0] -> 5[1b0] [send] via NET/AWS Libfabric/3\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:504 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 00/0 : 0[160] -> 8[160] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:573 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 02/0 : 12[1a0] -> 4[1a0] [send] via NET/AWS Libfabric/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:502 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 03/0 : 5[1b0] -> 13[1b0] [send] via NET/AWS Libfabric/3\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:566 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 05/0 : 9[170] -> 1[170] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:501 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 04/0 : 8[160] -> 0[160] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:500 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 02/0 : 4[1a0] -> 12[1a0] [send] via NET/AWS Libfabric/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:568 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 07/0 : 13[1b0] -> 5[1b0] [send] via NET/AWS Libfabric/3\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:504 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 04/0 : 0[160] -> 8[160] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:573 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 06/0 : 12[1a0] -> 4[1a0] [send] via NET/AWS Libfabric/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:502 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 07/0 : 5[1b0] -> 13[1b0] [send] via NET/AWS Libfabric/3\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:566 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 06/0 : 4[1a0] -> 12[1a0] [send] via NET/AWS Libfabric/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:568 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 00/0 : 5[1b0] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 02/0 : 5[1b0] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 00/0 : 13[1b0] -> 9[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 04/0 : 5[1b0] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 02/0 : 13[1b0] -> 9[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 06/0 : 5[1b0] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 04/0 : 13[1b0] -> 9[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 06/0 : 13[1b0] -> 9[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 00/1 : 1[170] -> 4[1a0] via P2P/indirect/0[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 00/1 : 9[170] -> 12[1a0] via P2P/indirect/8[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 01/1 : 1[170] -> 4[1a0] via P2P/indirect/0[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 01/1 : 9[170] -> 12[1a0] via P2P/indirect/8[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 01/0 : 4[1a0] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 03/0 : 4[1a0] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 01/0 : 12[1a0] -> 8[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 05/0 : 4[1a0] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 03/0 : 12[1a0] -> 8[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 07/0 : 4[1a0] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 05/0 : 12[1a0] -> 8[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 07/0 : 12[1a0] -> 8[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 00/1 : 0[160] -> 5[1b0] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 01/0 : 3[190] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 00/0 : 7[1d0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 02/0 : 3[190] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 00/1 : 8[160] -> 13[1b0] via P2P/indirect/9[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 01/0 : 11[190] -> 10[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 02/0 : 7[1d0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 00/0 : 15[1d0] -> 14[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 03/0 : 3[190] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 02/0 : 11[190] -> 10[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 03/0 : 7[1d0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 05/0 : 3[190] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 02/0 : 15[1d0] -> 14[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 03/0 : 11[190] -> 10[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 03/0 : 15[1d0] -> 14[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 04/0 : 7[1d0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 06/0 : 3[190] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 05/0 : 11[190] -> 10[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 06/0 : 7[1d0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 04/0 : 15[1d0] -> 14[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 07/0 : 3[190] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 06/0 : 11[190] -> 10[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 06/0 : 15[1d0] -> 14[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 07/0 : 7[1d0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 07/0 : 11[190] -> 10[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 07/0 : 15[1d0] -> 14[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 01/1 : 0[160] -> 5[1b0] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 01/1 : 8[160] -> 13[1b0] via P2P/indirect/9[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 00/1 : 3[190] -> 4[1a0] via P2P/indirect/0[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 00/1 : 11[190] -> 12[1a0] via P2P/indirect/8[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 00/1 : 2[180] -> 4[1a0] via P2P/indirect/0[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 01/1 : 3[190] -> 4[1a0] via P2P/indirect/0[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 01/1 : 11[190] -> 12[1a0] via P2P/indirect/8[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 00/1 : 3[190] -> 5[1b0] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 00/1 : 10[180] -> 12[1a0] via P2P/indirect/8[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 00/1 : 11[190] -> 13[1b0] via P2P/indirect/9[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 01/1 : 2[180] -> 4[1a0] via P2P/indirect/0[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 01/1 : 10[180] -> 12[1a0] via P2P/indirect/8[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 01/1 : 3[190] -> 5[1b0] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 01/1 : 11[190] -> 13[1b0] via P2P/indirect/9[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 00/1 : 2[180] -> 5[1b0] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 00/1 : 3[190] -> 6[1c0] via P2P/indirect/7[1d0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 00/1 : 11[190] -> 14[1c0] via P2P/indirect/15[1d0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 00/1 : 10[180] -> 13[1b0] via P2P/indirect/9[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 01/1 : 11[190] -> 14[1c0] via P2P/indirect/15[1d0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 01/1 : 2[180] -> 5[1b0] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 01/1 : 10[180] -> 13[1b0] via P2P/indirect/9[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 01/1 : 3[190] -> 6[1c0] via P2P/indirect/7[1d0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 00/1 : 4[1a0] -> 1[170] via P2P/indirect/5[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 00/1 : 1[170] -> 6[1c0] via P2P/indirect/5[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 00/1 : 2[180] -> 7[1d0] via P2P/indirect/6[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 00/1 : 10[180] -> 15[1d0] via P2P/indirect/14[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 00/1 : 12[1a0] -> 9[170] via P2P/indirect/13[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 01/1 : 4[1a0] -> 1[170] via P2P/indirect/5[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 00/1 : 9[170] -> 14[1c0] via P2P/indirect/13[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 01/1 : 12[1a0] -> 9[170] via P2P/indirect/13[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 01/1 : 1[170] -> 6[1c0] via P2P/indirect/5[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 01/1 : 2[180] -> 7[1d0] via P2P/indirect/6[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 01/1 : 10[180] -> 15[1d0] via P2P/indirect/14[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 01/1 : 9[170] -> 14[1c0] via P2P/indirect/13[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 00/1 : 8[160] -> 14[1c0] via P2P/indirect/12[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 00/1 : 13[1b0] -> 8[160] via P2P/indirect/12[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 00/1 : 5[1b0] -> 0[160] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 00/1 : 0[160] -> 6[1c0] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 01/1 : 13[1b0] -> 8[160] via P2P/indirect/12[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 00/1 : 1[170] -> 7[1d0] via P2P/indirect/3[190]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 00/1 : 9[170] -> 15[1d0] via P2P/indirect/11[190]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 01/1 : 5[1b0] -> 0[160] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 01/1 : 8[160] -> 14[1c0] via P2P/indirect/12[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 01/1 : 0[160] -> 6[1c0] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 01/1 : 1[170] -> 7[1d0] via P2P/indirect/3[190]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 01/1 : 9[170] -> 15[1d0] via P2P/indirect/11[190]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 00/1 : 0[160] -> 7[1d0] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 00/1 : 6[1c0] -> 0[160] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 00/1 : 8[160] -> 15[1d0] via P2P/indirect/12[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 00/1 : 14[1c0] -> 8[160] via P2P/indirect/12[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 01/1 : 0[160] -> 7[1d0] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 01/1 : 8[160] -> 15[1d0] via P2P/indirect/12[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 01/1 : 6[1c0] -> 0[160] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 01/1 : 14[1c0] -> 8[160] via P2P/indirect/12[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 00/1 : 7[1d0] -> 0[160] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 00/1 : 15[1d0] -> 8[160] via P2P/indirect/12[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 01/1 : 7[1d0] -> 0[160] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 01/1 : 15[1d0] -> 8[160] via P2P/indirect/12[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 00/1 : 7[1d0] -> 1[170] via P2P/indirect/5[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 00/1 : 15[1d0] -> 9[170] via P2P/indirect/13[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 01/1 : 7[1d0] -> 1[170] via P2P/indirect/5[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 01/1 : 15[1d0] -> 9[170] via P2P/indirect/13[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 00/1 : 6[1c0] -> 1[170] via P2P/indirect/5[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 00/1 : 7[1d0] -> 2[180] via P2P/indirect/3[190]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 00/1 : 14[1c0] -> 9[170] via P2P/indirect/13[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 00/1 : 15[1d0] -> 10[180] via P2P/indirect/11[190]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 01/1 : 6[1c0] -> 1[170] via P2P/indirect/5[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 01/1 : 7[1d0] -> 2[180] via P2P/indirect/3[190]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 01/1 : 14[1c0] -> 9[170] via P2P/indirect/13[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 01/1 : 15[1d0] -> 10[180] via P2P/indirect/11[190]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 00/1 : 5[1b0] -> 2[180] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 00/1 : 13[1b0] -> 10[180] via P2P/indirect/9[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 00/1 : 6[1c0] -> 3[190] via P2P/indirect/2[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 00/1 : 14[1c0] -> 11[190] via P2P/indirect/10[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 01/1 : 6[1c0] -> 3[190] via P2P/indirect/2[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 01/1 : 13[1b0] -> 10[180] via P2P/indirect/9[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 01/1 : 5[1b0] -> 2[180] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 01/1 : 14[1c0] -> 11[190] via P2P/indirect/10[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 00/1 : 12[1a0] -> 10[180] via P2P/indirect/14[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 00/1 : 5[1b0] -> 3[190] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 00/1 : 4[1a0] -> 2[180] via P2P/indirect/6[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 00/1 : 13[1b0] -> 11[190] via P2P/indirect/9[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 01/1 : 4[1a0] -> 2[180] via P2P/indirect/6[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 01/1 : 12[1a0] -> 10[180] via P2P/indirect/14[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 01/1 : 13[1b0] -> 11[190] via P2P/indirect/9[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 01/1 : 5[1b0] -> 3[190] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 00/1 : 4[1a0] -> 3[190] via P2P/indirect/0[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 00/1 : 12[1a0] -> 11[190] via P2P/indirect/8[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 01/1 : 4[1a0] -> 3[190] via P2P/indirect/0[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 01/1 : 12[1a0] -> 11[190] via P2P/indirect/8[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO comm 0x55c51b20ebe0 rank 10 nranks 16 cudaDev 2 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO comm 0x5606a07e10b0 rank 13 nranks 16 cudaDev 5 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO comm 0x55c9919cf8f0 rank 4 nranks 16 cudaDev 4 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO comm 0x56536b90bce0 rank 12 nranks 16 cudaDev 4 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO comm 0x564249e81af0 rank 1 nranks 16 cudaDev 1 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO comm 0x55ea7e483640 rank 8 nranks 16 cudaDev 0 busId 160 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO comm 0x5597929b0ee0 rank 0 nranks 16 cudaDev 0 busId 160 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO comm 0x557c43715d00 rank 9 nranks 16 cudaDev 1 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:NCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO comm 0x557127a3f7b0 rank 3 nranks 16 cudaDev 3 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO comm 0x5563ca008b20 rank 7 nranks 16 cudaDev 7 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO comm 0x55df867fc340 rank 6 nranks 16 cudaDev 6 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO comm 0x564b92bc4830 rank 14 nranks 16 cudaDev 6 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO comm 0x5629575718f0 rank 15 nranks 16 cudaDev 7 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO comm 0x555c9d491a00 rank 5 nranks 16 cudaDev 5 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO comm 0x55d9a75b6830 rank 2 nranks 16 cudaDev 2 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO comm 0x564033ee2590 rank 11 nranks 16 cudaDev 3 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:00:16.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:00:17.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:00:18.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:00:19.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 5/-1/-1->6->7 [3] 5/-1/-1->6->7 [4] 2/-1/-1->6->4 [5] 4/-1/-1->6->2 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 5/-1/-1->6->7 [9] 5/-1/-1->6->7 [10] 2/-1/-1->6->4 [11] 4/-1/-1->6->2\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Trees [0] 6/-1/-1->5->1 [1] 6/-1/-1->5->1 [2] 1/-1/-1->5->6 [3] 1/-1/-1->5->6 [4] 4/-1/-1->5->7 [5] 7/-1/-1->5->4 [6] 6/-1/-1->5->1 [7] 6/-1/-1->5->1 [8] 1/-1/-1->5->6 [9] 1/-1/-1->5->6 [10] 4/-1/-1->5->7 [11] 7/-1/-1->5->4\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Trees [0] -1/-1/-1->4->7 [1] -1/-1/-1->4->7 [2] 7/-1/-1->4->0 [3] 7/-1/-1->4->0 [4] 6/-1/-1->4->5 [5] 5/-1/-1->4->6 [6] -1/-1/-1->4->7 [7] -1/-1/-1->4->7 [8] 7/-1/-1->4->0 [9] 7/-1/-1->4->0 [10] 6/-1/-1->4->5 [11] 5/-1/-1->4->6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Trees [0] 4/-1/-1->7->6 [1] 4/-1/-1->7->6 [2] 6/-1/-1->7->4 [3] 6/-1/-1->7->4 [4] 5/-1/-1->7->3 [5] 3/-1/-1->7->5 [6] 4/-1/-1->7->6 [7] 4/-1/-1->7->6 [8] 6/-1/-1->7->4 [9] 6/-1/-1->7->4 [10] 5/-1/-1->7->3 [11] 3/-1/-1->7->5\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Trees [0] 1/-1/-1->2->3 [1] 1/-1/-1->2->3 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] -1/-1/-1->2->6 [5] 6/-1/-1->2->0 [6] 1/-1/-1->2->3 [7] 1/-1/-1->2->3 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] -1/-1/-1->2->6 [11] 6/-1/-1->2->0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Trees [0] 2/-1/-1->3->0 [1] 2/-1/-1->3->0 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] 7/-1/-1->3->1 [5] 1/-1/-1->3->7 [6] 2/-1/-1->3->0 [7] 2/-1/-1->3->0 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] 7/-1/-1->3->1 [11] 1/-1/-1->3->7\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Trees [0] 5/-1/-1->1->2 [1] 5/-1/-1->1->2 [2] 2/-1/-1->1->5 [3] 2/-1/-1->1->5 [4] 3/-1/-1->1->0 [5] -1/-1/-1->1->3 [6] 5/-1/-1->1->2 [7] 5/-1/-1->1->2 [8] 2/-1/-1->1->5 [9] 2/-1/-1->1->5 [10] 3/-1/-1->1->0 [11] -1/-1/-1->1->3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1 [1] 3/-1/-1->0->-1 [2] 4/-1/-1->0->-1 [3] 4/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 2/-1/-1->0->-1 [6] 3/-1/-1->0->-1 [7] 3/-1/-1->0->-1 [8] 4/-1/-1->0->-1 [9] 4/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 2/-1/-1->0->-1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 05/0 : 4[1a0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 04/0 : 0[160] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Trees [0] 1/-1/-1->2->3 [1] 1/-1/-1->2->3 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] -1/-1/-1->2->6 [5] 6/-1/-1->2->0 [6] 1/-1/-1->2->3 [7] 1/-1/-1->2->3 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] -1/-1/-1->2->6 [11] 6/-1/-1->2->0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Trees [0] 5/-1/-1->1->2 [1] 5/-1/-1->1->2 [2] 2/-1/-1->1->5 [3] 2/-1/-1->1->5 [4] 3/-1/-1->1->0 [5] -1/-1/-1->1->3 [6] 5/-1/-1->1->2 [7] 5/-1/-1->1->2 [8] 2/-1/-1->1->5 [9] 2/-1/-1->1->5 [10] 3/-1/-1->1->0 [11] -1/-1/-1->1->3\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Trees [0] 6/-1/-1->5->1 [1] 6/-1/-1->5->1 [2] 1/-1/-1->5->6 [3] 1/-1/-1->5->6 [4] 4/-1/-1->5->7 [5] 7/-1/-1->5->4 [6] 6/-1/-1->5->1 [7] 6/-1/-1->5->1 [8] 1/-1/-1->5->6 [9] 1/-1/-1->5->6 [10] 4/-1/-1->5->7 [11] 7/-1/-1->5->4\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Trees [0] 2/-1/-1->3->0 [1] 2/-1/-1->3->0 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] 7/-1/-1->3->1 [5] 1/-1/-1->3->7 [6] 2/-1/-1->3->0 [7] 2/-1/-1->3->0 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] 7/-1/-1->3->1 [11] 1/-1/-1->3->7\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Trees [0] 4/-1/-1->7->6 [1] 4/-1/-1->7->6 [2] 6/-1/-1->7->4 [3] 6/-1/-1->7->4 [4] 5/-1/-1->7->3 [5] 3/-1/-1->7->5 [6] 4/-1/-1->7->6 [7] 4/-1/-1->7->6 [8] 6/-1/-1->7->4 [9] 6/-1/-1->7->4 [10] 5/-1/-1->7->3 [11] 3/-1/-1->7->5\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1 [1] 3/-1/-1->0->-1 [2] 4/-1/-1->0->-1 [3] 4/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 2/-1/-1->0->-1 [6] 3/-1/-1->0->-1 [7] 3/-1/-1->0->-1 [8] 4/-1/-1->0->-1 [9] 4/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 2/-1/-1->0->-1\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 5/-1/-1->6->7 [3] 5/-1/-1->6->7 [4] 2/-1/-1->6->4 [5] 4/-1/-1->6->2 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 5/-1/-1->6->7 [9] 5/-1/-1->6->7 [10] 2/-1/-1->6->4 [11] 4/-1/-1->6->2\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Trees [0] -1/-1/-1->4->7 [1] -1/-1/-1->4->7 [2] 7/-1/-1->4->0 [3] 7/-1/-1->4->0 [4] 6/-1/-1->4->5 [5] 5/-1/-1->4->6 [6] -1/-1/-1->4->7 [7] -1/-1/-1->4->7 [8] 7/-1/-1->4->0 [9] 7/-1/-1->4->0 [10] 6/-1/-1->4->5 [11] 5/-1/-1->4->6\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 05/0 : 4[1a0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 04/0 : 0[160] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 11/0 : 4[1a0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 10/0 : 0[160] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 00/0 : 5[1b0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 02/0 : 1[170] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 11/0 : 4[1a0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 10/0 : 0[160] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 01/0 : 5[1b0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 03/0 : 1[170] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 06/0 : 5[1b0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 02/0 : 1[170] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 00/0 : 5[1b0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 08/0 : 1[170] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 02/0 : 2[180] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 00/0 : 6[1c0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 07/0 : 5[1b0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 03/0 : 1[170] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 09/0 : 1[170] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 03/0 : 2[180] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 01/0 : 5[1b0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 01/0 : 6[1c0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 08/0 : 1[170] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 08/0 : 2[180] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 06/0 : 6[1c0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 02/0 : 2[180] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 00/0 : 6[1c0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 06/0 : 5[1b0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 09/0 : 1[170] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 04/0 : 4[1a0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 09/0 : 2[180] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 01/0 : 6[1c0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 05/0 : 0[160] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 07/0 : 6[1c0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 03/0 : 2[180] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 07/0 : 5[1b0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 10/0 : 4[1a0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 11/0 : 0[160] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 06/0 : 6[1c0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 08/0 : 2[180] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 07/0 : 6[1c0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 09/0 : 2[180] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 05/0 : 0[160] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 04/0 : 4[1a0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 11/0 : 0[160] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 10/0 : 4[1a0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 04/0 : 1[170] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 05/0 : 5[1b0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 10/0 : 1[170] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 11/0 : 5[1b0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 05/0 : 5[1b0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 04/0 : 1[170] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 11/0 : 5[1b0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 10/0 : 1[170] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 02/0 : 4[1a0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 03/0 : 4[1a0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 04/0 : 6[1c0] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 08/0 : 4[1a0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 10/0 : 6[1c0] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 00/0 : 0[160] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 09/0 : 4[1a0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 02/0 : 4[1a0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 00/0 : 0[160] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 01/0 : 0[160] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 06/0 : 0[160] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 03/0 : 4[1a0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 01/0 : 0[160] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 07/0 : 0[160] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 02/0 : 5[1b0] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 05/0 : 2[180] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 00/0 : 1[170] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 08/0 : 4[1a0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 04/0 : 6[1c0] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 06/0 : 0[160] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 00/0 : 1[170] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 03/0 : 5[1b0] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 02/0 : 5[1b0] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 10/0 : 6[1c0] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 01/0 : 1[170] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 09/0 : 4[1a0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 11/0 : 2[180] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 07/0 : 0[160] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 01/0 : 1[170] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 03/0 : 5[1b0] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 08/0 : 5[1b0] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 05/0 : 2[180] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 06/0 : 1[170] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 06/0 : 1[170] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 05/0 : 7[1d0] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 08/0 : 5[1b0] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 09/0 : 5[1b0] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 11/0 : 2[180] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 07/0 : 1[170] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 11/0 : 7[1d0] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 07/0 : 1[170] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 09/0 : 5[1b0] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 00/0 : 4[1a0] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 05/0 : 6[1c0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 04/0 : 3[190] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 04/0 : 2[180] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 01/0 : 4[1a0] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 11/0 : 6[1c0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 10/0 : 3[190] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 10/0 : 2[180] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 05/0 : 6[1c0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 04/0 : 2[180] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 06/0 : 4[1a0] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 04/0 : 3[190] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 11/0 : 6[1c0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 10/0 : 3[190] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 02/0 : 0[160] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 10/0 : 2[180] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 07/0 : 4[1a0] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 03/0 : 0[160] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 05/0 : 7[1d0] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 11/0 : 7[1d0] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 08/0 : 0[160] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 00/0 : 7[1d0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 02/0 : 3[190] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 01/0 : 7[1d0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 00/0 : 4[1a0] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 03/0 : 3[190] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 06/0 : 7[1d0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 08/0 : 3[190] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 09/0 : 0[160] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 07/0 : 7[1d0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 01/0 : 4[1a0] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 09/0 : 3[190] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 02/0 : 0[160] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 02/0 : 3[190] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 00/0 : 7[1d0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 06/0 : 4[1a0] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 03/0 : 0[160] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 01/0 : 7[1d0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 03/0 : 3[190] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 07/0 : 4[1a0] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 08/0 : 0[160] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 08/0 : 3[190] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 06/0 : 7[1d0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 09/0 : 0[160] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 09/0 : 3[190] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 07/0 : 7[1d0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 04/0 : 7[1d0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 05/0 : 3[190] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 10/0 : 7[1d0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 11/0 : 3[190] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 04/0 : 7[1d0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 05/0 : 3[190] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 10/0 : 7[1d0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 11/0 : 3[190] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 02/0 : 7[1d0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 00/0 : 3[190] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 03/0 : 7[1d0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 01/0 : 3[190] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 02/0 : 7[1d0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 08/0 : 7[1d0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 00/0 : 3[190] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 06/0 : 3[190] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 09/0 : 7[1d0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 03/0 : 7[1d0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 07/0 : 3[190] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 01/0 : 3[190] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 04/0 : 5[1b0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 02/0 : 6[1c0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 05/0 : 1[170] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 00/0 : 2[180] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 08/0 : 7[1d0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 10/0 : 5[1b0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 03/0 : 6[1c0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 06/0 : 3[190] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 11/0 : 1[170] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 01/0 : 2[180] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 08/0 : 6[1c0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 09/0 : 7[1d0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 06/0 : 2[180] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 07/0 : 3[190] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 09/0 : 6[1c0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 04/0 : 5[1b0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 04/0 : 4[1a0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 07/0 : 2[180] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 02/0 : 6[1c0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 05/0 : 1[170] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 00/0 : 2[180] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 10/0 : 5[1b0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 10/0 : 4[1a0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 03/0 : 6[1c0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 11/0 : 1[170] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 01/0 : 2[180] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 08/0 : 6[1c0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 06/0 : 2[180] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 09/0 : 6[1c0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 04/0 : 4[1a0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 07/0 : 2[180] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 10/0 : 4[1a0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 00/0 : 1[170] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 01/0 : 1[170] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 02/0 : 5[1b0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 06/0 : 1[170] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 00/0 : 1[170] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 03/0 : 5[1b0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 07/0 : 1[170] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 08/0 : 5[1b0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 01/0 : 1[170] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 09/0 : 5[1b0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 02/0 : 6[1c0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 02/0 : 5[1b0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 06/0 : 1[170] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 00/0 : 2[180] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 03/0 : 6[1c0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 05/0 : 4[1a0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 01/0 : 2[180] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 03/0 : 5[1b0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 08/0 : 6[1c0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 07/0 : 1[170] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 11/0 : 4[1a0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 06/0 : 2[180] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 09/0 : 6[1c0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 08/0 : 5[1b0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 07/0 : 2[180] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 09/0 : 5[1b0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 02/0 : 6[1c0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 00/0 : 2[180] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 03/0 : 6[1c0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 01/0 : 2[180] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 08/0 : 6[1c0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 05/0 : 4[1a0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 06/0 : 2[180] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 09/0 : 6[1c0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 11/0 : 4[1a0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 07/0 : 2[180] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 05/0 : 1[170] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 04/0 : 5[1b0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 11/0 : 1[170] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 05/0 : 1[170] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 10/0 : 5[1b0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 04/0 : 5[1b0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 11/0 : 1[170] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 10/0 : 5[1b0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 04/0 : 2[180] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 10/0 : 2[180] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 02/0 : 1[170] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 00/0 : 5[1b0] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 03/0 : 1[170] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 01/0 : 5[1b0] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 04/0 : 2[180] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 08/0 : 1[170] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 06/0 : 5[1b0] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 05/0 : 3[190] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 09/0 : 1[170] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 00/0 : 4[1a0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 05/0 : 3[190] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 10/0 : 2[180] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 00/0 : 4[1a0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 01/0 : 4[1a0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 07/0 : 5[1b0] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 11/0 : 3[190] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 11/0 : 3[190] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 05/0 : 6[1c0] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 06/0 : 4[1a0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 01/0 : 4[1a0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 11/0 : 6[1c0] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 07/0 : 4[1a0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 05/0 : 6[1c0] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 06/0 : 4[1a0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 11/0 : 6[1c0] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 07/0 : 4[1a0] -> 7[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 00/0 : 5[1b0] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 05/0 : 2[180] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 04/0 : 6[1c0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 01/0 : 5[1b0] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 02/0 : 1[170] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 11/0 : 2[180] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 10/0 : 6[1c0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 06/0 : 5[1b0] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 03/0 : 1[170] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 07/0 : 5[1b0] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 02/0 : 4[1a0] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 08/0 : 1[170] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 03/0 : 4[1a0] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 09/0 : 1[170] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 05/0 : 2[180] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 02/0 : 4[1a0] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 08/0 : 4[1a0] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 04/0 : 6[1c0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 04/0 : 7[1d0] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 09/0 : 4[1a0] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 10/0 : 7[1d0] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 10/0 : 6[1c0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 11/0 : 2[180] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 03/0 : 4[1a0] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 08/0 : 4[1a0] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 04/0 : 7[1d0] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 09/0 : 4[1a0] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 10/0 : 7[1d0] -> 3[190] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 00/0 : 3[190] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 02/0 : 7[1d0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 01/0 : 3[190] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 03/0 : 7[1d0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 00/0 : 3[190] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 02/0 : 7[1d0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 06/0 : 3[190] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 08/0 : 7[1d0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 07/0 : 3[190] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 09/0 : 7[1d0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 01/0 : 3[190] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 03/0 : 7[1d0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 06/0 : 3[190] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 08/0 : 7[1d0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 07/0 : 3[190] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 09/0 : 7[1d0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 04/0 : 3[190] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 05/0 : 7[1d0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 10/0 : 3[190] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 11/0 : 7[1d0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 04/0 : 3[190] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 05/0 : 7[1d0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 10/0 : 3[190] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 11/0 : 7[1d0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 02/0 : 3[190] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 00/0 : 7[1d0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 03/0 : 3[190] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 01/0 : 7[1d0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 08/0 : 3[190] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 06/0 : 7[1d0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 02/0 : 2[180] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 09/0 : 3[190] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 07/0 : 7[1d0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 02/0 : 2[180] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 04/0 : 1[170] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 04/0 : 1[170] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 02/0 : 3[190] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 00/0 : 6[1c0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 05/0 : 5[1b0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 00/0 : 7[1d0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 10/0 : 1[170] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 03/0 : 2[180] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 03/0 : 2[180] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 11/0 : 5[1b0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 01/0 : 6[1c0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 00/0 : 6[1c0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 10/0 : 1[170] -> 0[160] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 03/0 : 3[190] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 01/0 : 7[1d0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 08/0 : 2[180] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 08/0 : 2[180] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 06/0 : 6[1c0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 01/0 : 6[1c0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 08/0 : 3[190] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 09/0 : 2[180] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 06/0 : 7[1d0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 07/0 : 6[1c0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 09/0 : 2[180] -> 1[170] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 06/0 : 6[1c0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 09/0 : 3[190] -> 2[180] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 07/0 : 7[1d0] -> 6[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 07/0 : 6[1c0] -> 5[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 05/0 : 5[1b0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 11/0 : 5[1b0] -> 4[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 08/1 : 3[190] -> 4[1a0] via P2P/indirect/0[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 08/1 : 3[190] -> 4[1a0] via P2P/indirect/0[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 08/1 : 7[1d0] -> 0[160] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 08/1 : 7[1d0] -> 0[160] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 04/1 : 2[180] -> 4[1a0] via P2P/indirect/0[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 04/1 : 6[1c0] -> 0[160] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 04/1 : 2[180] -> 4[1a0] via P2P/indirect/0[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 09/1 : 3[190] -> 4[1a0] via P2P/indirect/0[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 09/1 : 7[1d0] -> 0[160] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 09/1 : 3[190] -> 4[1a0] via P2P/indirect/0[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 09/1 : 7[1d0] -> 0[160] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 04/1 : 6[1c0] -> 0[160] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 04/1 : 7[1d0] -> 1[170] via P2P/indirect/5[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 05/1 : 2[180] -> 4[1a0] via P2P/indirect/0[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 04/1 : 3[190] -> 5[1b0] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 05/1 : 2[180] -> 4[1a0] via P2P/indirect/0[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 04/1 : 7[1d0] -> 1[170] via P2P/indirect/5[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 05/1 : 6[1c0] -> 0[160] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 05/1 : 7[1d0] -> 1[170] via P2P/indirect/5[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 05/1 : 6[1c0] -> 0[160] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 05/1 : 3[190] -> 5[1b0] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 05/1 : 7[1d0] -> 1[170] via P2P/indirect/5[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 04/1 : 3[190] -> 5[1b0] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 12/1 : 7[1d0] -> 2[180] via P2P/indirect/3[190]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 12/1 : 7[1d0] -> 2[180] via P2P/indirect/3[190]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 05/1 : 3[190] -> 5[1b0] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 12/1 : 3[190] -> 6[1c0] via P2P/indirect/7[1d0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO Channel 13/1 : 7[1d0] -> 2[180] via P2P/indirect/3[190]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 12/1 : 3[190] -> 6[1c0] via P2P/indirect/7[1d0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 12/1 : 6[1c0] -> 1[170] via P2P/indirect/5[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO Channel 13/1 : 7[1d0] -> 2[180] via P2P/indirect/3[190]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO Channel 13/1 : 3[190] -> 6[1c0] via P2P/indirect/7[1d0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 12/1 : 1[170] -> 4[1a0] via P2P/indirect/0[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 12/1 : 5[1b0] -> 0[160] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 12/1 : 2[180] -> 5[1b0] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 12/1 : 6[1c0] -> 1[170] via P2P/indirect/5[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO Channel 13/1 : 3[190] -> 6[1c0] via P2P/indirect/7[1d0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 13/1 : 6[1c0] -> 1[170] via P2P/indirect/5[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 12/1 : 1[170] -> 4[1a0] via P2P/indirect/0[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 12/1 : 5[1b0] -> 0[160] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 13/1 : 1[170] -> 4[1a0] via P2P/indirect/0[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 13/1 : 5[1b0] -> 0[160] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 12/1 : 2[180] -> 5[1b0] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 13/1 : 6[1c0] -> 1[170] via P2P/indirect/5[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 13/1 : 1[170] -> 4[1a0] via P2P/indirect/0[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 13/1 : 2[180] -> 5[1b0] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 13/1 : 5[1b0] -> 0[160] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 10/1 : 4[1a0] -> 1[170] via P2P/indirect/5[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 10/1 : 0[160] -> 5[1b0] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 10/1 : 4[1a0] -> 1[170] via P2P/indirect/5[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 11/1 : 4[1a0] -> 1[170] via P2P/indirect/5[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 10/1 : 0[160] -> 5[1b0] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 13/1 : 2[180] -> 5[1b0] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 11/1 : 4[1a0] -> 1[170] via P2P/indirect/5[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 11/1 : 0[160] -> 5[1b0] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 11/1 : 0[160] -> 5[1b0] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 10/1 : 6[1c0] -> 3[190] via P2P/indirect/2[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 10/1 : 6[1c0] -> 3[190] via P2P/indirect/2[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 10/1 : 1[170] -> 6[1c0] via P2P/indirect/5[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO Channel 11/1 : 6[1c0] -> 3[190] via P2P/indirect/2[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 10/1 : 1[170] -> 6[1c0] via P2P/indirect/5[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 10/1 : 5[1b0] -> 2[180] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO Channel 11/1 : 6[1c0] -> 3[190] via P2P/indirect/2[180]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 10/1 : 2[180] -> 7[1d0] via P2P/indirect/6[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 11/1 : 1[170] -> 6[1c0] via P2P/indirect/5[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 11/1 : 5[1b0] -> 2[180] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 11/1 : 1[170] -> 6[1c0] via P2P/indirect/5[1b0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO Channel 11/1 : 2[180] -> 7[1d0] via P2P/indirect/6[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 10/1 : 5[1b0] -> 2[180] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 10/1 : 2[180] -> 7[1d0] via P2P/indirect/6[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 06/1 : 0[160] -> 6[1c0] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 06/1 : 4[1a0] -> 2[180] via P2P/indirect/6[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 11/1 : 5[1b0] -> 2[180] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO Channel 11/1 : 2[180] -> 7[1d0] via P2P/indirect/6[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 06/1 : 4[1a0] -> 2[180] via P2P/indirect/6[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 06/1 : 1[170] -> 7[1d0] via P2P/indirect/3[190]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 07/1 : 4[1a0] -> 2[180] via P2P/indirect/6[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 07/1 : 0[160] -> 6[1c0] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 06/1 : 5[1b0] -> 3[190] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 07/1 : 4[1a0] -> 2[180] via P2P/indirect/6[1c0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO Channel 07/1 : 1[170] -> 7[1d0] via P2P/indirect/3[190]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 06/1 : 1[170] -> 7[1d0] via P2P/indirect/3[190]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 06/1 : 0[160] -> 6[1c0] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO Channel 07/1 : 5[1b0] -> 3[190] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO Channel 07/1 : 1[170] -> 7[1d0] via P2P/indirect/3[190]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 07/1 : 0[160] -> 6[1c0] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 06/1 : 5[1b0] -> 3[190] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO Channel 07/1 : 5[1b0] -> 3[190] via P2P/indirect/1[170]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 14/1 : 4[1a0] -> 3[190] via P2P/indirect/0[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO Channel 15/1 : 4[1a0] -> 3[190] via P2P/indirect/0[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 14/1 : 0[160] -> 7[1d0] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 14/1 : 0[160] -> 7[1d0] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO Channel 15/1 : 0[160] -> 7[1d0] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO Channel 15/1 : 0[160] -> 7[1d0] via P2P/indirect/4[1a0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 14/1 : 4[1a0] -> 3[190] via P2P/indirect/0[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 15/1 : 4[1a0] -> 3[190] via P2P/indirect/0[160]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:278:278 [1] NCCL INFO comm 0x56424a87bda0 rank 1 nranks 8 cudaDev 1 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:293:293 [4] NCCL INFO comm 0x55c992831f50 rank 4 nranks 8 cudaDev 4 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:279:279 [6] NCCL INFO comm 0x55df86b1a160 rank 6 nranks 8 cudaDev 6 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:255:255 [7] NCCL INFO comm 0x5563ca325810 rank 7 nranks 8 cudaDev 7 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:253:253 [3] NCCL INFO comm 0x557129496b20 rank 3 nranks 8 cudaDev 3 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:359:359 [0] NCCL INFO comm 0x559792cdf730 rank 0 nranks 8 cudaDev 0 busId 160 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:Running smdistributed.dataparallel v1.7.0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:SMDDP: Multi node EFA mode\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:292:292 [5] NCCL INFO comm 0x555c9ef074b0 rank 5 nranks 8 cudaDev 5 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:256:256 [2] NCCL INFO comm 0x55d9a78d41b0 rank 2 nranks 8 cudaDev 2 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:214:214 [5] NCCL INFO comm 0x5606a0b0c5c0 rank 5 nranks 8 cudaDev 5 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:200:200 [1] NCCL INFO comm 0x557c43a42a50 rank 1 nranks 8 cudaDev 1 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:295:295 [0] NCCL INFO comm 0x55ea7f53eb50 rank 0 nranks 8 cudaDev 0 busId 160 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:215:215 [4] NCCL INFO comm 0x56536c70dd50 rank 4 nranks 8 cudaDev 4 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:213:213 [3] NCCL INFO comm 0x564034e54ff0 rank 3 nranks 8 cudaDev 3 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:190:190 [2] NCCL INFO comm 0x55c51b52c460 rank 2 nranks 8 cudaDev 2 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:201:201 [6] NCCL INFO comm 0x564b94103270 rank 6 nranks 8 cudaDev 6 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:221:221 [7] NCCL INFO comm 0x56295788e510 rank 7 nranks 8 cudaDev 7 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:Successfully opened device rdmap0s21\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:Successfully opened device rdmap0s21\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:Successfully opened device rdmap0s21\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:Successfully opened device rdmap0s21\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:Successfully opened device rdmap0s21\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:Successfully opened device rdmap0s21\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:Successfully opened device rdmap0s21\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:Successfully opened device rdmap0s21\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:Successfully opened device rdmap0s21\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:Successfully opened device rdmap0s21\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:Successfully opened device rdmap0s21\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:Successfully opened device rdmap0s21\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:Successfully opened device rdmap0s21\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:Successfully opened device rdmap0s21\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:Successfully opened device rdmap0s21\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:Successfully opened device rdmap0s21\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:01/24/2024 01:10:53 - WARNING - __main__ - Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:01/24/2024 01:10:53 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:01/24/2024 01:10:53 - WARNING - __main__ - Process rank: 7, device: cuda:7, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:01/24/2024 01:10:53 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:01/24/2024 01:10:53 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:01/24/2024 01:10:53 - WARNING - __main__ - Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:01/24/2024 01:10:53 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:01/24/2024 01:10:53 - WARNING - __main__ - Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:01/24/2024 01:10:53 - WARNING - __main__ - Process rank: 7, device: cuda:7, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:01/24/2024 01:10:53 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:01/24/2024 01:10:53 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:01/24/2024 01:10:53 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:01/24/2024 01:10:53 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:_n_gpu=1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:adafactor=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:adam_beta1=0.9,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:adam_beta2=0.999,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:adam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:auto_find_batch_size=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:bf16=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:bf16_full_eval=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:data_seed=None,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:dataloader_drop_last=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:dataloader_num_workers=0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:dataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:ddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:ddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:ddp_timeout=1800,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:debug=[],\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:deepspeed=None,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:disable_tqdm=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:do_eval=True,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:do_predict=True,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:do_train=True,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:eval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:eval_delay=0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:eval_steps=None,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:evaluation_strategy=no,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:fp16=True,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:fp16_backend=auto,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:fp16_full_eval=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:fp16_opt_level=O1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:fsdp=[],\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:fsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:fsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:full_determinism=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:generation_max_length=None,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:generation_num_beams=None,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:gradient_accumulation_steps=1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:gradient_checkpointing=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:greater_is_better=None,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:group_by_length=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:half_precision_backend=auto,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:hub_model_id=None,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:hub_private_repo=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:hub_strategy=every_save,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:hub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:ignore_data_skip=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:include_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:jit_mode_eval=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:label_names=None,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:label_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:learning_rate=5e-05,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:length_column_name=length,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:load_best_model_at_end=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:local_rank=0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:log_level=passive,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:log_level_replica=passive,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:log_on_each_node=True,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:logging_dir=/opt/ml/model/runs/Jan24_01-10-47_algo-2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:logging_first_step=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:logging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:logging_steps=500,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:logging_strategy=steps,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:lr_scheduler_type=linear,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:max_grad_norm=1.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:max_steps=-1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:metric_for_best_model=None,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:mp_parameters=,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:no_cuda=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:num_train_epochs=3.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:optim=adamw_hf,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:optim_args=None,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:output_dir=/opt/ml/model,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:overwrite_output_dir=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:past_index=-1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:per_device_eval_batch_size=4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:per_device_train_batch_size=4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:predict_with_generate=True,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:prediction_loss_only=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:push_to_hub=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:push_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:push_to_hub_organization=None,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:ray_scope=last,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:remove_unused_columns=True,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:report_to=[],\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:resume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:run_name=/opt/ml/model,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:save_on_each_node=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:save_steps=500,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:save_strategy=steps,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:save_total_limit=None,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:seed=7,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:sharded_ddp=[],\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:skip_memory_metrics=True,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:sortish_sampler=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:tf32=None,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:torch_compile=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:torch_compile_backend=None,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:torch_compile_mode=None,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:torchdynamo=None,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:tpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:tpu_num_cores=None,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:use_ipex=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:use_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:use_mps_device=False,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:warmup_ratio=0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:warmup_steps=0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:weight_decay=0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:xpu_backend=None,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:)\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:01/24/2024 01:10:53 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:01/24/2024 01:10:53 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:01/24/2024 01:10:53 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:01/24/2024 01:10:53 - WARNING - __main__ - Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading builder script:   0%|          | 0.00/3.36k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading builder script: 100%|██████████| 3.36k/3.36k [00:00<00:00, 5.71MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading builder script:   0%|          | 0.00/3.36k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading builder script: 100%|██████████| 3.36k/3.36k [00:00<00:00, 5.63MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading builder script:   0%|          | 0.00/3.36k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading builder script: 100%|██████████| 3.36k/3.36k [00:00<00:00, 4.55MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading builder script:   0%|          | 0.00/3.36k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading builder script: 100%|██████████| 3.36k/3.36k [00:00<00:00, 5.73MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading builder script:   0%|          | 0.00/3.36k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading builder script: 100%|██████████| 3.36k/3.36k [00:00<00:00, 5.40MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading builder script:   0%|          | 0.00/3.36k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading builder script: 100%|██████████| 3.36k/3.36k [00:00<00:00, 5.60MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading readme:   0%|          | 0.00/7.04k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading readme: 100%|██████████| 7.04k/7.04k [00:00<00:00, 10.8MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading readme:   0%|          | 0.00/7.04k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading readme: 100%|██████████| 7.04k/7.04k [00:00<00:00, 11.4MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading builder script:   0%|          | 0.00/3.36k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading builder script: 100%|██████████| 3.36k/3.36k [00:00<00:00, 5.17MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading readme:   0%|          | 0.00/7.04k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading readme: 100%|██████████| 7.04k/7.04k [00:00<00:00, 10.3MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading builder script:   0%|          | 0.00/3.36k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading builder script: 100%|██████████| 3.36k/3.36k [00:00<00:00, 5.50MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:Downloading and preparing dataset samsum/samsum to /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e...\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:Downloading and preparing dataset samsum/samsum to /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e...\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading builder script:   0%|          | 0.00/3.36k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading builder script: 100%|██████████| 3.36k/3.36k [00:00<00:00, 5.60MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading readme:   0%|          | 0.00/7.04k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading readme: 100%|██████████| 7.04k/7.04k [00:00<00:00, 10.8MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading builder script:   0%|          | 0.00/3.36k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading builder script: 100%|██████████| 3.36k/3.36k [00:00<00:00, 5.47MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading readme:   0%|          | 0.00/7.04k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading readme: 100%|██████████| 7.04k/7.04k [00:00<00:00, 9.88MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading builder script:   0%|          | 0.00/3.36k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading builder script: 100%|██████████| 3.36k/3.36k [00:00<00:00, 5.17MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:01/24/2024 01:10:54 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/samsum/resolve/main/README.md not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpgcsoiyn5\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading readme:   0%|          | 0.00/7.04k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Downloading readme: 100%|██████████| 7.04k/7.04k [00:00<00:00, 9.97MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:01/24/2024 01:10:54 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/samsum/resolve/main/README.md in cache at /root/.cache/huggingface/datasets/downloads/b6457b928846696e9ade25081363514632deb3e3b9915653a91567d82f89dd36.a6f46b2df7c5c5a50d6ddd4ef101622d03591669c32521180dd5c54775518f41\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:01/24/2024 01:10:54 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/b6457b928846696e9ade25081363514632deb3e3b9915653a91567d82f89dd36.a6f46b2df7c5c5a50d6ddd4ef101622d03591669c32521180dd5c54775518f41\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading data:   0%|          | 0.00/2.94M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/2.94M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading builder script:   0%|          | 0.00/3.36k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Downloading builder script: 100%|██████████| 3.36k/3.36k [00:00<00:00, 5.35MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading data: 100%|██████████| 2.94M/2.94M [00:00<00:00, 107MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading data: 100%|██████████| 2.94M/2.94M [00:00<00:00, 98.3MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Generating train split:   0%|          | 0/14732 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Generating train split:   0%|          | 0/14732 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:01/24/2024 01:10:54 - INFO - datasets.builder - No config specified, defaulting to the single config: samsum/samsum\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:01/24/2024 01:10:54 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/samsum/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Generating train split:   0%|          | 1/14732 [00:00<1:14:22,  3.30 examples/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Generating train split:   0%|          | 1/14732 [00:00<1:16:42,  3.20 examples/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Generating train split:  26%|██▋       | 3901/14732 [00:00<00:00, 12499.41 examples/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Generating train split:  27%|██▋       | 3961/14732 [00:00<00:00, 12427.39 examples/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Generating train split:  54%|█████▍    | 7937/14732 [00:00<00:00, 21247.14 examples/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Generating train split:  54%|█████▍    | 8000/14732 [00:00<00:00, 20983.39 examples/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Generating train split:  81%|████████▏ | 12000/14732 [00:00<00:00, 27031.16 examples/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Generating train split:  82%|████████▏ | 12045/14732 [00:00<00:00, 26781.75 examples/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Generating test split:   0%|          | 1/819 [00:00<02:42,  5.04 examples/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Generating test split:   0%|          | 1/819 [00:00<02:41,  5.06 examples/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Generating validation split:   0%|          | 1/818 [00:00<02:41,  5.06 examples/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Generating validation split:   0%|          | 1/818 [00:00<02:41,  5.06 examples/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:Dataset samsum downloaded and prepared to /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015  0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:01/24/2024 01:10:55 - WARNING - datasets.builder - Found cached dataset samsum (/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015  0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015100%|██████████| 3/3 [00:00<00:00, 1026.67it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015100%|██████████| 3/3 [00:00<00:00, 1004.94it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:01/24/2024 01:10:55 - WARNING - datasets.builder - Found cached dataset samsum (/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015  0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:01/24/2024 01:10:55 - WARNING - datasets.builder - Found cached dataset samsum (/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:Dataset samsum downloaded and prepared to /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015  0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015  0%|          | 0/3 [00:00<?, ?it/s][1,mpirank:15,algo-2]<stderr>:#015100%|██████████| 3/3 [00:00<00:00, 962.51it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:01/24/2024 01:10:55 - WARNING - datasets.builder - Found cached dataset samsum (/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015  0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:01/24/2024 01:10:55 - WARNING - datasets.builder - Found cached dataset samsum (/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015100%|██████████| 3/3 [00:00<00:00, 968.44it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015100%|██████████| 3/3 [00:00<00:00, 1011.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015  0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015100%|██████████| 3/3 [00:00<00:00, 970.83it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:01/24/2024 01:10:55 - WARNING - datasets.builder - Found cached dataset samsum (/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015  0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015100%|██████████| 3/3 [00:00<00:00, 993.99it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:01/24/2024 01:10:55 - WARNING - datasets.builder - Found cached dataset samsum (/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015  0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015100%|██████████| 3/3 [00:00<00:00, 992.03it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015100%|██████████| 3/3 [00:00<00:00, 969.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:01/24/2024 01:10:55 - WARNING - datasets.builder - Found cached dataset samsum (/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015  0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015100%|██████████| 3/3 [00:00<00:00, 992.81it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:01/24/2024 01:10:55 - WARNING - datasets.builder - Found cached dataset samsum (/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:01/24/2024 01:10:55 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 3/3 [00:00<00:00, 958.26it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:01/24/2024 01:10:55 - WARNING - datasets.builder - Found cached dataset samsum (/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015  0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:01/24/2024 01:10:55 - WARNING - datasets.builder - Found cached dataset samsum (/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015  0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015100%|██████████| 3/3 [00:00<00:00, 1008.97it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015100%|██████████| 3/3 [00:00<00:00, 875.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:01/24/2024 01:10:55 - WARNING - datasets.builder - Found cached dataset samsum (/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015  0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:01/24/2024 01:10:55 - WARNING - datasets.builder - Found cached dataset samsum (/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015  0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015100%|██████████| 3/3 [00:00<00:00, 975.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015100%|██████████| 3/3 [00:00<00:00, 934.56it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:01/24/2024 01:10:55 - WARNING - datasets.builder - Found cached dataset samsum (/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015  0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015100%|██████████| 3/3 [00:00<00:00, 983.89it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"config.json\";:   0%|          | 0.00/1.58k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Downloading (…)\"config.json\";: 100%|██████████| 1.58k/1.58k [00:00<00:00, 729kB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:660] 2024-01-24 01:10:56,035 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/08436bb998cc59c90294f46c0ec716bf86556c71/config.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:660] 2024-01-24 01:10:56,035 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/08436bb998cc59c90294f46c0ec716bf86556c71/config.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:712] 2024-01-24 01:10:56,038 >> Model config BartConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"_name_or_path\": \"facebook/bart-large-cnn\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"_num_labels\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"activation_dropout\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"activation_function\": \"gelu\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"add_final_layer_norm\": false,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    \"BartForConditionalGeneration\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  ],\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"attention_dropout\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"classif_dropout\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"classifier_dropout\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"d_model\": 1024,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_layers\": 12,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"dropout\": 0.1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"encoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"encoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"encoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"encoder_layers\": 12,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"force_bos_token_to_be_generated\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"id2label\": {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  },\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"init_std\": 0.02,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"is_encoder_decoder\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"label2id\": {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  },\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_position_embeddings\": 1024,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"model_type\": \"bart\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"normalize_before\": false,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_hidden_layers\": 12,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"output_past\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"prefix\": \" \",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"scale_embedding\": false,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"task_specific_params\": {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    \"summarization\": {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:      \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:      \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:      \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:      \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:      \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:      \"num_beams\": 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    }\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  },\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"vocab_size\": 50264\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:712] 2024-01-24 01:10:56,038 >> Model config BartConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"_name_or_path\": \"facebook/bart-large-cnn\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"_num_labels\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"activation_dropout\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"activation_function\": \"gelu\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"add_final_layer_norm\": false,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    \"BartForConditionalGeneration\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  ],\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"attention_dropout\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"classif_dropout\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"classifier_dropout\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"d_model\": 1024,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_layers\": 12,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"dropout\": 0.1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"encoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"encoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"encoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"encoder_layers\": 12,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"force_bos_token_to_be_generated\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"id2label\": {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  },\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"init_std\": 0.02,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"is_encoder_decoder\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"label2id\": {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  },\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_position_embeddings\": 1024,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"model_type\": \"bart\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"normalize_before\": false,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_hidden_layers\": 12,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"output_past\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"prefix\": \" \",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"scale_embedding\": false,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"task_specific_params\": {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    \"summarization\": {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:      \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:      \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:      \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:      \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:      \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:      \"num_beams\": 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    }\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  },\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"vocab_size\": 50264\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|tokenization_auto.py:458] 2024-01-24 01:10:56,098 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|tokenization_auto.py:458] 2024-01-24 01:10:56,098 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:660] 2024-01-24 01:10:56,154 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/08436bb998cc59c90294f46c0ec716bf86556c71/config.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:660] 2024-01-24 01:10:56,154 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/08436bb998cc59c90294f46c0ec716bf86556c71/config.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:712] 2024-01-24 01:10:56,155 >> Model config BartConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"_name_or_path\": \"facebook/bart-large-cnn\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"_num_labels\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"activation_dropout\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"activation_function\": \"gelu\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"add_final_layer_norm\": false,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    \"BartForConditionalGeneration\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  ],\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"attention_dropout\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"classif_dropout\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"classifier_dropout\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"d_model\": 1024,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_layers\": 12,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"dropout\": 0.1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"encoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"encoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"encoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"encoder_layers\": 12,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"force_bos_token_to_be_generated\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"id2label\": {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  },\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"init_std\": 0.02,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"is_encoder_decoder\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"label2id\": {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  },\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_position_embeddings\": 1024,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"model_type\": \"bart\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"normalize_before\": false,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_hidden_layers\": 12,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"output_past\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"prefix\": \" \",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"scale_embedding\": false,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"task_specific_params\": {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    \"summarization\": {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:      \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:      \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:      \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:      \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:      \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:      \"num_beams\": 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    }\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  },\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"vocab_size\": 50264\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:712] 2024-01-24 01:10:56,155 >> Model config BartConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"_name_or_path\": \"facebook/bart-large-cnn\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"_num_labels\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"activation_dropout\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"activation_function\": \"gelu\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"add_final_layer_norm\": false,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    \"BartForConditionalGeneration\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  ],\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"attention_dropout\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"classif_dropout\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"classifier_dropout\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"d_model\": 1024,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_layers\": 12,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"dropout\": 0.1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"encoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"encoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"encoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"encoder_layers\": 12,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"force_bos_token_to_be_generated\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"id2label\": {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  },\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"init_std\": 0.02,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"is_encoder_decoder\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"label2id\": {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  },\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_position_embeddings\": 1024,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"model_type\": \"bart\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"normalize_before\": false,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_hidden_layers\": 12,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"output_past\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"prefix\": \" \",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"scale_embedding\": false,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"task_specific_params\": {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    \"summarization\": {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:      \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:      \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:      \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:      \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:      \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:      \"num_beams\": 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    }\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  },\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"vocab_size\": 50264\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"config.json\";:   0%|          | 0.00/1.58k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"config.json\";: 100%|██████████| 1.58k/1.58k [00:00<00:00, 789kB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"vocab.json\";:   0%|          | 0.00/899k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"vocab.json\";: 100%|██████████| 899k/899k [00:00<00:00, 62.8MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"merges.txt\";:   0%|          | 0.00/456k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"merges.txt\";: 100%|██████████| 456k/456k [00:00<00:00, 31.2MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading (…)\"vocab.json\";:   0%|          | 0.00/899k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading (…)\"vocab.json\";: 100%|██████████| 899k/899k [00:00<00:00, 22.3MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"tokenizer.json\";:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"tokenizer.json\";: 100%|██████████| 1.36M/1.36M [00:00<00:00, 27.9MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"merges.txt\";:   0%|          | 0.00/456k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"merges.txt\";: 100%|██████████| 456k/456k [00:00<00:00, 63.0MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|tokenization_utils_base.py:1802] 2024-01-24 01:10:56,775 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/08436bb998cc59c90294f46c0ec716bf86556c71/vocab.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|tokenization_utils_base.py:1802] 2024-01-24 01:10:56,775 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/08436bb998cc59c90294f46c0ec716bf86556c71/merges.txt\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|tokenization_utils_base.py:1802] 2024-01-24 01:10:56,775 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/08436bb998cc59c90294f46c0ec716bf86556c71/vocab.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|tokenization_utils_base.py:1802] 2024-01-24 01:10:56,775 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/08436bb998cc59c90294f46c0ec716bf86556c71/merges.txt\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|tokenization_utils_base.py:1802] 2024-01-24 01:10:56,776 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/08436bb998cc59c90294f46c0ec716bf86556c71/tokenizer.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|tokenization_utils_base.py:1802] 2024-01-24 01:10:56,776 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/08436bb998cc59c90294f46c0ec716bf86556c71/tokenizer.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|tokenization_utils_base.py:1802] 2024-01-24 01:10:56,776 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|tokenization_utils_base.py:1802] 2024-01-24 01:10:56,776 >> loading file special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|tokenization_utils_base.py:1802] 2024-01-24 01:10:56,776 >> loading file tokenizer_config.json from cache at None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|tokenization_utils_base.py:1802] 2024-01-24 01:10:56,776 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|tokenization_utils_base.py:1802] 2024-01-24 01:10:56,776 >> loading file special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|tokenization_utils_base.py:1802] 2024-01-24 01:10:56,776 >> loading file tokenizer_config.json from cache at None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:660] 2024-01-24 01:10:56,776 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/08436bb998cc59c90294f46c0ec716bf86556c71/config.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:660] 2024-01-24 01:10:56,776 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/08436bb998cc59c90294f46c0ec716bf86556c71/config.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:712] 2024-01-24 01:10:56,777 >> Model config BartConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"_name_or_path\": \"facebook/bart-large-cnn\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"_num_labels\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"activation_dropout\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"activation_function\": \"gelu\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"add_final_layer_norm\": false,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    \"BartForConditionalGeneration\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  ],\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"attention_dropout\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"classif_dropout\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"classifier_dropout\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"d_model\": 1024,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_layers\": 12,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"dropout\": 0.1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"encoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"encoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"encoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"encoder_layers\": 12,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"force_bos_token_to_be_generated\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"id2label\": {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  },\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"init_std\": 0.02,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"is_encoder_decoder\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"label2id\": {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  },\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_position_embeddings\": 1024,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"model_type\": \"bart\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"normalize_before\": false,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_hidden_layers\": 12,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"output_past\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"prefix\": \" \",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"scale_embedding\": false,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"task_specific_params\": {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    \"summarization\": {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:      \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:      \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:      \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:      \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:      \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:      \"num_beams\": 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:    }\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  },\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"vocab_size\": 50264\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:712] 2024-01-24 01:10:56,777 >> Model config BartConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"_name_or_path\": \"facebook/bart-large-cnn\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"_num_labels\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"activation_dropout\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"activation_function\": \"gelu\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"add_final_layer_norm\": false,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    \"BartForConditionalGeneration\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  ],\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"attention_dropout\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"classif_dropout\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"classifier_dropout\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"d_model\": 1024,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_layers\": 12,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"dropout\": 0.1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"encoder_attention_heads\": 16,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"encoder_ffn_dim\": 4096,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"encoder_layerdrop\": 0.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"encoder_layers\": 12,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"force_bos_token_to_be_generated\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"gradient_checkpointing\": false,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"id2label\": {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    \"1\": \"LABEL_1\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    \"2\": \"LABEL_2\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  },\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"init_std\": 0.02,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"is_encoder_decoder\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"label2id\": {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    \"LABEL_1\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    \"LABEL_2\": 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  },\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_position_embeddings\": 1024,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"model_type\": \"bart\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"normalize_before\": false,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_hidden_layers\": 12,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"output_past\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"prefix\": \" \",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"scale_embedding\": false,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"task_specific_params\": {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    \"summarization\": {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:      \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:      \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:      \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:      \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:      \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:      \"num_beams\": 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    }\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  },\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\",\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"vocab_size\": 50264\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"tokenizer.json\";:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"tokenizer.json\";: 100%|██████████| 1.36M/1.36M [00:00<00:00, 33.6MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/1.63G [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   3%|▎         | 41.9M/1.63G [00:00<00:04, 340MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/1.63G [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   6%|▌         | 94.4M/1.63G [00:00<00:03, 401MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   1%|▏         | 21.0M/1.63G [00:00<00:07, 207MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   3%|▎         | 52.4M/1.63G [00:00<00:06, 249MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   8%|▊         | 136M/1.63G [00:00<00:04, 320MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   5%|▌         | 83.9M/1.63G [00:00<00:05, 274MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  11%|█         | 178M/1.63G [00:00<00:04, 320MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:   8%|▊         | 126M/1.63G [00:00<00:04, 301MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  14%|█▎        | 220M/1.63G [00:00<00:04, 339MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  10%|█         | 168M/1.63G [00:00<00:04, 332MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  16%|█▌        | 262M/1.63G [00:00<00:03, 347MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  13%|█▎        | 210M/1.63G [00:00<00:04, 319MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  19%|█▊        | 304M/1.63G [00:00<00:03, 330MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  15%|█▌        | 252M/1.63G [00:00<00:04, 310MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  21%|██▏       | 346M/1.63G [00:01<00:03, 348MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  17%|█▋        | 283M/1.63G [00:00<00:04, 293MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  25%|██▍       | 398M/1.63G [00:01<00:03, 363MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  19%|█▉        | 315M/1.63G [00:01<00:04, 288MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  27%|██▋       | 440M/1.63G [00:01<00:03, 354MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  21%|██▏       | 346M/1.63G [00:01<00:04, 294MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  30%|██▉       | 482M/1.63G [00:01<00:03, 362MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  24%|██▍       | 388M/1.63G [00:01<00:03, 310MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  32%|███▏      | 524M/1.63G [00:01<00:03, 341MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  26%|██▋       | 430M/1.63G [00:01<00:03, 328MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  35%|███▍      | 566M/1.63G [00:01<00:02, 359MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  29%|██▉       | 472M/1.63G [00:01<00:03, 319MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  37%|███▋      | 608M/1.63G [00:01<00:02, 350MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  32%|███▏      | 524M/1.63G [00:01<00:03, 363MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  35%|███▍      | 566M/1.63G [00:01<00:03, 350MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  40%|████      | 650M/1.63G [00:01<00:03, 274MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  37%|███▋      | 608M/1.63G [00:01<00:03, 324MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  42%|████▏     | 682M/1.63G [00:02<00:03, 238MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  40%|████      | 650M/1.63G [00:02<00:03, 307MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  44%|████▍     | 713M/1.63G [00:02<00:04, 216MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  43%|████▎     | 692M/1.63G [00:02<00:02, 320MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  46%|████▋     | 755M/1.63G [00:02<00:03, 240MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  45%|████▌     | 734M/1.63G [00:02<00:02, 321MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  48%|████▊     | 776M/1.63G [00:02<00:02, 328MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  48%|████▊     | 786M/1.63G [00:02<00:03, 231MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  50%|█████     | 818M/1.63G [00:02<00:03, 230MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  50%|█████     | 818M/1.63G [00:02<00:02, 311MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  53%|█████▎    | 860M/1.63G [00:02<00:02, 256MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  53%|█████▎    | 860M/1.63G [00:02<00:02, 322MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  55%|█████▍    | 891M/1.63G [00:03<00:02, 262MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  55%|█████▌    | 902M/1.63G [00:02<00:02, 320MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  57%|█████▋    | 923M/1.63G [00:03<00:02, 242MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  58%|█████▊    | 944M/1.63G [00:03<00:02, 308MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  59%|█████▉    | 965M/1.63G [00:03<00:02, 283MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  60%|██████    | 975M/1.63G [00:03<00:02, 266MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  62%|██████▏   | 1.01G/1.63G [00:03<00:02, 303MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  62%|██████▏   | 1.01G/1.63G [00:03<00:02, 271MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  65%|██████▍   | 1.05G/1.63G [00:03<00:01, 306MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  65%|██████▍   | 1.05G/1.63G [00:03<00:02, 281MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  66%|██████▋   | 1.08G/1.63G [00:03<00:01, 302MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  66%|██████▋   | 1.08G/1.63G [00:03<00:01, 278MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  69%|██████▉   | 1.12G/1.63G [00:03<00:01, 292MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  68%|██████▊   | 1.11G/1.63G [00:03<00:01, 273MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  71%|███████   | 1.15G/1.63G [00:03<00:01, 287MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  70%|███████   | 1.14G/1.63G [00:03<00:01, 274MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  73%|███████▎  | 1.18G/1.63G [00:04<00:01, 287MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  72%|███████▏  | 1.17G/1.63G [00:03<00:01, 283MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  75%|███████▍  | 1.22G/1.63G [00:04<00:01, 276MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  74%|███████▍  | 1.21G/1.63G [00:04<00:01, 269MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  77%|███████▋  | 1.25G/1.63G [00:04<00:01, 256MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  76%|███████▌  | 1.24G/1.63G [00:04<00:01, 262MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  79%|███████▉  | 1.29G/1.63G [00:04<00:01, 291MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  79%|███████▊  | 1.28G/1.63G [00:04<00:01, 263MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  82%|████████▏ | 1.33G/1.63G [00:04<00:00, 310MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  81%|████████  | 1.31G/1.63G [00:04<00:01, 260MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  85%|████████▍ | 1.37G/1.63G [00:04<00:00, 301MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  83%|████████▎ | 1.34G/1.63G [00:04<00:01, 265MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  87%|████████▋ | 1.42G/1.63G [00:04<00:00, 315MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  85%|████████▌ | 1.38G/1.63G [00:04<00:00, 276MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  88%|████████▊ | 1.43G/1.63G [00:04<00:00, 296MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  90%|████████▉ | 1.46G/1.63G [00:05<00:00, 248MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  90%|█████████ | 1.47G/1.63G [00:04<00:00, 324MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  93%|█████████▎| 1.51G/1.63G [00:05<00:00, 346MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  92%|█████████▏| 1.49G/1.63G [00:05<00:00, 235MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  94%|█████████▎| 1.52G/1.63G [00:05<00:00, 243MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  95%|█████████▌| 1.55G/1.63G [00:05<00:00, 312MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  96%|█████████▌| 1.56G/1.63G [00:05<00:00, 266MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  98%|█████████▊| 1.59G/1.63G [00:05<00:00, 312MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 1.63G/1.63G [00:05<00:00, 302MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";:  98%|█████████▊| 1.59G/1.63G [00:05<00:00, 215MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 1.63G/1.63G [00:05<00:00, 232MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 1.63G/1.63G [00:05<00:00, 282MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|modeling_utils.py:2275] 2024-01-24 01:11:02,720 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/08436bb998cc59c90294f46c0ec716bf86556c71/pytorch_model.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|modeling_utils.py:2275] 2024-01-24 01:11:02,720 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/08436bb998cc59c90294f46c0ec716bf86556c71/pytorch_model.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:11:03,819 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:11:03,819 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)ration_config.json\";:   0%|          | 0.00/363 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Downloading (…)ration_config.json\";: 100%|██████████| 363/363 [00:00<00:00, 53.9kB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)ration_config.json\";:   0%|          | 0.00/363 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading (…)ration_config.json\";: 100%|██████████| 363/363 [00:00<00:00, 53.9kB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|modeling_utils.py:2857] 2024-01-24 01:11:08,612 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|modeling_utils.py:2857] 2024-01-24 01:11:08,612 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|modeling_utils.py:2865] 2024-01-24 01:11:08,612 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|modeling_utils.py:2865] 2024-01-24 01:11:08,612 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:507] 2024-01-24 01:11:08,684 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/08436bb998cc59c90294f46c0ec716bf86556c71/generation_config.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:507] 2024-01-24 01:11:08,684 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/08436bb998cc59c90294f46c0ec716bf86556c71/generation_config.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:11:08,684 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"_from_model_config\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:11:08,684 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"_from_model_config\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/15 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:01/24/2024 01:11:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-2b921d6fd552f02d.arrow\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 1/15 [00:00<00:02,  6.58ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  13%|█▎        | 2/15 [00:00<00:02,  4.99ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 3/15 [00:00<00:02,  5.99ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 4/15 [00:00<00:01,  6.58ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 5/15 [00:00<00:01,  6.95ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  40%|████      | 6/15 [00:00<00:01,  7.27ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 7/15 [00:01<00:01,  7.40ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 8/15 [00:01<00:00,  7.62ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 9/15 [00:01<00:00,  7.56ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 10/15 [00:01<00:00,  7.58ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 11/15 [00:01<00:00,  7.68ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  80%|████████  | 12/15 [00:01<00:00,  7.63ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  87%|████████▋ | 13/15 [00:01<00:00,  6.29ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 14/15 [00:02<00:00,  6.68ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 15/15 [00:02<00:00,  7.06ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:01/24/2024 01:11:11 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-2b921d6fd552f02d.arrow\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:01/24/2024 01:11:11 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-2b921d6fd552f02d.arrow\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:01/24/2024 01:11:11 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-2b921d6fd552f02d.arrow\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:01/24/2024 01:11:11 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-2b921d6fd552f02d.arrow\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:01/24/2024 01:11:11 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-2b921d6fd552f02d.arrow\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/15 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:01/24/2024 01:11:11 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-2b921d6fd552f02d.arrow\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:01/24/2024 01:11:11 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-2b921d6fd552f02d.arrow\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/15 [00:00<?, ?ba/s][1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/15 [00:00<?, ?ba/s][1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/15 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/15 [00:00<?, ?ba/s][1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/15 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/15 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/15 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:01/24/2024 01:11:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-0a2ffe23d601d189.arrow\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  9.54ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  9.52ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 1/15 [00:00<00:02,  6.65ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 1/15 [00:00<00:02,  6.49ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 1/15 [00:00<00:02,  6.51ba/s][1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 1/15 [00:00<00:02,  6.50ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 1/15 [00:00<00:02,  6.45ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 1/15 [00:00<00:02,  6.46ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 1/15 [00:00<00:02,  6.45ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 1/15 [00:00<00:02,  6.47ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  13%|█▎        | 2/15 [00:00<00:01,  6.77ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  13%|█▎        | 2/15 [00:00<00:01,  6.74ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  13%|█▎        | 2/15 [00:00<00:01,  6.76ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  13%|█▎        | 2/15 [00:00<00:01,  6.73ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  13%|█▎        | 2/15 [00:00<00:01,  6.72ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  13%|█▎        | 2/15 [00:00<00:02,  5.87ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  13%|█▎        | 2/15 [00:00<00:02,  4.67ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  13%|█▎        | 2/15 [00:00<00:02,  4.64ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 3/15 [00:00<00:02,  5.65ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 3/15 [00:00<00:02,  5.19ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 3/15 [00:00<00:02,  5.61ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 3/15 [00:00<00:02,  5.16ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 3/15 [00:00<00:02,  5.14ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 3/15 [00:00<00:02,  5.13ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 3/15 [00:00<00:02,  4.85ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 3/15 [00:00<00:02,  4.67ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 4/15 [00:00<00:01,  6.21ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 4/15 [00:00<00:01,  5.83ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 4/15 [00:00<00:01,  5.80ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 4/15 [00:00<00:01,  5.81ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 4/15 [00:00<00:01,  6.10ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 4/15 [00:00<00:01,  5.76ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 4/15 [00:00<00:01,  5.58ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 4/15 [00:00<00:02,  5.41ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 5/15 [00:00<00:01,  6.56ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 5/15 [00:00<00:01,  6.29ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 5/15 [00:00<00:01,  6.25ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 5/15 [00:00<00:01,  6.22ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 5/15 [00:00<00:01,  6.21ba/s][1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 5/15 [00:00<00:01,  6.45ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 5/15 [00:00<00:01,  5.89ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 5/15 [00:00<00:01,  5.63ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  40%|████      | 6/15 [00:00<00:01,  6.85ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  40%|████      | 6/15 [00:00<00:01,  6.61ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  40%|████      | 6/15 [00:00<00:01,  6.60ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  40%|████      | 6/15 [00:00<00:01,  6.74ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  40%|████      | 6/15 [00:00<00:01,  6.57ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  40%|████      | 6/15 [00:00<00:01,  6.05ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  40%|████      | 6/15 [00:01<00:01,  6.32ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  40%|████      | 6/15 [00:01<00:01,  6.13ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 7/15 [00:01<00:01,  7.00ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 7/15 [00:01<00:01,  6.78ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 7/15 [00:01<00:01,  6.76ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 7/15 [00:01<00:01,  6.75ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 7/15 [00:01<00:01,  6.84ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 7/15 [00:01<00:01,  6.54ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 7/15 [00:01<00:01,  6.45ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 7/15 [00:01<00:01,  5.78ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 8/15 [00:01<00:00,  7.15ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 8/15 [00:01<00:00,  7.02ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 8/15 [00:01<00:00,  7.01ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 8/15 [00:01<00:01,  6.99ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 8/15 [00:01<00:01,  6.97ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 8/15 [00:01<00:01,  6.84ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 8/15 [00:01<00:01,  6.76ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 8/15 [00:01<00:01,  6.20ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 9/15 [00:01<00:00,  7.06ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 9/15 [00:01<00:00,  7.05ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 9/15 [00:01<00:00,  7.01ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 9/15 [00:01<00:00,  6.93ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 9/15 [00:01<00:00,  6.57ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 9/15 [00:01<00:00,  6.91ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 9/15 [00:01<00:00,  6.85ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 9/15 [00:01<00:00,  6.46ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 10/15 [00:01<00:00,  7.06ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 10/15 [00:01<00:00,  7.06ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 10/15 [00:01<00:00,  6.97ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 10/15 [00:01<00:00,  6.93ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 10/15 [00:01<00:00,  6.74ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 10/15 [00:01<00:00,  6.15ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 10/15 [00:01<00:00,  6.94ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 10/15 [00:01<00:00,  6.62ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 11/15 [00:01<00:00,  7.18ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 11/15 [00:01<00:00,  7.15ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 11/15 [00:01<00:00,  7.10ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 11/15 [00:01<00:00,  7.04ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 11/15 [00:01<00:00,  7.10ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 11/15 [00:01<00:00,  6.91ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 11/15 [00:01<00:00,  6.82ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 11/15 [00:01<00:00,  5.92ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  80%|████████  | 12/15 [00:01<00:00,  7.16ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  80%|████████  | 12/15 [00:01<00:00,  7.14ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  80%|████████  | 12/15 [00:01<00:00,  7.10ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  80%|████████  | 12/15 [00:01<00:00,  7.02ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  80%|████████  | 12/15 [00:01<00:00,  7.09ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  80%|████████  | 12/15 [00:01<00:00,  6.93ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  80%|████████  | 12/15 [00:01<00:00,  6.82ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  87%|████████▋ | 13/15 [00:01<00:00,  7.18ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  87%|████████▋ | 13/15 [00:01<00:00,  7.14ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  87%|████████▋ | 13/15 [00:01<00:00,  7.12ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  80%|████████  | 12/15 [00:01<00:00,  5.69ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  87%|████████▋ | 13/15 [00:01<00:00,  7.14ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  87%|████████▋ | 13/15 [00:02<00:00,  6.94ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  87%|████████▋ | 13/15 [00:02<00:00,  6.89ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 14/15 [00:02<00:00,  7.19ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  87%|████████▋ | 13/15 [00:02<00:00,  5.71ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 14/15 [00:02<00:00,  7.18ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 14/15 [00:02<00:00,  7.16ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 14/15 [00:02<00:00,  7.21ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  87%|████████▋ | 13/15 [00:02<00:00,  5.63ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 14/15 [00:02<00:00,  6.99ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 15/15 [00:02<00:00,  7.82ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 15/15 [00:02<00:00,  6.94ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 15/15 [00:02<00:00,  7.80ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 15/15 [00:02<00:00,  6.91ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 14/15 [00:02<00:00,  6.95ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 15/15 [00:02<00:00,  7.77ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 15/15 [00:02<00:00,  6.88ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 14/15 [00:02<00:00,  6.07ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 15/15 [00:02<00:00,  7.81ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 15/15 [00:02<00:00,  6.70ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 15/15 [00:02<00:00,  7.61ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 15/15 [00:02<00:00,  6.66ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 15/15 [00:02<00:00,  7.60ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 15/15 [00:02<00:00,  6.59ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 14/15 [00:02<00:00,  5.96ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 15/15 [00:02<00:00,  6.85ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 15/15 [00:02<00:00,  6.50ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 15/15 [00:02<00:00,  6.29ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s][1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s][1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s][1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s][1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on prediction dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:01/24/2024 01:11:14 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-3e5f4a2235591c86.arrow\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  9.33ba/s][1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  9.31ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  9.31ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  9.29ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  9.29ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  9.27ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  9.16ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  9.15ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  9.22ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  9.20ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  9.18ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  9.15ba/s][1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  9.17ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  9.13ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  8.92ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  8.90ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on prediction dataset: 100%|██████████| 1/1 [00:00<00:00,  8.83ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on prediction dataset: 100%|██████████| 1/1 [00:00<00:00,  8.81ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  7.94ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  7.93ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  7.91ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  7.90ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  7.86ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  7.84ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  7.91ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  7.90ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  7.81ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  7.80ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  7.77ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  7.76ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  6.55ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  6.54ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:01/24/2024 01:11:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-3e5f4a2235591c86.arrow\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:01/24/2024 01:11:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-3e5f4a2235591c86.arrow\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:01/24/2024 01:11:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-3e5f4a2235591c86.arrow\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:01/24/2024 01:11:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-3e5f4a2235591c86.arrow\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:01/24/2024 01:11:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-3e5f4a2235591c86.arrow\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:01/24/2024 01:11:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-3e5f4a2235591c86.arrow\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on prediction dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on prediction dataset:   0%|          | 0/1 [00:00<?, ?ba/s][1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on prediction dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on prediction dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on prediction dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on prediction dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on prediction dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on prediction dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:01/24/2024 01:11:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-3e5f4a2235591c86.arrow\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on prediction dataset: 100%|██████████| 1/1 [00:00<00:00,  8.93ba/s][1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on prediction dataset: 100%|██████████| 1/1 [00:00<00:00,  8.91ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on prediction dataset: 100%|██████████| 1/1 [00:00<00:00,  8.89ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on prediction dataset: 100%|██████████| 1/1 [00:00<00:00,  8.87ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on prediction dataset: 100%|██████████| 1/1 [00:00<00:00,  8.84ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on prediction dataset: 100%|██████████| 1/1 [00:00<00:00,  8.82ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on prediction dataset: 100%|██████████| 1/1 [00:00<00:00,  8.85ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on prediction dataset: 100%|██████████| 1/1 [00:00<00:00,  8.84ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on prediction dataset: 100%|██████████| 1/1 [00:00<00:00,  8.88ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on prediction dataset: 100%|██████████| 1/1 [00:00<00:00,  8.86ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on prediction dataset: 100%|██████████| 1/1 [00:00<00:00,  8.75ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on prediction dataset: 100%|██████████| 1/1 [00:00<00:00,  8.73ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on prediction dataset: 100%|██████████| 1/1 [00:00<00:00,  8.71ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on prediction dataset: 100%|██████████| 1/1 [00:00<00:00,  8.69ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on prediction dataset: 100%|██████████| 1/1 [00:00<00:00,  4.61ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on prediction dataset: 100%|██████████| 1/1 [00:00<00:00,  4.60ba/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 9.70MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 9.54MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 9.61MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 9.22MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 9.22MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 9.28MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 7.35MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 7.53MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 7.86MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 7.64MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 7.21MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 6.98MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:565] 2024-01-24 01:11:14,928 >> Using cuda_amp half precision backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:565] 2024-01-24 01:11:14,928 >> Using cuda_amp half precision backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:1650] 2024-01-24 01:11:15,460 >> ***** Running training *****\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:1651] 2024-01-24 01:11:15,460 >>   Num examples = 14731\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:1652] 2024-01-24 01:11:15,460 >>   Num Epochs = 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:1650] 2024-01-24 01:11:15,460 >> ***** Running training *****\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:1651] 2024-01-24 01:11:15,460 >>   Num examples = 14731\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:1652] 2024-01-24 01:11:15,460 >>   Num Epochs = 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:1653] 2024-01-24 01:11:15,460 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:1654] 2024-01-24 01:11:15,460 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:1655] 2024-01-24 01:11:15,460 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:1656] 2024-01-24 01:11:15,460 >>   Total optimization steps = 693\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:1653] 2024-01-24 01:11:15,460 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:1654] 2024-01-24 01:11:15,460 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:1655] 2024-01-24 01:11:15,460 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:1656] 2024-01-24 01:11:15,460 >>   Total optimization steps = 693\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:1657] 2024-01-24 01:11:15,461 >>   Number of trainable parameters = 406291456\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:1657] 2024-01-24 01:11:15,461 >>   Number of trainable parameters = 406291456\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/693 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2024-01-24 01:11:15.543: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2024-01-24 01:11:15.564: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2024-01-24 01:11:15.564: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2024-01-24 01:11:15.564: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2024-01-24 01:11:15.564: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2024-01-24 01:11:15.565: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2024-01-24 01:11:15.570: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2024-01-24 01:11:15.570: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2024-01-24 01:11:15.570: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2024-01-24 01:11:15.570: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2024-01-24 01:11:15.570: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2024-01-24 01:11:15.570: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2024-01-24 01:11:15.570: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2024-01-24 01:11:15.571: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2024-01-24 01:11:15.574: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2024-01-24 01:11:15.576 algo-2:359 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2024-01-24 01:11:15.604 algo-1:295 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2024-01-24 01:11:15.605 algo-1:214 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2024-01-24 01:11:15.612 algo-2:359 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2024-01-24 01:11:15.612 algo-2:359 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2024-01-24 01:11:15.613 algo-2:359 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2024-01-24 01:11:15.613 algo-2:359 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2024-01-24 01:11:15.614 algo-2:359 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[WARNING|logging.py:281] 2024-01-24 01:11:15,616 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[WARNING|logging.py:281] 2024-01-24 01:11:15,616 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2024-01-24 01:11:15.641 algo-1:295 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2024-01-24 01:11:15.641 algo-1:214 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2024-01-24 01:11:15.641 algo-1:295 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2024-01-24 01:11:15.641 algo-1:214 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2024-01-24 01:11:15.642 algo-1:295 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2024-01-24 01:11:15.642 algo-1:214 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2024-01-24 01:11:15.642 algo-1:214 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2024-01-24 01:11:15.642 algo-1:214 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2024-01-24 01:11:15.642 algo-1:295 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2024-01-24 01:11:15.643 algo-1:295 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:[WARNING|logging.py:281] 2024-01-24 01:11:15,645 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[WARNING|logging.py:281] 2024-01-24 01:11:15,645 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:[WARNING|logging.py:281] 2024-01-24 01:11:15,646 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[WARNING|logging.py:281] 2024-01-24 01:11:15,646 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2024-01-24 01:11:15.671: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2024-01-24 01:11:15.705 algo-2:278 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2024-01-24 01:11:15.706 algo-2:256 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2024-01-24 01:11:15.708 algo-2:293 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2024-01-24 01:11:15.708 algo-2:255 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2024-01-24 01:11:15.708 algo-2:253 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2024-01-24 01:11:15.708 algo-2:279 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2024-01-24 01:11:15.714 algo-1:215 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2024-01-24 01:11:15.714 algo-1:190 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2024-01-24 01:11:15.715 algo-1:213 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2024-01-24 01:11:15.715 algo-1:200 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2024-01-24 01:11:15.716 algo-1:221 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2024-01-24 01:11:15.719 algo-2:292 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2024-01-24 01:11:15.720 algo-1:201 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2024-01-24 01:11:15.739 algo-2:278 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2024-01-24 01:11:15.739 algo-2:256 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2024-01-24 01:11:15.739 algo-2:278 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2024-01-24 01:11:15.739 algo-2:278 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2024-01-24 01:11:15.739 algo-2:256 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2024-01-24 01:11:15.740 algo-2:256 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2024-01-24 01:11:15.740 algo-2:278 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2024-01-24 01:11:15.740 algo-2:278 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2024-01-24 01:11:15.740 algo-2:256 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2024-01-24 01:11:15.740 algo-2:256 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2024-01-24 01:11:15.741 algo-2:253 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2024-01-24 01:11:15.742 algo-2:253 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:[WARNING|logging.py:281] 2024-01-24 01:11:15,742 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[WARNING|logging.py:281] 2024-01-24 01:11:15,742 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2024-01-24 01:11:15.742 algo-2:253 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[WARNING|logging.py:281] 2024-01-24 01:11:15,742 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2024-01-24 01:11:15.742 algo-2:255 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2024-01-24 01:11:15.742 algo-2:293 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2024-01-24 01:11:15.742 algo-2:279 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:[WARNING|logging.py:281] 2024-01-24 01:11:15,742 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2024-01-24 01:11:15.743 algo-2:253 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2024-01-24 01:11:15.743 algo-2:253 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2024-01-24 01:11:15.743 algo-2:293 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2024-01-24 01:11:15.743 algo-2:255 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2024-01-24 01:11:15.743 algo-2:279 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2024-01-24 01:11:15.743 algo-2:293 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2024-01-24 01:11:15.743 algo-2:255 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2024-01-24 01:11:15.743 algo-2:279 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2024-01-24 01:11:15.744 algo-2:293 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2024-01-24 01:11:15.744 algo-2:279 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2024-01-24 01:11:15.744 algo-2:255 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2024-01-24 01:11:15.744 algo-2:279 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2024-01-24 01:11:15.744 algo-2:293 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2024-01-24 01:11:15.744 algo-2:255 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:[WARNING|logging.py:281] 2024-01-24 01:11:15,745 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[WARNING|logging.py:281] 2024-01-24 01:11:15,745 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[WARNING|logging.py:281] 2024-01-24 01:11:15,746 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:[WARNING|logging.py:281] 2024-01-24 01:11:15,746 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:[WARNING|logging.py:281] 2024-01-24 01:11:15,746 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[WARNING|logging.py:281] 2024-01-24 01:11:15,746 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[WARNING|logging.py:281] 2024-01-24 01:11:15,746 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:[WARNING|logging.py:281] 2024-01-24 01:11:15,746 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2024-01-24 01:11:15.748 algo-1:215 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2024-01-24 01:11:15.748 algo-1:200 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2024-01-24 01:11:15.748 algo-1:190 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2024-01-24 01:11:15.748 algo-1:213 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2024-01-24 01:11:15.749 algo-1:215 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2024-01-24 01:11:15.749 algo-1:200 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2024-01-24 01:11:15.749 algo-1:215 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2024-01-24 01:11:15.749 algo-1:190 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2024-01-24 01:11:15.749 algo-1:213 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2024-01-24 01:11:15.749 algo-1:200 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2024-01-24 01:11:15.749 algo-1:190 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2024-01-24 01:11:15.749 algo-1:213 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2024-01-24 01:11:15.749 algo-1:221 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2024-01-24 01:11:15.749 algo-1:215 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2024-01-24 01:11:15.750 algo-1:215 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2024-01-24 01:11:15.750 algo-1:200 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2024-01-24 01:11:15.750 algo-1:190 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2024-01-24 01:11:15.750 algo-1:213 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2024-01-24 01:11:15.750 algo-1:200 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2024-01-24 01:11:15.750 algo-1:190 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2024-01-24 01:11:15.750 algo-1:213 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2024-01-24 01:11:15.750 algo-1:221 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2024-01-24 01:11:15.750 algo-1:221 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2024-01-24 01:11:15.751 algo-1:221 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2024-01-24 01:11:15.751 algo-1:221 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:[WARNING|logging.py:281] 2024-01-24 01:11:15,752 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[WARNING|logging.py:281] 2024-01-24 01:11:15,752 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:[WARNING|logging.py:281] 2024-01-24 01:11:15,752 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:[WARNING|logging.py:281] 2024-01-24 01:11:15,752 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:[WARNING|logging.py:281] 2024-01-24 01:11:15,752 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[WARNING|logging.py:281] 2024-01-24 01:11:15,752 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[WARNING|logging.py:281] 2024-01-24 01:11:15,752 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[WARNING|logging.py:281] 2024-01-24 01:11:15,752 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:[WARNING|logging.py:281] 2024-01-24 01:11:15,753 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[WARNING|logging.py:281] 2024-01-24 01:11:15,753 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2024-01-24 01:11:15.754 algo-2:292 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2024-01-24 01:11:15.755 algo-2:292 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2024-01-24 01:11:15.755 algo-2:292 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2024-01-24 01:11:15.755 algo-2:292 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2024-01-24 01:11:15.756 algo-2:292 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2024-01-24 01:11:15.755 algo-1:201 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2024-01-24 01:11:15.756 algo-1:201 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2024-01-24 01:11:15.756 algo-1:201 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2024-01-24 01:11:15.757 algo-1:201 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2024-01-24 01:11:15.757 algo-1:201 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:[WARNING|logging.py:281] 2024-01-24 01:11:15,758 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[WARNING|logging.py:281] 2024-01-24 01:11:15,758 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:[WARNING|logging.py:281] 2024-01-24 01:11:15,759 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[WARNING|logging.py:281] 2024-01-24 01:11:15,759 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 1/693 [00:01<19:15,  1.67s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 2/693 [00:02<11:46,  1.02s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 3/693 [00:02<08:19,  1.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  1%|          | 4/693 [00:03<07:00,  1.64it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  1%|          | 5/693 [00:03<06:18,  1.82it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  1%|          | 6/693 [00:03<05:51,  1.96it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  1%|          | 7/693 [00:04<05:33,  2.05it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  1%|          | 8/693 [00:04<05:18,  2.15it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  1%|▏         | 9/693 [00:05<05:09,  2.21it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  1%|▏         | 10/693 [00:05<05:03,  2.25it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  2%|▏         | 11/693 [00:06<04:58,  2.29it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  2%|▏         | 12/693 [00:06<04:53,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  2%|▏         | 13/693 [00:06<04:57,  2.28it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  2%|▏         | 14/693 [00:07<04:55,  2.29it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  2%|▏         | 15/693 [00:07<04:58,  2.27it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  2%|▏         | 16/693 [00:08<04:55,  2.29it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  2%|▏         | 17/693 [00:08<04:50,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  3%|▎         | 18/693 [00:09<04:55,  2.28it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  3%|▎         | 19/693 [00:09<04:49,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  3%|▎         | 20/693 [00:09<04:49,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  3%|▎         | 21/693 [00:10<04:48,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  3%|▎         | 22/693 [00:10<04:46,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  3%|▎         | 23/693 [00:11<04:42,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  3%|▎         | 24/693 [00:11<04:43,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  4%|▎         | 25/693 [00:12<04:43,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  4%|▍         | 26/693 [00:12<04:45,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  4%|▍         | 27/693 [00:12<04:44,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  4%|▍         | 28/693 [00:13<04:45,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  4%|▍         | 29/693 [00:13<04:45,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  4%|▍         | 30/693 [00:14<04:44,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  4%|▍         | 31/693 [00:14<04:43,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  5%|▍         | 32/693 [00:15<04:42,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  5%|▍         | 33/693 [00:15<04:43,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  5%|▍         | 34/693 [00:15<04:43,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  5%|▌         | 35/693 [00:16<04:44,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  5%|▌         | 36/693 [00:16<04:45,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  5%|▌         | 37/693 [00:17<04:42,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  5%|▌         | 38/693 [00:17<04:44,  2.30it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  6%|▌         | 39/693 [00:18<04:42,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  6%|▌         | 40/693 [00:18<04:41,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  6%|▌         | 41/693 [00:18<04:40,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  6%|▌         | 42/693 [00:19<04:42,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  6%|▌         | 43/693 [00:19<04:40,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  6%|▋         | 44/693 [00:20<04:41,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  6%|▋         | 45/693 [00:20<04:40,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  7%|▋         | 46/693 [00:21<04:35,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  7%|▋         | 47/693 [00:21<04:31,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  7%|▋         | 48/693 [00:21<04:30,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  7%|▋         | 49/693 [00:22<04:28,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  7%|▋         | 50/693 [00:22<04:30,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  7%|▋         | 51/693 [00:23<04:31,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 52/693 [00:23<04:30,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 53/693 [00:24<04:29,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 54/693 [00:24<04:27,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 55/693 [00:24<04:34,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 56/693 [00:25<04:21,  2.44it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 57/693 [00:25<04:24,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 58/693 [00:26<04:22,  2.42it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  9%|▊         | 59/693 [00:26<04:24,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  9%|▊         | 60/693 [00:26<04:26,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  9%|▉         | 61/693 [00:27<04:27,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  9%|▉         | 62/693 [00:27<04:33,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  9%|▉         | 63/693 [00:28<04:31,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  9%|▉         | 64/693 [00:28<04:30,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  9%|▉         | 65/693 [00:29<04:31,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 10%|▉         | 66/693 [00:29<04:32,  2.30it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 10%|▉         | 67/693 [00:30<04:30,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 10%|▉         | 68/693 [00:30<04:29,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 10%|▉         | 69/693 [00:30<04:28,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 10%|█         | 70/693 [00:31<04:31,  2.29it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 10%|█         | 71/693 [00:31<04:29,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 10%|█         | 72/693 [00:32<04:29,  2.30it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 11%|█         | 73/693 [00:32<04:27,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 11%|█         | 74/693 [00:33<04:23,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 11%|█         | 75/693 [00:33<04:21,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 11%|█         | 76/693 [00:33<04:20,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 11%|█         | 77/693 [00:34<04:19,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 11%|█▏        | 78/693 [00:34<04:18,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 11%|█▏        | 79/693 [00:35<04:20,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▏        | 80/693 [00:35<04:19,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▏        | 81/693 [00:35<04:16,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▏        | 82/693 [00:36<04:14,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▏        | 83/693 [00:36<04:17,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▏        | 84/693 [00:37<04:15,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▏        | 85/693 [00:37<04:15,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▏        | 86/693 [00:38<04:13,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 87/693 [00:38<04:12,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 88/693 [00:38<04:11,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 89/693 [00:39<04:14,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 90/693 [00:39<04:11,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 91/693 [00:40<04:13,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 92/693 [00:40<04:16,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 93/693 [00:41<04:17,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 14%|█▎        | 94/693 [00:41<04:17,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 14%|█▎        | 95/693 [00:41<04:18,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 14%|█▍        | 96/693 [00:42<04:17,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 14%|█▍        | 97/693 [00:42<04:16,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 14%|█▍        | 98/693 [00:43<04:16,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 14%|█▍        | 99/693 [00:43<04:16,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 14%|█▍        | 100/693 [00:44<04:15,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▍        | 101/693 [00:44<04:15,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▍        | 102/693 [00:44<04:15,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▍        | 103/693 [00:45<04:14,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▌        | 104/693 [00:45<04:15,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▌        | 105/693 [00:46<04:14,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▌        | 106/693 [00:46<04:14,  2.30it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▌        | 107/693 [00:47<04:14,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 16%|█▌        | 108/693 [00:47<04:13,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 16%|█▌        | 109/693 [00:47<04:11,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 16%|█▌        | 110/693 [00:48<04:18,  2.26it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 16%|█▌        | 111/693 [00:48<04:17,  2.26it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 16%|█▌        | 112/693 [00:49<04:15,  2.27it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 16%|█▋        | 113/693 [00:49<04:15,  2.27it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 16%|█▋        | 114/693 [00:50<04:16,  2.26it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 115/693 [00:50<04:13,  2.28it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 116/693 [00:51<04:13,  2.28it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 117/693 [00:51<04:10,  2.30it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 118/693 [00:51<04:08,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 119/693 [00:52<04:07,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 120/693 [00:52<04:07,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 121/693 [00:53<04:06,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 122/693 [00:53<04:05,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 123/693 [00:54<04:02,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 124/693 [00:54<03:59,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 125/693 [00:54<04:04,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 126/693 [00:55<04:01,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 127/693 [00:55<03:58,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 128/693 [00:56<03:56,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 19%|█▊        | 129/693 [00:56<03:58,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 19%|█▉        | 130/693 [00:56<03:56,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 19%|█▉        | 131/693 [00:57<03:57,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 19%|█▉        | 132/693 [00:57<03:55,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 19%|█▉        | 133/693 [00:58<03:53,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 19%|█▉        | 134/693 [00:58<03:52,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 19%|█▉        | 135/693 [00:59<03:51,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 20%|█▉        | 136/693 [00:59<03:50,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 20%|█▉        | 137/693 [00:59<03:51,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 20%|█▉        | 138/693 [01:00<03:51,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 20%|██        | 139/693 [01:00<03:56,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 20%|██        | 140/693 [01:01<03:53,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 20%|██        | 141/693 [01:01<03:51,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 20%|██        | 142/693 [01:02<03:59,  2.30it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 21%|██        | 143/693 [01:02<03:59,  2.29it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 21%|██        | 144/693 [01:02<03:59,  2.29it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 21%|██        | 145/693 [01:03<03:58,  2.30it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 21%|██        | 146/693 [01:03<03:57,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 21%|██        | 147/693 [01:04<03:56,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 21%|██▏       | 148/693 [01:04<03:55,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▏       | 149/693 [01:05<03:54,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▏       | 150/693 [01:05<03:54,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▏       | 151/693 [01:05<03:53,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▏       | 152/693 [01:06<03:52,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▏       | 153/693 [01:06<03:52,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▏       | 154/693 [01:07<03:48,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▏       | 155/693 [01:07<03:46,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 156/693 [01:08<03:44,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 157/693 [01:08<03:43,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 158/693 [01:08<03:42,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 159/693 [01:09<03:41,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 160/693 [01:09<03:45,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 161/693 [01:10<03:50,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 162/693 [01:10<03:48,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▎       | 163/693 [01:11<03:45,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▎       | 164/693 [01:11<03:43,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▍       | 165/693 [01:11<03:42,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▍       | 166/693 [01:12<03:40,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▍       | 167/693 [01:12<03:38,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▍       | 168/693 [01:13<03:37,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▍       | 169/693 [01:13<03:36,  2.42it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 25%|██▍       | 170/693 [01:13<03:36,  2.42it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 25%|██▍       | 171/693 [01:14<03:35,  2.42it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 25%|██▍       | 172/693 [01:14<03:36,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 25%|██▍       | 173/693 [01:15<03:35,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 25%|██▌       | 174/693 [01:15<03:37,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 25%|██▌       | 175/693 [01:15<03:36,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 25%|██▌       | 176/693 [01:16<03:36,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▌       | 177/693 [01:16<03:35,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▌       | 178/693 [01:17<03:36,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▌       | 179/693 [01:17<03:36,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▌       | 180/693 [01:18<03:34,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▌       | 181/693 [01:18<03:32,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▋       | 182/693 [01:18<03:32,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▋       | 183/693 [01:19<03:32,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 184/693 [01:19<03:31,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 185/693 [01:20<03:32,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 186/693 [01:20<03:32,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 187/693 [01:21<03:31,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 188/693 [01:21<03:30,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 189/693 [01:21<03:31,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 190/693 [01:22<03:30,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 191/693 [01:22<03:34,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 192/693 [01:23<03:35,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 193/693 [01:23<03:35,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 194/693 [01:23<03:33,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 195/693 [01:24<03:30,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 196/693 [01:24<03:30,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 197/693 [01:25<03:30,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 29%|██▊       | 198/693 [01:25<03:27,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 29%|██▊       | 199/693 [01:26<03:28,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 29%|██▉       | 200/693 [01:26<03:29,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 29%|██▉       | 201/693 [01:26<03:30,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 29%|██▉       | 202/693 [01:27<03:31,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 29%|██▉       | 203/693 [01:27<03:31,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 29%|██▉       | 204/693 [01:28<03:31,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 30%|██▉       | 205/693 [01:28<03:30,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 30%|██▉       | 206/693 [01:29<03:29,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 30%|██▉       | 207/693 [01:29<03:31,  2.30it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 30%|███       | 208/693 [01:29<03:28,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 30%|███       | 209/693 [01:30<03:26,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 30%|███       | 210/693 [01:30<03:24,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 30%|███       | 211/693 [01:31<03:28,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 31%|███       | 212/693 [01:31<03:26,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 31%|███       | 213/693 [01:32<03:23,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 31%|███       | 214/693 [01:32<03:21,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 31%|███       | 215/693 [01:32<03:21,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 31%|███       | 216/693 [01:33<03:18,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 31%|███▏      | 217/693 [01:33<03:18,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 31%|███▏      | 218/693 [01:34<03:20,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 32%|███▏      | 219/693 [01:34<03:18,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 32%|███▏      | 220/693 [01:35<03:19,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 32%|███▏      | 221/693 [01:35<03:20,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 32%|███▏      | 222/693 [01:35<03:18,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 32%|███▏      | 223/693 [01:36<03:17,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 32%|███▏      | 224/693 [01:36<03:18,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 32%|███▏      | 225/693 [01:37<03:16,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 226/693 [01:37<03:14,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 227/693 [01:37<03:14,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 228/693 [01:38<03:14,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 229/693 [01:38<03:12,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 230/693 [01:39<03:11,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 231/693 [01:39<03:09,  2.43it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 232/693 [01:40<03:10,  2.42it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▎      | 233/693 [01:40<03:09,  2.42it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▍      | 234/693 [01:40<03:09,  2.42it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▍      | 235/693 [01:41<03:09,  2.42it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▍      | 236/693 [01:41<03:09,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▍      | 237/693 [01:42<03:08,  2.42it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▍      | 238/693 [01:42<03:08,  2.42it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▍      | 239/693 [01:42<03:08,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 35%|███▍      | 240/693 [01:43<03:07,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 35%|███▍      | 241/693 [01:43<03:10,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 35%|███▍      | 242/693 [01:44<03:09,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 35%|███▌      | 243/693 [01:44<03:08,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 35%|███▌      | 244/693 [01:45<03:07,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 35%|███▌      | 245/693 [01:45<03:07,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 35%|███▌      | 246/693 [01:45<03:07,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▌      | 247/693 [01:46<03:10,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▌      | 248/693 [01:46<03:10,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▌      | 249/693 [01:47<03:08,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▌      | 250/693 [01:47<03:11,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▌      | 251/693 [01:48<03:08,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▋      | 252/693 [01:48<03:07,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 37%|███▋      | 253/693 [01:48<03:05,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 37%|███▋      | 254/693 [01:49<03:04,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 37%|███▋      | 255/693 [01:49<03:06,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 37%|███▋      | 256/693 [01:50<03:06,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 37%|███▋      | 257/693 [01:50<03:07,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 37%|███▋      | 258/693 [01:51<03:05,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 37%|███▋      | 259/693 [01:51<03:05,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 260/693 [01:51<03:06,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 261/693 [01:52<03:07,  2.30it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 262/693 [01:52<03:06,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 263/693 [01:53<03:05,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 264/693 [01:53<03:04,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 265/693 [01:54<03:09,  2.26it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 266/693 [01:54<03:07,  2.28it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 39%|███▊      | 267/693 [01:54<03:05,  2.29it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 39%|███▊      | 268/693 [01:55<03:04,  2.30it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 39%|███▉      | 269/693 [01:55<03:04,  2.29it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 39%|███▉      | 270/693 [01:56<03:03,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 39%|███▉      | 271/693 [01:56<03:03,  2.30it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 39%|███▉      | 272/693 [01:57<03:01,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 39%|███▉      | 273/693 [01:57<03:01,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 40%|███▉      | 274/693 [01:57<03:00,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 40%|███▉      | 275/693 [01:58<02:59,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 40%|███▉      | 276/693 [01:58<03:00,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 40%|███▉      | 277/693 [01:59<02:57,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 40%|████      | 278/693 [01:59<02:55,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 40%|████      | 279/693 [02:00<02:55,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 40%|████      | 280/693 [02:00<02:53,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 41%|████      | 281/693 [02:00<02:53,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 41%|████      | 282/693 [02:01<02:52,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 41%|████      | 283/693 [02:01<02:51,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 41%|████      | 284/693 [02:02<02:50,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 41%|████      | 285/693 [02:02<02:49,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 41%|████▏     | 286/693 [02:03<02:49,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 41%|████▏     | 287/693 [02:03<02:48,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 42%|████▏     | 288/693 [02:03<02:47,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 42%|████▏     | 289/693 [02:04<02:47,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 42%|████▏     | 290/693 [02:04<02:47,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 42%|████▏     | 291/693 [02:05<02:48,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 42%|████▏     | 292/693 [02:05<02:48,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 42%|████▏     | 293/693 [02:05<02:49,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 42%|████▏     | 294/693 [02:06<02:50,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 295/693 [02:06<02:51,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 296/693 [02:07<02:50,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 297/693 [02:07<02:50,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 298/693 [02:08<02:49,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 299/693 [02:08<02:49,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 300/693 [02:08<02:49,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 301/693 [02:09<02:48,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▎     | 302/693 [02:09<02:48,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▎     | 303/693 [02:10<02:49,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▍     | 304/693 [02:10<02:48,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▍     | 305/693 [02:11<02:48,  2.30it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▍     | 306/693 [02:11<02:46,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▍     | 307/693 [02:11<02:43,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▍     | 308/693 [02:12<02:41,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 45%|████▍     | 309/693 [02:12<02:42,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 45%|████▍     | 310/693 [02:13<02:45,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 45%|████▍     | 311/693 [02:13<02:44,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 45%|████▌     | 312/693 [02:14<02:42,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 45%|████▌     | 313/693 [02:14<02:41,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 45%|████▌     | 314/693 [02:14<02:41,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 45%|████▌     | 315/693 [02:15<02:40,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▌     | 316/693 [02:15<02:39,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▌     | 317/693 [02:16<02:37,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▌     | 318/693 [02:16<02:38,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▌     | 319/693 [02:17<02:36,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▌     | 320/693 [02:17<02:37,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▋     | 321/693 [02:17<02:39,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▋     | 322/693 [02:18<02:37,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 323/693 [02:18<02:37,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 324/693 [02:19<02:36,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 325/693 [02:19<02:36,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 326/693 [02:20<02:34,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 327/693 [02:20<02:34,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 328/693 [02:20<02:35,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 329/693 [02:21<02:33,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 330/693 [02:21<02:32,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 331/693 [02:22<02:32,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 332/693 [02:22<02:31,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 333/693 [02:22<02:32,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 334/693 [02:23<02:30,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 335/693 [02:23<02:29,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 336/693 [02:24<02:29,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 49%|████▊     | 337/693 [02:24<02:28,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 49%|████▉     | 338/693 [02:25<02:27,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 49%|████▉     | 339/693 [02:25<02:28,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 49%|████▉     | 340/693 [02:25<02:28,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 49%|████▉     | 341/693 [02:26<02:27,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 49%|████▉     | 342/693 [02:26<02:27,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 49%|████▉     | 343/693 [02:27<02:25,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 50%|████▉     | 344/693 [02:27<02:25,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 50%|████▉     | 345/693 [02:27<02:24,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 50%|████▉     | 346/693 [02:28<02:23,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 50%|█████     | 347/693 [02:28<02:23,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 50%|█████     | 348/693 [02:29<02:24,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 50%|█████     | 349/693 [02:29<02:23,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████     | 350/693 [02:30<02:22,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████     | 351/693 [02:30<02:21,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████     | 352/693 [02:30<02:22,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████     | 353/693 [02:31<02:22,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████     | 354/693 [02:31<02:24,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████     | 355/693 [02:32<02:24,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████▏    | 356/693 [02:32<02:25,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▏    | 357/693 [02:33<02:23,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▏    | 358/693 [02:33<02:23,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▏    | 359/693 [02:33<02:22,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▏    | 360/693 [02:34<02:20,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▏    | 361/693 [02:34<02:19,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▏    | 362/693 [02:35<02:19,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▏    | 363/693 [02:35<02:18,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 364/693 [02:35<02:17,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 365/693 [02:36<02:16,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 366/693 [02:36<02:16,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 367/693 [02:37<02:15,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 368/693 [02:37<02:14,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 369/693 [02:38<02:14,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 370/693 [02:38<02:14,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▎    | 371/693 [02:38<02:13,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▎    | 372/693 [02:39<02:12,  2.42it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▍    | 373/693 [02:39<02:12,  2.42it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▍    | 374/693 [02:40<02:12,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▍    | 375/693 [02:40<02:12,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▍    | 376/693 [02:40<02:11,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▍    | 377/693 [02:41<02:10,  2.42it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▍    | 378/693 [02:41<02:10,  2.42it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▍    | 379/693 [02:42<02:09,  2.42it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▍    | 380/693 [02:42<02:10,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▍    | 381/693 [02:43<02:10,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▌    | 382/693 [02:43<02:09,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▌    | 383/693 [02:43<02:08,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▌    | 384/693 [02:44<02:07,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▌    | 385/693 [02:44<02:07,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▌    | 386/693 [02:45<02:07,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▌    | 387/693 [02:45<02:07,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▌    | 388/693 [02:45<02:06,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▌    | 389/693 [02:46<02:06,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▋    | 390/693 [02:46<02:05,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▋    | 391/693 [02:47<02:05,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▋    | 392/693 [02:47<02:04,  2.42it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▋    | 393/693 [02:48<02:05,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▋    | 394/693 [02:48<02:04,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▋    | 395/693 [02:48<02:04,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▋    | 396/693 [02:49<02:05,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▋    | 397/693 [02:49<02:03,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▋    | 398/693 [02:50<02:04,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 58%|█████▊    | 399/693 [02:50<02:04,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 58%|█████▊    | 400/693 [02:50<02:03,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 58%|█████▊    | 401/693 [02:51<02:03,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 58%|█████▊    | 402/693 [02:51<02:02,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 58%|█████▊    | 403/693 [02:52<02:01,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 58%|█████▊    | 404/693 [02:52<02:00,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 58%|█████▊    | 405/693 [02:53<02:04,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▊    | 406/693 [02:53<02:05,  2.29it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▊    | 407/693 [02:53<02:03,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▉    | 408/693 [02:54<02:02,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▉    | 409/693 [02:54<02:00,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▉    | 410/693 [02:55<01:58,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▉    | 411/693 [02:55<01:58,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▉    | 412/693 [02:56<01:57,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 60%|█████▉    | 413/693 [02:56<01:56,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 60%|█████▉    | 414/693 [02:56<01:55,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 60%|█████▉    | 415/693 [02:57<01:55,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 60%|██████    | 416/693 [02:57<01:56,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 60%|██████    | 417/693 [02:58<01:54,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 60%|██████    | 418/693 [02:58<01:55,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 60%|██████    | 419/693 [02:58<01:55,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████    | 420/693 [02:59<01:54,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████    | 421/693 [02:59<01:53,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████    | 422/693 [03:00<01:54,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████    | 423/693 [03:00<01:53,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████    | 424/693 [03:01<01:53,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████▏   | 425/693 [03:01<01:52,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████▏   | 426/693 [03:01<01:52,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 427/693 [03:02<01:51,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 428/693 [03:02<01:50,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 429/693 [03:03<01:50,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 430/693 [03:03<01:50,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 431/693 [03:04<01:52,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 432/693 [03:04<01:51,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 433/693 [03:04<01:51,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 63%|██████▎   | 434/693 [03:05<01:51,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 63%|██████▎   | 435/693 [03:05<01:50,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 63%|██████▎   | 436/693 [03:06<01:50,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 63%|██████▎   | 437/693 [03:06<01:50,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 63%|██████▎   | 438/693 [03:07<01:50,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 63%|██████▎   | 439/693 [03:07<01:49,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 63%|██████▎   | 440/693 [03:07<01:48,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 64%|██████▎   | 441/693 [03:08<01:48,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 64%|██████▍   | 442/693 [03:08<01:48,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 64%|██████▍   | 443/693 [03:09<01:47,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 64%|██████▍   | 444/693 [03:09<01:47,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 64%|██████▍   | 445/693 [03:10<01:47,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 64%|██████▍   | 446/693 [03:10<01:45,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 65%|██████▍   | 447/693 [03:10<01:43,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 65%|██████▍   | 448/693 [03:11<01:43,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 65%|██████▍   | 449/693 [03:11<01:42,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 65%|██████▍   | 450/693 [03:12<01:41,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 65%|██████▌   | 451/693 [03:12<01:41,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 65%|██████▌   | 452/693 [03:12<01:41,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 65%|██████▌   | 453/693 [03:13<01:40,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▌   | 454/693 [03:13<01:39,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▌   | 455/693 [03:14<01:39,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▌   | 456/693 [03:14<01:38,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▌   | 457/693 [03:15<01:37,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▌   | 458/693 [03:15<01:37,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▌   | 459/693 [03:15<01:37,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▋   | 460/693 [03:16<01:36,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 461/693 [03:16<01:36,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 462/693 [03:17<01:35,  2.43it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 463/693 [03:17<01:35,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 464/693 [03:17<01:35,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 465/693 [03:18<01:34,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 466/693 [03:18<01:35,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 467/693 [03:19<01:35,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 68%|██████▊   | 468/693 [03:19<01:35,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 68%|██████▊   | 469/693 [03:20<01:34,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 68%|██████▊   | 470/693 [03:20<01:33,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 68%|██████▊   | 471/693 [03:20<01:33,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 68%|██████▊   | 472/693 [03:21<01:32,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 68%|██████▊   | 473/693 [03:21<01:31,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 68%|██████▊   | 474/693 [03:22<01:30,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▊   | 475/693 [03:22<01:30,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▊   | 476/693 [03:23<01:30,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▉   | 477/693 [03:23<01:30,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▉   | 478/693 [03:23<01:31,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▉   | 479/693 [03:24<01:30,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▉   | 480/693 [03:24<01:29,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▉   | 481/693 [03:25<01:28,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 70%|██████▉   | 482/693 [03:25<01:28,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 70%|██████▉   | 483/693 [03:25<01:27,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 70%|██████▉   | 484/693 [03:26<01:26,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 70%|██████▉   | 485/693 [03:26<01:26,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 70%|███████   | 486/693 [03:27<01:29,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 70%|███████   | 487/693 [03:27<01:27,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 70%|███████   | 488/693 [03:28<01:28,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 71%|███████   | 489/693 [03:28<01:28,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 71%|███████   | 490/693 [03:28<01:28,  2.29it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 71%|███████   | 491/693 [03:29<01:27,  2.30it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 71%|███████   | 492/693 [03:29<01:26,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 71%|███████   | 493/693 [03:30<01:24,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 71%|███████▏  | 494/693 [03:30<01:23,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 71%|███████▏  | 495/693 [03:31<01:23,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 496/693 [03:31<01:22,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 497/693 [03:31<01:21,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 498/693 [03:32<01:21,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 499/693 [03:32<01:20,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 500/693 [03:33<01:20,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:{'loss': 1.2026, 'learning_rate': 1.4285714285714285e-05, 'epoch': 2.16}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 500/693 [03:33<01:20,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:2709] 2024-01-24 01:14:48,607 >> Saving model checkpoint to /opt/ml/model/checkpoint-500\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:2709] 2024-01-24 01:14:48,607 >> Saving model checkpoint to /opt/ml/model/checkpoint-500\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:453] 2024-01-24 01:14:48,608 >> Configuration saved in /opt/ml/model/checkpoint-500/config.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:453] 2024-01-24 01:14:48,608 >> Configuration saved in /opt/ml/model/checkpoint-500/config.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:336] 2024-01-24 01:14:48,610 >> Configuration saved in /opt/ml/model/checkpoint-500/generation_config.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:336] 2024-01-24 01:14:48,610 >> Configuration saved in /opt/ml/model/checkpoint-500/generation_config.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|modeling_utils.py:1704] 2024-01-24 01:14:50,293 >> Model weights saved in /opt/ml/model/checkpoint-500/pytorch_model.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|modeling_utils.py:1704] 2024-01-24 01:14:50,293 >> Model weights saved in /opt/ml/model/checkpoint-500/pytorch_model.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|tokenization_utils_base.py:2160] 2024-01-24 01:14:50,293 >> tokenizer config file saved in /opt/ml/model/checkpoint-500/tokenizer_config.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|tokenization_utils_base.py:2160] 2024-01-24 01:14:50,293 >> tokenizer config file saved in /opt/ml/model/checkpoint-500/tokenizer_config.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|tokenization_utils_base.py:2167] 2024-01-24 01:14:50,294 >> Special tokens file saved in /opt/ml/model/checkpoint-500/special_tokens_map.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|tokenization_utils_base.py:2167] 2024-01-24 01:14:50,294 >> Special tokens file saved in /opt/ml/model/checkpoint-500/special_tokens_map.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 501/693 [03:38<06:22,  1.99s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 502/693 [03:39<04:50,  1.52s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 503/693 [03:39<03:45,  1.19s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 504/693 [03:40<03:00,  1.05it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 505/693 [03:40<02:29,  1.26it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 506/693 [03:40<02:08,  1.46it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 507/693 [03:41<01:52,  1.66it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 508/693 [03:41<01:40,  1.83it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 509/693 [03:42<01:33,  1.98it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▎  | 510/693 [03:42<01:27,  2.09it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▎  | 511/693 [03:42<01:24,  2.15it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▍  | 512/693 [03:43<01:22,  2.20it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▍  | 513/693 [03:43<01:20,  2.23it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▍  | 514/693 [03:44<01:19,  2.25it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▍  | 515/693 [03:44<01:18,  2.26it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▍  | 516/693 [03:45<01:17,  2.28it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 75%|███████▍  | 517/693 [03:45<01:16,  2.29it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 75%|███████▍  | 518/693 [03:46<01:16,  2.29it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 75%|███████▍  | 519/693 [03:46<01:15,  2.29it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 75%|███████▌  | 520/693 [03:46<01:15,  2.30it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 75%|███████▌  | 521/693 [03:47<01:14,  2.30it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 75%|███████▌  | 522/693 [03:47<01:14,  2.30it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 75%|███████▌  | 523/693 [03:48<01:15,  2.25it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▌  | 524/693 [03:48<01:14,  2.27it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▌  | 525/693 [03:49<01:14,  2.26it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▌  | 526/693 [03:49<01:13,  2.28it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▌  | 527/693 [03:49<01:12,  2.29it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▌  | 528/693 [03:50<01:12,  2.29it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▋  | 529/693 [03:50<01:11,  2.29it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▋  | 530/693 [03:51<01:10,  2.30it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 531/693 [03:51<01:10,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 532/693 [03:52<01:09,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 533/693 [03:52<01:09,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 534/693 [03:52<01:07,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 535/693 [03:53<01:08,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 536/693 [03:53<01:06,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 537/693 [03:54<01:05,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 538/693 [03:54<01:05,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 539/693 [03:55<01:04,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 540/693 [03:55<01:03,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 541/693 [03:55<01:03,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 542/693 [03:56<01:04,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 543/693 [03:56<01:04,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 544/693 [03:57<01:03,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 79%|███████▊  | 545/693 [03:57<01:03,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 79%|███████▉  | 546/693 [03:58<01:03,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 79%|███████▉  | 547/693 [03:58<01:02,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 79%|███████▉  | 548/693 [03:58<01:02,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 79%|███████▉  | 549/693 [03:59<01:01,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 79%|███████▉  | 550/693 [03:59<01:00,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 80%|███████▉  | 551/693 [04:00<01:00,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 80%|███████▉  | 552/693 [04:00<01:00,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 80%|███████▉  | 553/693 [04:01<00:59,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 80%|███████▉  | 554/693 [04:01<00:59,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 80%|████████  | 555/693 [04:01<00:59,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 80%|████████  | 556/693 [04:02<00:59,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 80%|████████  | 557/693 [04:02<00:58,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████  | 558/693 [04:03<00:58,  2.30it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████  | 559/693 [04:03<00:58,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████  | 560/693 [04:04<00:56,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████  | 561/693 [04:04<00:56,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████  | 562/693 [04:04<00:55,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████  | 563/693 [04:05<00:55,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████▏ | 564/693 [04:05<00:54,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▏ | 565/693 [04:06<00:54,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▏ | 566/693 [04:06<00:53,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▏ | 567/693 [04:07<00:53,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▏ | 568/693 [04:07<00:52,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▏ | 569/693 [04:07<00:51,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▏ | 570/693 [04:08<00:51,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▏ | 571/693 [04:08<00:51,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 572/693 [04:09<00:50,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 573/693 [04:09<00:49,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 574/693 [04:09<00:49,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 575/693 [04:10<00:49,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 576/693 [04:10<00:48,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 577/693 [04:11<00:48,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 578/693 [04:11<00:48,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 84%|████████▎ | 579/693 [04:12<00:47,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 84%|████████▎ | 580/693 [04:12<00:47,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 84%|████████▍ | 581/693 [04:12<00:46,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 84%|████████▍ | 582/693 [04:13<00:45,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 84%|████████▍ | 583/693 [04:13<00:45,  2.42it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 84%|████████▍ | 584/693 [04:14<00:45,  2.42it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 84%|████████▍ | 585/693 [04:14<00:44,  2.42it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▍ | 586/693 [04:14<00:44,  2.42it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▍ | 587/693 [04:15<00:43,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▍ | 588/693 [04:15<00:43,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▍ | 589/693 [04:16<00:43,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▌ | 590/693 [04:16<00:43,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▌ | 591/693 [04:17<00:42,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▌ | 592/693 [04:17<00:42,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▌ | 593/693 [04:17<00:42,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▌ | 594/693 [04:18<00:41,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▌ | 595/693 [04:18<00:40,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▌ | 596/693 [04:19<00:40,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▌ | 597/693 [04:19<00:39,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▋ | 598/693 [04:19<00:39,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▋ | 599/693 [04:20<00:39,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 600/693 [04:20<00:38,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 601/693 [04:21<00:38,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 602/693 [04:21<00:37,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 603/693 [04:22<00:37,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 604/693 [04:22<00:37,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 605/693 [04:22<00:37,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 606/693 [04:23<00:37,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 607/693 [04:23<00:36,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 608/693 [04:24<00:36,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 609/693 [04:24<00:35,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 610/693 [04:24<00:34,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 611/693 [04:25<00:34,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 612/693 [04:25<00:33,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 613/693 [04:26<00:33,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 89%|████████▊ | 614/693 [04:26<00:33,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 89%|████████▊ | 615/693 [04:27<00:32,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 89%|████████▉ | 616/693 [04:27<00:31,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 89%|████████▉ | 617/693 [04:27<00:31,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 89%|████████▉ | 618/693 [04:28<00:31,  2.42it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 89%|████████▉ | 619/693 [04:28<00:30,  2.42it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 89%|████████▉ | 620/693 [04:29<00:30,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 90%|████████▉ | 621/693 [04:29<00:29,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 90%|████████▉ | 622/693 [04:29<00:29,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 90%|████████▉ | 623/693 [04:30<00:29,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 90%|█████████ | 624/693 [04:30<00:28,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 90%|█████████ | 625/693 [04:31<00:28,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 90%|█████████ | 626/693 [04:31<00:27,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 90%|█████████ | 627/693 [04:32<00:27,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 91%|█████████ | 628/693 [04:32<00:27,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 91%|█████████ | 629/693 [04:32<00:26,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 91%|█████████ | 630/693 [04:33<00:27,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 91%|█████████ | 631/693 [04:33<00:26,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 91%|█████████ | 632/693 [04:34<00:26,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 91%|█████████▏| 633/693 [04:34<00:25,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 91%|█████████▏| 634/693 [04:35<00:25,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 635/693 [04:35<00:24,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 636/693 [04:35<00:24,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 637/693 [04:36<00:24,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 638/693 [04:36<00:23,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 639/693 [04:37<00:23,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 640/693 [04:37<00:22,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 641/693 [04:38<00:22,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 642/693 [04:38<00:21,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 643/693 [04:38<00:21,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 644/693 [04:39<00:21,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 645/693 [04:39<00:20,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 646/693 [04:40<00:20,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 647/693 [04:40<00:19,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▎| 648/693 [04:41<00:19,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▎| 649/693 [04:41<00:18,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▍| 650/693 [04:41<00:18,  2.30it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▍| 651/693 [04:42<00:18,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▍| 652/693 [04:42<00:17,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▍| 653/693 [04:43<00:16,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▍| 654/693 [04:43<00:16,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 95%|█████████▍| 655/693 [04:44<00:15,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 95%|█████████▍| 656/693 [04:44<00:15,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 95%|█████████▍| 657/693 [04:44<00:15,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 95%|█████████▍| 658/693 [04:45<00:14,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 95%|█████████▌| 659/693 [04:45<00:14,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 95%|█████████▌| 660/693 [04:46<00:13,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 95%|█████████▌| 661/693 [04:46<00:13,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▌| 662/693 [04:47<00:13,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▌| 663/693 [04:47<00:12,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▌| 664/693 [04:47<00:12,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▌| 665/693 [04:48<00:11,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▌| 666/693 [04:48<00:11,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▌| 667/693 [04:49<00:11,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▋| 668/693 [04:49<00:10,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 669/693 [04:50<00:10,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 670/693 [04:50<00:09,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 671/693 [04:50<00:09,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 672/693 [04:51<00:08,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 673/693 [04:51<00:08,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 674/693 [04:52<00:07,  2.41it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 675/693 [04:52<00:07,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 676/693 [04:52<00:07,  2.39it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 677/693 [04:53<00:06,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 678/693 [04:53<00:06,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 679/693 [04:54<00:05,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 680/693 [04:54<00:05,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 681/693 [04:55<00:05,  2.40it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 682/693 [04:55<00:04,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▊| 683/693 [04:55<00:04,  2.36it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▊| 684/693 [04:56<00:03,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▉| 685/693 [04:56<00:03,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▉| 686/693 [04:57<00:02,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▉| 687/693 [04:57<00:02,  2.33it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▉| 688/693 [04:58<00:02,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▉| 689/693 [04:58<00:01,  2.38it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015100%|█████████▉| 690/693 [04:58<00:01,  2.37it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015100%|█████████▉| 691/693 [04:59<00:00,  2.35it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015100%|█████████▉| 692/693 [04:59<00:00,  2.34it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 693/693 [05:00<00:00,  2.32it/s][1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:1901] 2024-01-24 01:16:15,608 >> \u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:1901] 2024-01-24 01:16:15,608 >> \u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:{'train_runtime': 300.1467, 'train_samples_per_second': 147.238, 'train_steps_per_second': 2.309, 'train_loss': 1.1094466260329297, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 693/693 [05:00<00:00,  2.32it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 693/693 [05:00<00:00,  2.31it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:2709] 2024-01-24 01:16:15,622 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:2709] 2024-01-24 01:16:15,622 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:453] 2024-01-24 01:16:15,623 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:453] 2024-01-24 01:16:15,623 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:336] 2024-01-24 01:16:15,625 >> Configuration saved in /opt/ml/model/generation_config.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:336] 2024-01-24 01:16:15,625 >> Configuration saved in /opt/ml/model/generation_config.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|modeling_utils.py:1704] 2024-01-24 01:16:17,338 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|modeling_utils.py:1704] 2024-01-24 01:16:17,338 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|tokenization_utils_base.py:2160] 2024-01-24 01:16:17,339 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|tokenization_utils_base.py:2160] 2024-01-24 01:16:17,339 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|tokenization_utils_base.py:2167] 2024-01-24 01:16:17,339 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|tokenization_utils_base.py:2167] 2024-01-24 01:16:17,339 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:***** train metrics *****\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  epoch                    =        3.0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  train_loss               =     1.1094\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  train_runtime            = 0:05:00.14\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  train_samples            =      14731\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  train_samples_per_second =    147.238\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  train_steps_per_second   =      2.309\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:01/24/2024 01:16:17 - INFO - __main__ - *** Evaluate ***\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:2964] 2024-01-24 01:16:17,442 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:2964] 2024-01-24 01:16:17,442 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:2966] 2024-01-24 01:16:17,442 >>   Num examples = 818\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:2966] 2024-01-24 01:16:17,442 >>   Num examples = 818\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:2969] 2024-01-24 01:16:17,442 >>   Batch size = 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:2969] 2024-01-24 01:16:17,442 >>   Batch size = 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:17,448 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:17,448 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/13 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:18,814 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:18,814 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▌        | 2/13 [00:01<00:07,  1.51it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:20,136 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:20,136 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 3/13 [00:02<00:10,  1.00s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:21,613 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:21,613 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 31%|███       | 4/13 [00:04<00:10,  1.15s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:23,020 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:23,020 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 5/13 [00:05<00:09,  1.20s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:24,324 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:24,324 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▌     | 6/13 [00:06<00:08,  1.27s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:25,723 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:25,723 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▍    | 7/13 [00:08<00:07,  1.30s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:27,077 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:27,077 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 8/13 [00:09<00:06,  1.39s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:28,678 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:28,678 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▉   | 9/13 [00:11<00:05,  1.34s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:29,895 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:29,895 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 10/13 [00:12<00:03,  1.30s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:31,115 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:31,115 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▍ | 11/13 [00:13<00:02,  1.34s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:32,532 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:32,532 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 12/13 [00:15<00:01,  1.33s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:33,858 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:33,858 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 13/13 [00:16<00:00,  1.33s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 13/13 [00:19<00:00,  1.51s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:***** eval metrics *****\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  epoch                   =        3.0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  eval_gen_len            =    59.7421\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  eval_loss               =     1.4201\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  eval_rouge1             =    43.1754\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  eval_rouge2             =    22.2026\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  eval_rougeL             =    33.6383\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  eval_rougeLsum          =    40.1594\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  eval_runtime            = 0:00:21.04\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  eval_samples            =        818\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  eval_samples_per_second =     38.866\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  eval_steps_per_second   =      0.618\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:01/24/2024 01:16:38 - INFO - __main__ - *** Predict ***\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:2964] 2024-01-24 01:16:38,491 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:2964] 2024-01-24 01:16:38,491 >> ***** Running Prediction *****\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:2966] 2024-01-24 01:16:38,491 >>   Num examples = 819\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:2966] 2024-01-24 01:16:38,491 >>   Num examples = 819\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:2969] 2024-01-24 01:16:38,491 >>   Batch size = 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:2969] 2024-01-24 01:16:38,491 >>   Batch size = 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:38,496 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:38,496 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/13 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:40,048 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:40,048 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▌        | 2/13 [00:01<00:08,  1.29it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:41,601 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:41,601 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 3/13 [00:03<00:10,  1.07s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:43,082 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:43,082 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 31%|███       | 4/13 [00:04<00:10,  1.12s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:44,294 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:44,294 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 5/13 [00:05<00:09,  1.21s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:45,680 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:45,680 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▌     | 6/13 [00:06<00:08,  1.26s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:47,046 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:47,046 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▍    | 7/13 [00:08<00:07,  1.26s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:48,296 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:48,296 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 8/13 [00:09<00:06,  1.24s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:49,496 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:49,496 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▉   | 9/13 [00:10<00:05,  1.31s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:50,953 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:50,953 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 10/13 [00:12<00:03,  1.29s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:52,211 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:52,211 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▍ | 11/13 [00:13<00:02,  1.38s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:53,775 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:53,775 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 12/13 [00:15<00:01,  1.36s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:55,093 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:543] 2024-01-24 01:16:55,093 >> Generate config GenerationConfig {\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"decoder_start_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"early_stopping\": true,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_bos_token_id\": 0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"forced_eos_token_id\": 2,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"length_penalty\": 2.0,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"max_length\": 142,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"min_length\": 56,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"no_repeat_ngram_size\": 3,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"num_beams\": 4,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 1,\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.26.0\"\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 13/13 [00:16<00:00,  1.36s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 13/13 [00:19<00:00,  1.51s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:***** predict metrics *****\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  predict_gen_len            =    59.7509\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  predict_loss               =     1.4537\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  predict_rouge1             =    42.3058\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  predict_rouge2             =    21.2062\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  predict_rougeL             =    32.6251\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  predict_rougeLsum          =    39.0093\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  predict_runtime            = 0:00:21.23\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  predict_samples            =        819\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  predict_samples_per_second =     38.573\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  predict_steps_per_second   =      0.612\u001b[0m\n",
      "\u001b[34m2024-01-24 01:17:09,515 sagemaker-training-toolkit INFO     Invoked on_terminate from psutil.wait_for_procs\u001b[0m\n",
      "\u001b[34m2024-01-24 01:17:09,515 sagemaker-training-toolkit INFO     process psutil.Process(pid=75, name='orted', status='terminated', started='01:10:42') terminated with exit code None\u001b[0m\n",
      "\u001b[34m2024-01-24 01:17:09,515 sagemaker-training-toolkit INFO     Reporting status for ORTEd process. gone: [psutil.Process(pid=75, name='orted', status='terminated', started='01:10:42')] alive: []\u001b[0m\n",
      "\u001b[34m2024-01-24 01:17:09,515 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\u001b[35m2024-01-24 01:17:09,510 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2024-01-24 01:17:09,510 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2024-01-24 01:17:09,511 sagemaker-training-toolkit INFO     Begin writing status file from leader node to worker nodes\u001b[0m\n",
      "\u001b[35m2024-01-24 01:17:09,511 sagemaker-training-toolkit INFO     Start writing mpirun finished status to algo-1\u001b[0m\n",
      "\u001b[35m2024-01-24 01:17:09,662 sagemaker-training-toolkit INFO     output from subprocess run CompletedProcess(args=['ssh', 'algo-1', 'touch', '/tmp/done.algo-2'], returncode=0, stdout='', stderr='')\u001b[0m\n",
      "\u001b[35m2024-01-24 01:17:09,662 sagemaker-training-toolkit INFO     Finished writing status file\u001b[0m\n",
      "\n",
      "2024-01-24 01:17:46 Uploading - Uploading generated training model\u001b[34m2024-01-24 01:17:39,545 sagemaker-training-toolkit INFO     Begin looking for status file on algo-1\u001b[0m\n",
      "\u001b[34m2024-01-24 01:17:39,546 sagemaker-training-toolkit INFO     MPI training job status file found. Exit gracefully\u001b[0m\n",
      "\u001b[34m2024-01-24 01:17:39,546 sagemaker-training-toolkit INFO     End looking for status file\u001b[0m\n",
      "\u001b[34m2024-01-24 01:17:39,546 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[34m2024-01-24 01:17:39,546 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[35m2024-01-24 01:17:39,692 sagemaker-training-toolkit INFO     Finished writing status file from leader node to worker nodes\u001b[0m\n",
      "\u001b[35m2024-01-24 01:17:39,693 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-01-24 01:19:15 Completed - Training job completed\n",
      "Training seconds: 1598\n",
      "Billable seconds: 1598\n"
     ]
    }
   ],
   "source": [
    "# starting the train job\n",
    "huggingface_estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bdde1b-e67d-4522-8e9d-8fafc552f693",
   "metadata": {},
   "source": [
    "## Deploying the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09f1209-4635-472b-9526-a031dbdbee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = huggingface_estimator.deploy(1, \"ml.g4dn.xlarge\")\n",
    "\n",
    "conversation = '''Jeff: Can I train a 🤗 Transformers model on Amazon SageMaker? \n",
    "    Philipp: Sure you can use the new Hugging Face Deep Learning Container. \n",
    "    Jeff: ok.\n",
    "    Jeff: and how can I get started? \n",
    "    Jeff: where can I find documentation? \n",
    "    Philipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face                                           \n",
    "    '''\n",
    "\n",
    "data= {\"inputs\":conversation}\n",
    "\n",
    "predictor.predict(data)\n",
    "\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843839e1-b658-409b-8c6d-4cd5cd44c9e2",
   "metadata": {},
   "source": [
    "## Download the model from S3 and unzip it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b87fdb6d-f9e1-4cac-9b3c-f4f41a191190",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "local_path = 'my_bart_model'\n",
    "\n",
    "os.makedirs(local_path, exist_ok = True)\n",
    "\n",
    "# download model from S3\n",
    "S3Downloader.download(\n",
    "    s3_uri=huggingface_estimator.model_data, # s3 uri where the trained model is located\n",
    "    local_path=local_path, # local path where *.targ.gz is saved\n",
    "    sagemaker_session=sess # sagemaker session used for training the model\n",
    ")\n",
    "\n",
    "# unzip model\n",
    "tar = tarfile.open(f\"{local_path}/model.tar.gz\", \"r:gz\")\n",
    "tar.extractall(path=local_path)\n",
    "tar.close()\n",
    "os.remove(f\"{local_path}/model.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cee47a1-3471-4722-9794-67927c4149e2",
   "metadata": {},
   "source": [
    "## Create a model card\n",
    "\n",
    "The model_card describes the model includes hyperparameters, results and which dataset was used for training. To create a model_card we create a README.md in our local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d783a920-d2cc-4d7a-9757-05d17261c73b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_rouge1': 43.1754, 'eval_rouge2': 22.2026, 'eval_rougeL': 33.6383, 'eval_rougeLsum': 40.1594}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# read eval and test results \n",
    "with open(f\"{local_path}/eval_results.json\") as f:\n",
    "    eval_results_raw = json.load(f)\n",
    "    eval_results={}\n",
    "    eval_results[\"eval_rouge1\"] = eval_results_raw[\"eval_rouge1\"]\n",
    "    eval_results[\"eval_rouge2\"] = eval_results_raw[\"eval_rouge2\"]\n",
    "    eval_results[\"eval_rougeL\"] = eval_results_raw[\"eval_rougeL\"]\n",
    "    eval_results[\"eval_rougeLsum\"] = eval_results_raw[\"eval_rougeLsum\"]\n",
    "\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf70a8df-71a6-43f7-b1a8-b82c6abc7335",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "MODEL_CARD_TEMPLATE = \"\"\"\n",
    "---\n",
    "language: en\n",
    "tags:\n",
    "- sagemaker\n",
    "- bart\n",
    "- summarization\n",
    "license: apache-2.0\n",
    "datasets:\n",
    "- samsum\n",
    "model-index:\n",
    "- name: {model_name}\n",
    "  results:\n",
    "  - task: \n",
    "      name: Abstractive Text Summarization\n",
    "      type: abstractive-text-summarization\n",
    "    dataset:\n",
    "      name: \"SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization\" \n",
    "      type: samsum\n",
    "    metrics:\n",
    "       - name: Validation ROGUE-1\n",
    "         type: rogue-1\n",
    "         value: 42.621\n",
    "       - name: Validation ROGUE-2\n",
    "         type: rogue-2\n",
    "         value: 21.9825\n",
    "       - name: Validation ROGUE-L\n",
    "         type: rogue-l\n",
    "         value: 33.034\n",
    "       - name: Test ROGUE-1\n",
    "         type: rogue-1\n",
    "         value: 41.3174\n",
    "       - name: Test ROGUE-2\n",
    "         type: rogue-2\n",
    "         value: 20.8716\n",
    "       - name: Test ROGUE-L\n",
    "         type: rogue-l\n",
    "         value: 32.1337\n",
    "widget:\n",
    "- text: | \n",
    "    Jeff: Can I train a 🤗 Transformers model on Amazon SageMaker? \n",
    "    Philipp: Sure you can use the new Hugging Face Deep Learning Container. \n",
    "    Jeff: ok.\n",
    "    Jeff: and how can I get started? \n",
    "    Jeff: where can I find documentation? \n",
    "    Philipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face \n",
    "---\n",
    "## `{model_name}`\n",
    "This model was trained using Amazon SageMaker and the new Hugging Face Deep Learning container.\n",
    "For more information look at:\n",
    "- [🤗 Transformers Documentation: Amazon SageMaker](https://huggingface.co/transformers/sagemaker.html)\n",
    "- [Example Notebooks](https://github.com/huggingface/notebooks/tree/master/sagemaker)\n",
    "- [Amazon SageMaker documentation for Hugging Face](https://docs.aws.amazon.com/sagemaker/latest/dg/hugging-face.html)\n",
    "- [Python SDK SageMaker documentation for Hugging Face](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/index.html)\n",
    "- [Deep Learning Container](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-training-containers)\n",
    "## Hyperparameters\n",
    "    {hyperparameters}\n",
    "## Usage\n",
    "    from transformers import pipeline\n",
    "    summarizer = pipeline(\"summarization\", model=\"philschmid/{model_name}\")\n",
    "    conversation = '''Jeff: Can I train a 🤗 Transformers model on Amazon SageMaker? \n",
    "    Philipp: Sure you can use the new Hugging Face Deep Learning Container. \n",
    "    Jeff: ok.\n",
    "    Jeff: and how can I get started? \n",
    "    Jeff: where can I find documentation? \n",
    "    Philipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face                                           \n",
    "    '''\n",
    "    summarizer(conversation)\n",
    "## Results\n",
    "| key | value |\n",
    "| --- | ----- |\n",
    "{eval_table}\n",
    "\"\"\"\n",
    "\n",
    "# Generate model card (todo: add more data from Trainer)\n",
    "model_card = MODEL_CARD_TEMPLATE.format(\n",
    "    model_name=f\"{hyperparameters['model_name_or_path'].split('/')[1]}-{hyperparameters['dataset_name']}\",\n",
    "    hyperparameters=json.dumps(hyperparameters, indent=4, sort_keys=True),\n",
    "    eval_table=\"\\n\".join(f\"| {k} | {v} |\" for k, v in eval_results.items()),\n",
    ")\n",
    "with open(f\"{local_path}/README.md\", \"w\") as f:\n",
    "    f.write(model_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1004087-6986-49d9-9654-691cc897f9b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ac32da-2e85-4895-8ddb-b48fbc812218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949c1dd6-a88e-4998-aa1e-a838ce47ac4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775a9c50-0dc9-450d-b549-8078b39b89ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d07988-2e16-4f89-95dd-703482b15341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165c8901-5c56-4386-97bd-f0f62d8916b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
