{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e9af52-2514-4661-a54e-09e3770396b4",
   "metadata": {},
   "source": [
    "# In SageMaker Studio\n",
    "\n",
    "https://github.com/huggingface/notebooks/blob/main/sagemaker/01_getting_started_pytorch/sagemaker-notebook.ipynb\n",
    "\n",
    "Do below changes to make it work:\n",
    "!pip install \"torch\" --upgrade\n",
    "\n",
    "from datasets import load_dataset, VerificationMode\n",
    "train_dataset, test_dataset = load_dataset('imdb', split=['train', 'test'], verification_mode=VerificationMode.NO_CHECKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd048926-82e5-43a8-9f8a-ef11c4794d62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker>=2.140.0 in /opt/conda/lib/python3.10/site-packages (2.203.1)\n",
      "Requirement already satisfied: transformers==4.26.1 in /opt/conda/lib/python3.10/site-packages (4.26.1)\n",
      "Requirement already satisfied: datasets==2.10.1 in /opt/conda/lib/python3.10/site-packages (from datasets[s3]==2.10.1) (2.10.1)\n",
      "Collecting torch\n",
      "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from transformers==4.26.1) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (2022.7.9)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (4.64.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1->datasets[s3]==2.10.1) (14.0.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1->datasets[s3]==2.10.1) (0.3.6)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1->datasets[s3]==2.10.1) (2.1.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1->datasets[s3]==2.10.1) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1->datasets[s3]==2.10.1) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.11.1->datasets==2.10.1->datasets[s3]==2.10.1) (2022.7.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1->datasets[s3]==2.10.1) (3.9.1)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1->datasets[s3]==2.10.1) (0.18.0)\n",
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.10/site-packages (from datasets[s3]==2.10.1) (0.4.2)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.33.3 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (1.33.9)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (0.2.0)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (4.25.1)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (4.11.3)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (0.3.0)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (0.7.5)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (4.20.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (2.5.2)\n",
      "Requirement already satisfied: tblib<3,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (1.7.0)\n",
      "Requirement already satisfied: urllib3<1.27 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (1.26.18)\n",
      "Requirement already satisfied: uvicorn==0.22.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (0.22.0)\n",
      "Requirement already satisfied: fastapi==0.95.2 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (0.95.2)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (6.1.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (5.9.0)\n",
      "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from fastapi==0.95.2->sagemaker>=2.140.0) (1.10.13)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi==0.95.2->sagemaker>=2.140.0) (0.27.0)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn==0.22.0->sagemaker>=2.140.0) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn==0.22.0->sagemaker>=2.140.0) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.1.0 (from torch)\n",
      "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: botocore<1.34.0,>=1.33.9 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker>=2.140.0) (1.33.9)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker>=2.140.0) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.9.0,>=0.8.2 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker>=2.140.0) (0.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker>=2.140.0) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.26.1) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.26.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.26.1) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.26.1) (2023.11.17)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1->datasets[s3]==2.10.1) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1->datasets[s3]==2.10.1) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1->datasets[s3]==2.10.1) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1->datasets[s3]==2.10.1) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1->datasets[s3]==2.10.1) (4.0.3)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from docker->sagemaker>=2.140.0) (0.58.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-pasta->sagemaker>=2.140.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker>=2.140.0) (2023.11.2)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker>=2.140.0) (0.31.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker>=2.140.0) (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.10.1->datasets[s3]==2.10.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.10.1->datasets[s3]==2.10.1) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.10.1->datasets[s3]==2.10.1) (2023.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2.140.0) (1.7.6.7)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2.140.0) (0.3.3)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker>=2.140.0) (21.6.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker>=2.140.0) (3.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker>=2.140.0) (1.2.0)\n",
      "Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 torch-2.1.2 triton-2.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.140.0\" \"transformers==4.26.1\" \"datasets[s3]==2.10.1\" \"torch\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e37c6ef-606f-48af-aa30-318894c27dfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4b9294-00fa-4d0a-817d-2f880b0478bc",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e1c3599-1755-4b18-b5e3-7a804e46f1cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::802575742115:role/service-role/AmazonSageMaker-ExecutionRole-20230929T143152\n",
      "sagemaker bucket: sagemaker-us-east-1-802575742115\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "# ACCESS to an IAM role\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc79ebe-2e0c-463d-bb26-24b9c1f56ef8",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d28db1d7-9fae-4a50-b4d9-2c0eaed2ec3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/parquet/plain_text-745310791ff4d097/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9777be5d11694a45bb3df63fc22e0d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/parquet/plain_text-745310791ff4d097/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-ec473c0cf9d9f4ca.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.'], 'label': [0, 0]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.'], 'label': [0, 0], 'input_ids': [[101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1000, 1045, 2572, 8025, 1024, 3756, 1000, 2003, 1037, 15544, 19307, 1998, 3653, 6528, 20771, 19986, 8632, 1012, 2009, 2987, 1005, 1056, 3043, 2054, 2028, 1005, 1055, 2576, 5328, 2024, 2138, 2023, 2143, 2064, 6684, 2022, 2579, 5667, 2006, 2151, 2504, 1012, 2004, 2005, 1996, 4366, 2008, 19124, 3287, 16371, 25469, 2003, 2019, 6882, 13316, 1011, 2459, 1010, 2008, 3475, 1005, 1056, 2995, 1012, 1045, 1005, 2310, 2464, 1054, 1011, 6758, 3152, 2007, 3287, 16371, 25469, 1012, 4379, 1010, 2027, 2069, 3749, 2070, 25085, 5328, 1010, 2021, 2073, 2024, 1996, 1054, 1011, 6758, 3152, 2007, 21226, 24728, 22144, 2015, 1998, 20916, 4691, 6845, 2401, 1029, 7880, 1010, 2138, 2027, 2123, 1005, 1056, 4839, 1012, 1996, 2168, 3632, 2005, 2216, 10231, 7685, 5830, 3065, 1024, 8040, 7317, 5063, 2015, 11820, 1999, 1996, 9478, 2021, 2025, 1037, 17962, 21239, 1999, 4356, 1012, 1998, 2216, 3653, 6528, 20771, 10271, 5691, 2066, 1996, 2829, 16291, 1010, 1999, 2029, 2057, 1005, 2128, 5845, 2000, 1996, 2609, 1997, 6320, 25624, 1005, 1055, 17061, 3779, 1010, 2021, 2025, 1037, 7637, 1997, 5061, 5710, 2006, 9318, 7367, 5737, 19393, 1012, 2077, 6933, 1006, 2030, 20242, 1007, 1000, 3313, 1011, 3115, 1000, 1999, 5609, 1997, 16371, 25469, 1010, 1996, 10597, 27885, 5809, 2063, 2323, 2202, 2046, 4070, 2028, 14477, 6767, 8524, 6321, 5793, 28141, 4489, 2090, 2273, 1998, 2308, 1024, 2045, 2024, 2053, 8991, 18400, 2015, 2006, 4653, 2043, 19910, 3544, 15287, 1010, 1998, 1996, 2168, 3685, 2022, 2056, 2005, 1037, 2158, 1012, 1999, 2755, 1010, 2017, 3227, 2180, 1005, 1056, 2156, 2931, 8991, 18400, 2015, 1999, 2019, 2137, 2143, 1999, 2505, 2460, 1997, 22555, 2030, 13216, 14253, 2050, 1012, 2023, 6884, 3313, 1011, 3115, 2003, 2625, 1037, 3313, 3115, 2084, 2019, 4914, 2135, 2139, 24128, 3754, 2000, 2272, 2000, 3408, 20547, 2007, 1996, 19008, 1997, 2308, 1005, 1055, 4230, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "{'labels': tensor([0, 0]), 'input_ids': tensor([[  101,  1045, 12524,  ...,     0,     0,     0],\n",
      "        [  101,  1000,  1045,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, VerificationMode\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer used in preprocessing\n",
    "tokenizer_name = 'distilbert-base-uncased'\n",
    "\n",
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# load dataset\n",
    "train_dataset, test_dataset = load_dataset('imdb', split=['train', 'test'], verification_mode=VerificationMode.NO_CHECKS)\n",
    "test_dataset = test_dataset.shuffle().select(range(10000)) # smaller the size for test dataset to 10k \n",
    "\n",
    "print(train_dataset[0:2])\n",
    "\n",
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "print(train_dataset[0:2])\n",
    "\n",
    "# input_ids are tokenized and mapped to corresponding indices in the model's vocabulary.\n",
    "\n",
    "# segment_ids \n",
    "# BERT was designed to handle tasks involving multiple sentences. segment_ids are used to distinguish different sentences in the input.\n",
    "# For a single sentence task, all elements in segment_ids would be set to 0. For tasks involving multiple sentences, you might have alternating 0s and 1s to indicate different segments.\n",
    "\n",
    "# attention_mask:\n",
    "# BERT models have a fixed input size, the attention mask is used to indicate which elements of the input sequence are actual tokens versus padding tokens.\n",
    "# 0 indicates it is a padding token. This mask helps the model focus only on the relevant tokens and ignore the padding tokens during training.\n",
    "\n",
    "# set format for pytorch\n",
    "# set_format('torch'): This method is used to set the format of the dataset to be compatible with PyTorch. \n",
    "# columns=['input_ids', 'attention_mask', 'labels']: This specifies which columns of the dataset should be converted to PyTorch tensors. \n",
    "train_dataset =  train_dataset.rename_column(\"label\", \"labels\")\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "print(train_dataset[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712ec7fd-cf90-455a-83fe-be37d71cd557",
   "metadata": {},
   "source": [
    "## Uploading data to sagemaker_session_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f1ae09f-5b3f-48fe-b908-a19b13ffa725",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# s3 key prefix for the data\n",
    "s3_prefix = 'samples/datasets/imdb'\n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "train_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "test_dataset.save_to_disk(test_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800fbcb7-0600-4722-bd59-451abb4d56e9",
   "metadata": {},
   "source": [
    "## Fine-tuning & starting SageMaker Training Job\n",
    "In order to create a sagemaker training job we need an HuggingFace Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In a Estimator we define, which fine-tuning script should be used as entry_point, which instance_type should be used, which hyperparameters are passed in .....\n",
    "\n",
    "```python\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            base_job_name='huggingface-sdk-extension',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            transformers_version='4.4',\n",
    "                            pytorch_version='1.6',\n",
    "                            py_version='py36',\n",
    "                            role=role,\n",
    "                            hyperparameters = {'epochs': 1,\n",
    "                                               'train_batch_size': 32,\n",
    "                                               'model_name':'distilbert-base-uncased'\n",
    "                                                })\n",
    "```\n",
    "\n",
    "When we create a SageMaker training job, SageMaker takes care of starting and managing all the required ec2 instances for us with the huggingface container, uploads the provided fine-tuning script train.py and downloads the data from our sagemaker_session_bucket into the container at /opt/ml/input/data. Then, it starts the training job by running.\n",
    "\n",
    "```python\n",
    "/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
    "The hyperparameters you define in the HuggingFace estimator are passed in as named arguments.\n",
    "```\n",
    "\n",
    "Sagemaker is providing useful properties about the training environment through various environment variables, including the following:\n",
    "\n",
    "- SM_MODEL_DIR: A string that represents the path where the training job writes the model artifacts to. After training, artifacts in this directory are uploaded to S3 for model hosting.\n",
    "\n",
    "- SM_NUM_GPUS: An integer representing the number of GPUs available to the host.\n",
    "\n",
    "- SM_CHANNEL_XXXX: A string that represents the path to the directory that contains the input data for the specified channel. For example, if you specify two input channels in the HuggingFace estimator’s fit call, named train and test, the environment variables SM_CHANNEL_TRAIN and SM_CHANNEL_TEST are set.\n",
    "\n",
    "To run your training job locally you can define instance_type='local' or instance_type='local_gpu' for gpu usage. Note: this does not working within SageMaker Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026e2141-b1b5-41a4-823c-50d4003db5c3",
   "metadata": {},
   "source": [
    "## Creating an Estimator and start a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fd8a9d0-d8e5-4074-89f9-6826eab1ae74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2024-01-22-05-34-03-155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-22 05:34:03 Starting - Starting the training job...\n",
      "2024-01-22 05:34:03 Pending - Training job waiting for capacity.........\n",
      "2024-01-22 05:35:35 Pending - Preparing the instances for training...\n",
      "2024-01-22 05:36:32 Downloading - Downloading input data...\n",
      "2024-01-22 05:36:57 Downloading - Downloading the training image...........................\n",
      "2024-01-22 05:41:18 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-01-22 05:41:36,924 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-01-22 05:41:36,944 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-22 05:41:36,958 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-01-22 05:41:36,960 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-01-22 05:41:37,277 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-22 05:41:37,312 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-22 05:41:37,345 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-22 05:41:37,359 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"train_batch_size\": 32\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2024-01-22-05-34-03-155\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-802575742115/huggingface-pytorch-training-2024-01-22-05-34-03-155/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-802575742115/huggingface-pytorch-training-2024-01-22-05-34-03-155/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2024-01-22-05-34-03-155\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-802575742115/huggingface-pytorch-training-2024-01-22-05-34-03-155/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"32\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\u001b[0m\n",
      "\u001b[34m[2024-01-22 05:41:39.522: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2024-01-22 05:41:39,529 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2024-01-22 05:41:39,560 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m2024-01-22 05:41:42,943 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m2024-01-22 05:41:42,943 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34mDownloading (…)\"config.json\";:   0%|          | 0.00/483 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"config.json\";: 100%|██████████| 483/483 [00:00<00:00, 62.3kB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/268M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  16%|█▌        | 41.9M/268M [00:00<00:00, 390MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  35%|███▌      | 94.4M/268M [00:00<00:00, 435MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  55%|█████▍    | 147M/268M [00:00<00:00, 462MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  74%|███████▍  | 199M/268M [00:00<00:00, 463MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  94%|█████████▍| 252M/268M [00:00<00:00, 474MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";: 100%|██████████| 268M/268M [00:00<00:00, 458MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading (…)enizer_config.json\";:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)enizer_config.json\";: 100%|██████████| 28.0/28.0 [00:00<00:00, 10.3kB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"vocab.txt\";:   0%|          | 0.00/232k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"vocab.txt\";: 100%|██████████| 232k/232k [00:00<00:00, 43.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"tokenizer.json\";:   0%|          | 0.00/466k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"tokenizer.json\";: 100%|██████████| 466k/466k [00:00<00:00, 48.6MB/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 25000\u001b[0m\n",
      "\u001b[34mNum examples = 25000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34mNum Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 782\u001b[0m\n",
      "\u001b[34mTotal optimization steps = 782\u001b[0m\n",
      "\u001b[34mNumber of trainable parameters = 66955010\u001b[0m\n",
      "\u001b[34mNumber of trainable parameters = 66955010\u001b[0m\n",
      "\u001b[34m0%|          | 0/782 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2024-01-22 05:41:46.955: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2024-01-22 05:41:46,962 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2024-01-22 05:41:46.997 algo-1:49 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-01-22 05:41:47.041 algo-1:49 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2024-01-22 05:41:47.042 algo-1:49 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-01-22 05:41:47.042 algo-1:49 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2024-01-22 05:41:47.043 algo-1:49 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-01-22 05:41:47.043 algo-1:49 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m0%|          | 1/782 [00:01<22:13,  1.71s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/782 [00:02<12:38,  1.03it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 3/782 [00:02<09:36,  1.35it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 4/782 [00:03<08:08,  1.59it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 5/782 [00:03<07:20,  1.77it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 6/782 [00:03<06:49,  1.89it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 7/782 [00:04<06:31,  1.98it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 8/782 [00:04<06:18,  2.05it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 9/782 [00:05<06:09,  2.09it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 10/782 [00:05<06:03,  2.12it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 11/782 [00:06<05:59,  2.15it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 12/782 [00:06<05:56,  2.16it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 13/782 [00:07<05:53,  2.17it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 14/782 [00:07<05:52,  2.18it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 15/782 [00:08<05:51,  2.18it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 16/782 [00:08<05:50,  2.18it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 17/782 [00:09<05:49,  2.19it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 18/782 [00:09<05:50,  2.18it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 19/782 [00:09<05:50,  2.18it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 20/782 [00:10<05:49,  2.18it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 21/782 [00:10<05:47,  2.19it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 22/782 [00:11<05:47,  2.19it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 23/782 [00:11<05:46,  2.19it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 24/782 [00:12<05:45,  2.19it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 25/782 [00:12<05:45,  2.19it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 26/782 [00:13<05:45,  2.19it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 27/782 [00:13<05:44,  2.19it/s]\u001b[0m\n",
      "\u001b[34m4%|▎         | 28/782 [00:14<05:44,  2.19it/s]\u001b[0m\n",
      "\u001b[34m4%|▎         | 29/782 [00:14<05:44,  2.19it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 30/782 [00:14<05:43,  2.19it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 31/782 [00:15<05:44,  2.18it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 32/782 [00:15<05:43,  2.18it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 33/782 [00:16<05:42,  2.19it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 34/782 [00:16<05:41,  2.19it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 35/782 [00:17<05:40,  2.20it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 36/782 [00:17<05:39,  2.20it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 37/782 [00:18<05:38,  2.20it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 38/782 [00:18<05:37,  2.20it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 39/782 [00:19<05:37,  2.20it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 40/782 [00:19<05:36,  2.20it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 41/782 [00:19<05:36,  2.20it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 42/782 [00:20<05:36,  2.20it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 43/782 [00:20<05:36,  2.19it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 44/782 [00:21<05:36,  2.19it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 45/782 [00:21<05:35,  2.19it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 46/782 [00:22<05:34,  2.20it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 47/782 [00:22<05:34,  2.20it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 48/782 [00:23<05:33,  2.20it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 49/782 [00:23<05:33,  2.20it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 50/782 [00:24<05:32,  2.20it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 51/782 [00:24<05:32,  2.20it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 52/782 [00:24<05:32,  2.19it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 53/782 [00:25<05:32,  2.19it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 54/782 [00:25<05:32,  2.19it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 55/782 [00:26<05:31,  2.19it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 56/782 [00:26<05:30,  2.20it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 57/782 [00:27<05:29,  2.20it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 58/782 [00:27<05:28,  2.20it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 59/782 [00:28<05:28,  2.20it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 60/782 [00:28<05:28,  2.20it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 61/782 [00:29<05:28,  2.20it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 62/782 [00:29<05:28,  2.19it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 63/782 [00:29<05:29,  2.18it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 64/782 [00:30<05:30,  2.17it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 65/782 [00:30<05:29,  2.17it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 66/782 [00:31<05:28,  2.18it/s]\u001b[0m\n",
      "\u001b[34m9%|▊         | 67/782 [00:31<05:28,  2.17it/s]\u001b[0m\n",
      "\u001b[34m9%|▊         | 68/782 [00:32<05:28,  2.18it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 69/782 [00:32<05:27,  2.18it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 70/782 [00:33<05:26,  2.18it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 71/782 [00:33<05:25,  2.18it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 72/782 [00:34<05:26,  2.18it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 73/782 [00:34<05:25,  2.18it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 74/782 [00:35<05:24,  2.18it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 75/782 [00:35<05:24,  2.18it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 76/782 [00:35<05:24,  2.18it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 77/782 [00:36<05:23,  2.18it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 78/782 [00:36<05:23,  2.18it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 79/782 [00:37<05:22,  2.18it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 80/782 [00:37<05:21,  2.18it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 81/782 [00:38<05:20,  2.19it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 82/782 [00:38<05:19,  2.19it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 83/782 [00:39<05:18,  2.19it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 84/782 [00:39<05:18,  2.19it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 85/782 [00:40<05:18,  2.19it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 86/782 [00:40<05:18,  2.19it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 87/782 [00:40<05:17,  2.19it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 88/782 [00:41<05:16,  2.19it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 89/782 [00:41<05:16,  2.19it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 90/782 [00:42<05:15,  2.19it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 91/782 [00:42<05:15,  2.19it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 92/782 [00:43<05:15,  2.19it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 93/782 [00:43<05:14,  2.19it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 94/782 [00:44<05:14,  2.19it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 95/782 [00:44<05:14,  2.19it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 96/782 [00:45<05:13,  2.19it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 97/782 [00:45<05:12,  2.19it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 98/782 [00:46<05:12,  2.19it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 99/782 [00:46<05:12,  2.19it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 100/782 [00:46<05:12,  2.18it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 101/782 [00:47<05:11,  2.19it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 102/782 [00:47<05:10,  2.19it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 103/782 [00:48<05:09,  2.19it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 104/782 [00:48<05:09,  2.19it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 105/782 [00:49<05:08,  2.19it/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 106/782 [00:49<05:08,  2.19it/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 107/782 [00:50<05:09,  2.18it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 108/782 [00:50<05:08,  2.18it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 109/782 [00:51<05:07,  2.19it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 110/782 [00:51<05:06,  2.19it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 111/782 [00:51<05:06,  2.19it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 112/782 [00:52<05:05,  2.19it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 113/782 [00:52<05:04,  2.19it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 114/782 [00:53<05:04,  2.20it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 115/782 [00:53<05:03,  2.20it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 116/782 [00:54<05:03,  2.19it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 117/782 [00:54<05:04,  2.19it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 118/782 [00:55<05:03,  2.19it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 119/782 [00:55<05:02,  2.19it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 120/782 [00:56<05:02,  2.19it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 121/782 [00:56<05:02,  2.19it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 122/782 [00:56<05:01,  2.19it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 123/782 [00:57<05:01,  2.19it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 124/782 [00:57<05:00,  2.19it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 125/782 [00:58<04:59,  2.19it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 126/782 [00:58<04:59,  2.19it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 127/782 [00:59<04:58,  2.20it/s]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 128/782 [00:59<04:57,  2.20it/s]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 129/782 [01:00<04:57,  2.20it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 130/782 [01:00<04:57,  2.19it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 131/782 [01:01<04:57,  2.19it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 132/782 [01:01<04:57,  2.18it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 133/782 [01:01<04:57,  2.18it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 134/782 [01:02<04:57,  2.18it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 135/782 [01:02<04:55,  2.19it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 136/782 [01:03<04:54,  2.19it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 137/782 [01:03<04:54,  2.19it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 138/782 [01:04<04:54,  2.19it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 139/782 [01:04<04:53,  2.19it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 140/782 [01:05<04:53,  2.19it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 141/782 [01:05<04:52,  2.19it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 142/782 [01:06<04:53,  2.18it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 143/782 [01:06<04:52,  2.18it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 144/782 [01:07<04:52,  2.18it/s]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 145/782 [01:07<04:51,  2.19it/s]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 146/782 [01:07<04:51,  2.18it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 147/782 [01:08<04:51,  2.18it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 148/782 [01:08<04:50,  2.18it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 149/782 [01:09<04:49,  2.18it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 150/782 [01:09<04:49,  2.19it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 151/782 [01:10<04:48,  2.19it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 152/782 [01:10<04:47,  2.19it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 153/782 [01:11<04:47,  2.19it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 154/782 [01:11<04:47,  2.19it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 155/782 [01:12<04:46,  2.19it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 156/782 [01:12<04:45,  2.19it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 157/782 [01:12<04:45,  2.19it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 158/782 [01:13<04:44,  2.19it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 159/782 [01:13<04:44,  2.19it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 160/782 [01:14<04:44,  2.19it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 161/782 [01:14<04:43,  2.19it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 162/782 [01:15<04:43,  2.19it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 163/782 [01:15<04:44,  2.17it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 164/782 [01:16<04:44,  2.17it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 165/782 [01:16<04:43,  2.18it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 166/782 [01:17<04:42,  2.18it/s]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 167/782 [01:17<04:41,  2.19it/s]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 168/782 [01:18<04:40,  2.19it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 169/782 [01:18<04:39,  2.19it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 170/782 [01:18<04:38,  2.20it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 171/782 [01:19<04:38,  2.19it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 172/782 [01:19<04:38,  2.19it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 173/782 [01:20<04:38,  2.19it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 174/782 [01:20<04:37,  2.19it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 175/782 [01:21<04:37,  2.19it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 176/782 [01:21<04:37,  2.19it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 177/782 [01:22<04:36,  2.18it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 178/782 [01:22<04:36,  2.18it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 179/782 [01:23<04:35,  2.19it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 180/782 [01:23<04:35,  2.19it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 181/782 [01:23<04:34,  2.19it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 182/782 [01:24<04:34,  2.19it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 183/782 [01:24<04:33,  2.19it/s]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 184/782 [01:25<04:33,  2.19it/s]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 185/782 [01:25<04:33,  2.19it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 186/782 [01:26<04:33,  2.18it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 187/782 [01:26<04:32,  2.18it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 188/782 [01:27<04:31,  2.19it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 189/782 [01:27<04:31,  2.18it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 190/782 [01:28<04:31,  2.18it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 191/782 [01:28<04:30,  2.18it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 192/782 [01:28<04:30,  2.18it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 193/782 [01:29<04:29,  2.19it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 194/782 [01:29<04:29,  2.18it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 195/782 [01:30<04:28,  2.18it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 196/782 [01:30<04:27,  2.19it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 197/782 [01:31<04:27,  2.19it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 198/782 [01:31<04:27,  2.18it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 199/782 [01:32<04:26,  2.19it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 200/782 [01:32<04:25,  2.19it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 201/782 [01:33<04:24,  2.20it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 202/782 [01:33<04:24,  2.19it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 203/782 [01:34<04:24,  2.19it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 204/782 [01:34<04:23,  2.19it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 205/782 [01:34<04:23,  2.19it/s]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 206/782 [01:35<04:22,  2.19it/s]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 207/782 [01:35<04:21,  2.20it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 208/782 [01:36<04:21,  2.20it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 209/782 [01:36<04:20,  2.20it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 210/782 [01:37<04:20,  2.20it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 211/782 [01:37<04:20,  2.20it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 212/782 [01:38<04:19,  2.19it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 213/782 [01:38<04:19,  2.19it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 214/782 [01:39<04:18,  2.19it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 215/782 [01:39<04:18,  2.19it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 216/782 [01:39<04:17,  2.19it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 217/782 [01:40<04:17,  2.20it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 218/782 [01:40<04:16,  2.20it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 219/782 [01:41<04:16,  2.20it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 220/782 [01:41<04:15,  2.20it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 221/782 [01:42<04:15,  2.20it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 222/782 [01:42<04:15,  2.20it/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 223/782 [01:43<04:14,  2.20it/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 224/782 [01:43<04:14,  2.19it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 225/782 [01:44<04:14,  2.19it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 226/782 [01:44<04:13,  2.20it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 227/782 [01:44<04:12,  2.19it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 228/782 [01:45<04:12,  2.19it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 229/782 [01:45<04:12,  2.19it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 230/782 [01:46<04:12,  2.19it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 231/782 [01:46<04:11,  2.19it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 232/782 [01:47<04:10,  2.19it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 233/782 [01:47<04:10,  2.20it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 234/782 [01:48<04:09,  2.19it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 235/782 [01:48<04:09,  2.20it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 236/782 [01:49<04:08,  2.20it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 237/782 [01:49<04:07,  2.20it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 238/782 [01:49<04:07,  2.20it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 239/782 [01:50<04:06,  2.20it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 240/782 [01:50<04:06,  2.20it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 241/782 [01:51<04:06,  2.20it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 242/782 [01:51<04:05,  2.20it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 243/782 [01:52<04:05,  2.20it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 244/782 [01:52<04:04,  2.20it/s]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 245/782 [01:53<04:05,  2.19it/s]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 246/782 [01:53<04:05,  2.19it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 247/782 [01:54<04:04,  2.19it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 248/782 [01:54<04:04,  2.19it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 249/782 [01:54<04:03,  2.19it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 250/782 [01:55<04:02,  2.19it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 251/782 [01:55<04:02,  2.19it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 252/782 [01:56<04:01,  2.19it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 253/782 [01:56<04:00,  2.20it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 254/782 [01:57<04:00,  2.20it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 255/782 [01:57<04:00,  2.20it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 256/782 [01:58<03:59,  2.19it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 257/782 [01:58<03:59,  2.19it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 258/782 [01:59<03:59,  2.19it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 259/782 [01:59<03:58,  2.19it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 260/782 [01:59<03:58,  2.19it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 261/782 [02:00<03:57,  2.19it/s]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 262/782 [02:00<03:57,  2.19it/s]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 263/782 [02:01<03:56,  2.19it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 264/782 [02:01<03:56,  2.19it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 265/782 [02:02<03:55,  2.19it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 266/782 [02:02<03:55,  2.19it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 267/782 [02:03<03:55,  2.19it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 268/782 [02:03<03:54,  2.19it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 269/782 [02:04<03:54,  2.19it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 270/782 [02:04<03:53,  2.19it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 271/782 [02:05<03:53,  2.19it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 272/782 [02:05<03:53,  2.19it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 273/782 [02:05<03:52,  2.19it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 274/782 [02:06<03:52,  2.19it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 275/782 [02:06<03:51,  2.19it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 276/782 [02:07<03:50,  2.19it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 277/782 [02:07<03:50,  2.19it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 278/782 [02:08<03:50,  2.19it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 279/782 [02:08<03:49,  2.19it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 280/782 [02:09<03:48,  2.19it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 281/782 [02:09<03:48,  2.19it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 282/782 [02:10<03:48,  2.18it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 283/782 [02:10<03:48,  2.19it/s]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 284/782 [02:10<03:47,  2.19it/s]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 285/782 [02:11<03:47,  2.18it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 286/782 [02:11<03:47,  2.18it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 287/782 [02:12<03:46,  2.18it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 288/782 [02:12<03:45,  2.19it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 289/782 [02:13<03:45,  2.19it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 290/782 [02:13<03:44,  2.19it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 291/782 [02:14<03:43,  2.20it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 292/782 [02:14<03:43,  2.19it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 293/782 [02:15<03:42,  2.19it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 294/782 [02:15<03:42,  2.19it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 295/782 [02:15<03:42,  2.19it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 296/782 [02:16<03:42,  2.19it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 297/782 [02:16<03:41,  2.19it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 298/782 [02:17<03:40,  2.19it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 299/782 [02:17<03:40,  2.19it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 300/782 [02:18<03:39,  2.20it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 301/782 [02:18<03:39,  2.19it/s]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 302/782 [02:19<03:39,  2.19it/s]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 303/782 [02:19<03:39,  2.19it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 304/782 [02:20<03:38,  2.19it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 305/782 [02:20<03:38,  2.18it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 306/782 [02:20<03:37,  2.19it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 307/782 [02:21<03:37,  2.19it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 308/782 [02:21<03:36,  2.19it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 309/782 [02:22<03:35,  2.19it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 310/782 [02:22<03:35,  2.19it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 311/782 [02:23<03:35,  2.19it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 312/782 [02:23<03:34,  2.19it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 313/782 [02:24<03:34,  2.19it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 314/782 [02:24<03:33,  2.19it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 315/782 [02:25<03:33,  2.19it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 316/782 [02:25<03:33,  2.19it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 317/782 [02:26<03:33,  2.18it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 318/782 [02:26<03:32,  2.18it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 319/782 [02:26<03:31,  2.19it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 320/782 [02:27<03:31,  2.19it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 321/782 [02:27<03:30,  2.19it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 322/782 [02:28<03:29,  2.19it/s]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 323/782 [02:28<03:29,  2.19it/s]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 324/782 [02:29<03:28,  2.19it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 325/782 [02:29<03:28,  2.19it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 326/782 [02:30<03:27,  2.19it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 327/782 [02:30<03:27,  2.19it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 328/782 [02:31<03:27,  2.19it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 329/782 [02:31<03:26,  2.19it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 330/782 [02:31<03:26,  2.19it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 331/782 [02:32<03:26,  2.19it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 332/782 [02:32<03:25,  2.19it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 333/782 [02:33<03:25,  2.18it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 334/782 [02:33<03:24,  2.19it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 335/782 [02:34<03:24,  2.19it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 336/782 [02:34<03:23,  2.19it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 337/782 [02:35<03:23,  2.18it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 338/782 [02:35<03:23,  2.18it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 339/782 [02:36<03:23,  2.18it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 340/782 [02:36<03:22,  2.18it/s]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 341/782 [02:36<03:22,  2.18it/s]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 342/782 [02:37<03:21,  2.18it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 343/782 [02:37<03:21,  2.18it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 344/782 [02:38<03:20,  2.19it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 345/782 [02:38<03:19,  2.19it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 346/782 [02:39<03:18,  2.19it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 347/782 [02:39<03:18,  2.19it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 348/782 [02:40<03:18,  2.19it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 349/782 [02:40<03:17,  2.19it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 350/782 [02:41<03:17,  2.19it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 351/782 [02:41<03:16,  2.19it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 352/782 [02:42<03:15,  2.20it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 353/782 [02:42<03:15,  2.20it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 354/782 [02:42<03:14,  2.20it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 355/782 [02:43<03:14,  2.20it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 356/782 [02:43<03:13,  2.20it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 357/782 [02:44<03:13,  2.19it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 358/782 [02:44<03:13,  2.19it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 359/782 [02:45<03:13,  2.19it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 360/782 [02:45<03:13,  2.19it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 361/782 [02:46<03:12,  2.19it/s]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 362/782 [02:46<03:11,  2.19it/s]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 363/782 [02:47<03:10,  2.19it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 364/782 [02:47<03:10,  2.19it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 365/782 [02:47<03:10,  2.19it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 366/782 [02:48<03:09,  2.19it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 367/782 [02:48<03:09,  2.19it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 368/782 [02:49<03:09,  2.19it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 369/782 [02:49<03:08,  2.19it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 370/782 [02:50<03:07,  2.19it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 371/782 [02:50<03:07,  2.19it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 372/782 [02:51<03:07,  2.19it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 373/782 [02:51<03:06,  2.19it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 374/782 [02:52<03:05,  2.20it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 375/782 [02:52<03:05,  2.20it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 376/782 [02:52<03:04,  2.20it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 377/782 [02:53<03:04,  2.20it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 378/782 [02:53<03:04,  2.20it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 379/782 [02:54<03:04,  2.19it/s]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 380/782 [02:54<03:03,  2.19it/s]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 381/782 [02:55<03:03,  2.19it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 382/782 [02:55<03:02,  2.19it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 383/782 [02:56<03:01,  2.19it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 384/782 [02:56<03:01,  2.19it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 385/782 [02:57<03:01,  2.19it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 386/782 [02:57<03:00,  2.19it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 387/782 [02:57<03:00,  2.19it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 388/782 [02:58<03:00,  2.19it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 389/782 [02:58<02:59,  2.19it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 390/782 [02:59<02:58,  2.19it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 391/782 [02:59<02:58,  2.19it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 392/782 [03:00<02:58,  2.19it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 393/782 [03:00<02:57,  2.19it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 394/782 [03:01<02:56,  2.19it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 395/782 [03:01<02:57,  2.19it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 396/782 [03:02<02:56,  2.19it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 397/782 [03:02<02:56,  2.18it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 398/782 [03:03<02:55,  2.18it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 399/782 [03:03<02:54,  2.19it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 400/782 [03:03<02:54,  2.19it/s]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 401/782 [03:04<02:54,  2.19it/s]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 402/782 [03:04<02:53,  2.19it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 403/782 [03:05<02:53,  2.18it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 404/782 [03:05<02:53,  2.18it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 405/782 [03:06<02:53,  2.18it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 406/782 [03:06<02:52,  2.18it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 407/782 [03:07<02:52,  2.18it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 408/782 [03:07<02:52,  2.17it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 409/782 [03:08<02:51,  2.17it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 410/782 [03:08<02:51,  2.17it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 411/782 [03:08<02:50,  2.18it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 412/782 [03:09<02:49,  2.18it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 413/782 [03:09<02:49,  2.18it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 414/782 [03:10<02:49,  2.17it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 415/782 [03:10<02:48,  2.18it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 416/782 [03:11<02:47,  2.18it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 417/782 [03:11<02:47,  2.18it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 418/782 [03:12<02:47,  2.18it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 419/782 [03:12<02:46,  2.18it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 420/782 [03:13<02:46,  2.17it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 421/782 [03:13<02:45,  2.18it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 422/782 [03:14<02:45,  2.18it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 423/782 [03:14<02:45,  2.17it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 424/782 [03:14<02:45,  2.17it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 425/782 [03:15<02:44,  2.17it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 426/782 [03:15<02:43,  2.17it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 427/782 [03:16<02:43,  2.17it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 428/782 [03:16<02:42,  2.17it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 429/782 [03:17<02:42,  2.17it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 430/782 [03:17<02:41,  2.17it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 431/782 [03:18<02:41,  2.17it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 432/782 [03:18<02:40,  2.18it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 433/782 [03:19<02:40,  2.18it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 434/782 [03:19<02:39,  2.18it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 435/782 [03:20<02:38,  2.18it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 436/782 [03:20<02:39,  2.18it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 437/782 [03:20<02:38,  2.18it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 438/782 [03:21<02:37,  2.18it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 439/782 [03:21<02:36,  2.19it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 440/782 [03:22<02:36,  2.18it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 441/782 [03:22<02:36,  2.18it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 442/782 [03:23<02:35,  2.18it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 443/782 [03:23<02:34,  2.19it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 444/782 [03:24<02:34,  2.18it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 445/782 [03:24<02:34,  2.19it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 446/782 [03:25<02:33,  2.19it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 447/782 [03:25<02:33,  2.19it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 448/782 [03:25<02:32,  2.19it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 449/782 [03:26<02:32,  2.19it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 450/782 [03:26<02:32,  2.18it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 451/782 [03:27<02:31,  2.18it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 452/782 [03:27<02:31,  2.18it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 453/782 [03:28<02:30,  2.19it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 454/782 [03:28<02:29,  2.19it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 455/782 [03:29<02:29,  2.19it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 456/782 [03:29<02:28,  2.19it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 457/782 [03:30<02:28,  2.19it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 458/782 [03:30<02:28,  2.19it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 459/782 [03:30<02:27,  2.19it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 460/782 [03:31<02:27,  2.19it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 461/782 [03:31<02:26,  2.19it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 462/782 [03:32<02:25,  2.19it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 463/782 [03:32<02:25,  2.19it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 464/782 [03:33<02:25,  2.19it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 465/782 [03:33<02:24,  2.19it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 466/782 [03:34<02:24,  2.19it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 467/782 [03:34<02:24,  2.19it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 468/782 [03:35<02:23,  2.18it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 469/782 [03:35<02:23,  2.19it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 470/782 [03:36<02:22,  2.19it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 471/782 [03:36<02:22,  2.19it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 472/782 [03:36<02:21,  2.19it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 473/782 [03:37<02:20,  2.19it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 474/782 [03:37<02:20,  2.19it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 475/782 [03:38<02:19,  2.19it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 476/782 [03:38<02:19,  2.20it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 477/782 [03:39<02:18,  2.20it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 478/782 [03:39<02:18,  2.19it/s]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 479/782 [03:40<02:18,  2.19it/s]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 480/782 [03:40<02:17,  2.19it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 481/782 [03:41<02:17,  2.19it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 482/782 [03:41<02:16,  2.20it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 483/782 [03:41<02:16,  2.20it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 484/782 [03:42<02:15,  2.19it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 485/782 [03:42<02:15,  2.20it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 486/782 [03:43<02:15,  2.19it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 487/782 [03:43<02:14,  2.19it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 488/782 [03:44<02:14,  2.19it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 489/782 [03:44<02:13,  2.19it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 490/782 [03:45<02:13,  2.18it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 491/782 [03:45<02:13,  2.18it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 492/782 [03:46<02:13,  2.17it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 493/782 [03:46<02:12,  2.18it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 494/782 [03:46<02:12,  2.17it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 495/782 [03:47<02:12,  2.17it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 496/782 [03:47<02:11,  2.17it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 497/782 [03:48<02:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 498/782 [03:48<02:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 499/782 [03:49<02:09,  2.18it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 500/782 [03:49<02:08,  2.19it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.3465, 'learning_rate': 5e-05, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 500/782 [03:49<02:08,  2.19it/s]\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-500\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-500\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-500/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-500/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-500/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-500/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-500/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-500/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-500/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-500/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 501/782 [03:51<03:55,  1.19it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 502/782 [03:51<03:22,  1.38it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 503/782 [03:52<02:59,  1.55it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 504/782 [03:52<02:43,  1.70it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 505/782 [03:53<02:31,  1.83it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 506/782 [03:53<02:23,  1.92it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 507/782 [03:54<02:17,  2.00it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 508/782 [03:54<02:13,  2.05it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 509/782 [03:55<02:10,  2.09it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 510/782 [03:55<02:08,  2.12it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 511/782 [03:56<02:06,  2.14it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 512/782 [03:56<02:05,  2.15it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 513/782 [03:56<02:04,  2.16it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 514/782 [03:57<02:03,  2.17it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 515/782 [03:57<02:02,  2.18it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 516/782 [03:58<02:02,  2.18it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 517/782 [03:58<02:01,  2.18it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 518/782 [03:59<02:01,  2.18it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 519/782 [03:59<02:00,  2.18it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 520/782 [04:00<02:00,  2.18it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 521/782 [04:00<01:59,  2.18it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 522/782 [04:01<01:59,  2.18it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 523/782 [04:01<01:58,  2.18it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 524/782 [04:01<01:58,  2.19it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 525/782 [04:02<01:57,  2.19it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 526/782 [04:02<01:56,  2.19it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 527/782 [04:03<01:56,  2.19it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 528/782 [04:03<01:56,  2.19it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 529/782 [04:04<01:55,  2.18it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 530/782 [04:04<01:55,  2.19it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 531/782 [04:05<01:54,  2.19it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 532/782 [04:05<01:54,  2.19it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 533/782 [04:06<01:53,  2.19it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 534/782 [04:06<01:53,  2.18it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 535/782 [04:06<01:52,  2.19it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 536/782 [04:07<01:52,  2.19it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 537/782 [04:07<01:51,  2.19it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 538/782 [04:08<01:51,  2.19it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 539/782 [04:08<01:50,  2.19it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 540/782 [04:09<01:50,  2.19it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 541/782 [04:09<01:49,  2.19it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 542/782 [04:10<01:49,  2.19it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 543/782 [04:10<01:49,  2.19it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 544/782 [04:11<01:48,  2.19it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 545/782 [04:11<01:48,  2.19it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 546/782 [04:12<01:47,  2.19it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 547/782 [04:12<01:47,  2.18it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 548/782 [04:12<01:47,  2.18it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 549/782 [04:13<01:46,  2.18it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 550/782 [04:13<01:46,  2.18it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 551/782 [04:14<01:45,  2.19it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 552/782 [04:14<01:45,  2.19it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 553/782 [04:15<01:44,  2.18it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 554/782 [04:15<01:44,  2.19it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 555/782 [04:16<01:44,  2.18it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 556/782 [04:16<01:43,  2.18it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 557/782 [04:17<01:43,  2.18it/s]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 558/782 [04:17<01:42,  2.18it/s]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 559/782 [04:17<01:42,  2.18it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 560/782 [04:18<01:41,  2.18it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 561/782 [04:18<01:41,  2.18it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 562/782 [04:19<01:40,  2.18it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 563/782 [04:19<01:40,  2.19it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 564/782 [04:20<01:39,  2.19it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 565/782 [04:20<01:39,  2.19it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 566/782 [04:21<01:38,  2.19it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 567/782 [04:21<01:38,  2.18it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 568/782 [04:22<01:37,  2.19it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 569/782 [04:22<01:37,  2.18it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 570/782 [04:23<01:37,  2.18it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 571/782 [04:23<01:36,  2.18it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 572/782 [04:23<01:36,  2.18it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 573/782 [04:24<01:35,  2.18it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 574/782 [04:24<01:35,  2.18it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 575/782 [04:25<01:34,  2.18it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 576/782 [04:25<01:34,  2.19it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 577/782 [04:26<01:33,  2.19it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 578/782 [04:26<01:33,  2.19it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 579/782 [04:27<01:32,  2.19it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 580/782 [04:27<01:32,  2.19it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 581/782 [04:28<01:31,  2.19it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 582/782 [04:28<01:31,  2.19it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 583/782 [04:28<01:31,  2.18it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 584/782 [04:29<01:30,  2.18it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 585/782 [04:29<01:30,  2.18it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 586/782 [04:30<01:29,  2.18it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 587/782 [04:30<01:29,  2.19it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 588/782 [04:31<01:28,  2.19it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 589/782 [04:31<01:28,  2.19it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 590/782 [04:32<01:27,  2.19it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 591/782 [04:32<01:27,  2.19it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 592/782 [04:33<01:27,  2.18it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 593/782 [04:33<01:26,  2.18it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 594/782 [04:33<01:26,  2.18it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 595/782 [04:34<01:25,  2.18it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 596/782 [04:34<01:25,  2.18it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 597/782 [04:35<01:25,  2.18it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 598/782 [04:35<01:24,  2.17it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 599/782 [04:36<01:24,  2.17it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 600/782 [04:36<01:23,  2.18it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 601/782 [04:37<01:22,  2.18it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 602/782 [04:37<01:22,  2.18it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 603/782 [04:38<01:22,  2.18it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 604/782 [04:38<01:21,  2.18it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 605/782 [04:39<01:21,  2.18it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 606/782 [04:39<01:20,  2.18it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 607/782 [04:39<01:20,  2.18it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 608/782 [04:40<01:19,  2.18it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 609/782 [04:40<01:19,  2.18it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 610/782 [04:41<01:18,  2.18it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 611/782 [04:41<01:18,  2.18it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 612/782 [04:42<01:18,  2.18it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 613/782 [04:42<01:17,  2.17it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 614/782 [04:43<01:17,  2.18it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 615/782 [04:43<01:16,  2.18it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 616/782 [04:44<01:15,  2.19it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 617/782 [04:44<01:15,  2.18it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 618/782 [04:45<01:15,  2.18it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 619/782 [04:45<01:14,  2.17it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 620/782 [04:45<01:14,  2.18it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 621/782 [04:46<01:13,  2.18it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 622/782 [04:46<01:13,  2.18it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 623/782 [04:47<01:12,  2.18it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 624/782 [04:47<01:12,  2.19it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 625/782 [04:48<01:11,  2.19it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 626/782 [04:48<01:11,  2.19it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 627/782 [04:49<01:11,  2.18it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 628/782 [04:49<01:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 629/782 [04:50<01:10,  2.19it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 630/782 [04:50<01:09,  2.18it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 631/782 [04:50<01:09,  2.18it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 632/782 [04:51<01:08,  2.18it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 633/782 [04:51<01:08,  2.18it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 634/782 [04:52<01:07,  2.18it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 635/782 [04:52<01:07,  2.18it/s]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 636/782 [04:53<01:06,  2.18it/s]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 637/782 [04:53<01:06,  2.18it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 638/782 [04:54<01:05,  2.19it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 639/782 [04:54<01:05,  2.19it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 640/782 [04:55<01:04,  2.19it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 641/782 [04:55<01:04,  2.19it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 642/782 [04:55<01:03,  2.19it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 643/782 [04:56<01:03,  2.19it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 644/782 [04:56<01:03,  2.19it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 645/782 [04:57<01:02,  2.19it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 646/782 [04:57<01:02,  2.19it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 647/782 [04:58<01:01,  2.19it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 648/782 [04:58<01:01,  2.19it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 649/782 [04:59<01:00,  2.19it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 650/782 [04:59<01:00,  2.19it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 651/782 [05:00<00:59,  2.19it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 652/782 [05:00<00:59,  2.19it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 653/782 [05:01<00:58,  2.19it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 654/782 [05:01<00:58,  2.19it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 655/782 [05:01<00:57,  2.19it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 656/782 [05:02<00:57,  2.19it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 657/782 [05:02<00:57,  2.19it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 658/782 [05:03<00:56,  2.19it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 659/782 [05:03<00:56,  2.19it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 660/782 [05:04<00:55,  2.18it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 661/782 [05:04<00:55,  2.19it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 662/782 [05:05<00:54,  2.18it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 663/782 [05:05<00:54,  2.18it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 664/782 [05:06<00:53,  2.19it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 665/782 [05:06<00:53,  2.19it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 666/782 [05:06<00:53,  2.19it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 667/782 [05:07<00:52,  2.19it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 668/782 [05:07<00:52,  2.19it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 669/782 [05:08<00:51,  2.19it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 670/782 [05:08<00:51,  2.19it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 671/782 [05:09<00:50,  2.19it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 672/782 [05:09<00:50,  2.18it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 673/782 [05:10<00:49,  2.18it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 674/782 [05:10<00:49,  2.18it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 675/782 [05:11<00:49,  2.18it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 676/782 [05:11<00:48,  2.17it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 677/782 [05:12<00:48,  2.18it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 678/782 [05:12<00:47,  2.18it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 679/782 [05:12<00:47,  2.18it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 680/782 [05:13<00:46,  2.18it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 681/782 [05:13<00:46,  2.19it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 682/782 [05:14<00:45,  2.19it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 683/782 [05:14<00:45,  2.19it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 684/782 [05:15<00:44,  2.19it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 685/782 [05:15<00:44,  2.19it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 686/782 [05:16<00:43,  2.19it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 687/782 [05:16<00:43,  2.19it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 688/782 [05:17<00:43,  2.18it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 689/782 [05:17<00:42,  2.19it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 690/782 [05:17<00:41,  2.19it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 691/782 [05:18<00:41,  2.19it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 692/782 [05:18<00:41,  2.19it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 693/782 [05:19<00:40,  2.18it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 694/782 [05:19<00:40,  2.19it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 695/782 [05:20<00:39,  2.19it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 696/782 [05:20<00:39,  2.19it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 697/782 [05:21<00:38,  2.19it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 698/782 [05:21<00:38,  2.19it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 699/782 [05:22<00:37,  2.19it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 700/782 [05:22<00:37,  2.19it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 701/782 [05:22<00:37,  2.19it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 702/782 [05:23<00:36,  2.19it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 703/782 [05:23<00:36,  2.19it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 704/782 [05:24<00:35,  2.19it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 705/782 [05:24<00:35,  2.19it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 706/782 [05:25<00:34,  2.19it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 707/782 [05:25<00:34,  2.19it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 708/782 [05:26<00:33,  2.19it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 709/782 [05:26<00:33,  2.19it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 710/782 [05:27<00:32,  2.19it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 711/782 [05:27<00:32,  2.19it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 712/782 [05:28<00:31,  2.19it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 713/782 [05:28<00:31,  2.20it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 714/782 [05:28<00:31,  2.19it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 715/782 [05:29<00:30,  2.19it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 716/782 [05:29<00:30,  2.19it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 717/782 [05:30<00:29,  2.19it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 718/782 [05:30<00:29,  2.18it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 719/782 [05:31<00:28,  2.18it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 720/782 [05:31<00:28,  2.19it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 721/782 [05:32<00:27,  2.19it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 722/782 [05:32<00:27,  2.19it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 723/782 [05:33<00:26,  2.19it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 724/782 [05:33<00:26,  2.18it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 725/782 [05:33<00:26,  2.18it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 726/782 [05:34<00:25,  2.18it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 727/782 [05:34<00:25,  2.18it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 728/782 [05:35<00:24,  2.18it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 729/782 [05:35<00:24,  2.18it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 730/782 [05:36<00:23,  2.18it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 731/782 [05:36<00:23,  2.18it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 732/782 [05:37<00:22,  2.18it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 733/782 [05:37<00:22,  2.19it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 734/782 [05:38<00:21,  2.19it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 735/782 [05:38<00:21,  2.18it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 736/782 [05:38<00:21,  2.18it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 737/782 [05:39<00:20,  2.19it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 738/782 [05:39<00:20,  2.18it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 739/782 [05:40<00:19,  2.18it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 740/782 [05:40<00:19,  2.18it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 741/782 [05:41<00:18,  2.18it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 742/782 [05:41<00:18,  2.18it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 743/782 [05:42<00:17,  2.17it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 744/782 [05:42<00:17,  2.18it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 745/782 [05:43<00:16,  2.18it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 746/782 [05:43<00:16,  2.18it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 747/782 [05:44<00:16,  2.18it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 748/782 [05:44<00:15,  2.18it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 749/782 [05:44<00:15,  2.18it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 750/782 [05:45<00:14,  2.19it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 751/782 [05:45<00:14,  2.19it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 752/782 [05:46<00:13,  2.18it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 753/782 [05:46<00:13,  2.18it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 754/782 [05:47<00:12,  2.18it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 755/782 [05:47<00:12,  2.18it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 756/782 [05:48<00:11,  2.19it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 757/782 [05:48<00:11,  2.18it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 758/782 [05:49<00:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 759/782 [05:49<00:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 760/782 [05:49<00:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 761/782 [05:50<00:09,  2.18it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 762/782 [05:50<00:09,  2.18it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 763/782 [05:51<00:08,  2.18it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 764/782 [05:51<00:08,  2.18it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 765/782 [05:52<00:07,  2.18it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 766/782 [05:52<00:07,  2.19it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 767/782 [05:53<00:06,  2.19it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 768/782 [05:53<00:06,  2.19it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 769/782 [05:54<00:05,  2.19it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 770/782 [05:54<00:05,  2.19it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 771/782 [05:55<00:05,  2.18it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 772/782 [05:55<00:04,  2.18it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 773/782 [05:55<00:04,  2.18it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 774/782 [05:56<00:03,  2.18it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 775/782 [05:56<00:03,  2.18it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 776/782 [05:57<00:02,  2.18it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 777/782 [05:57<00:02,  2.19it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 778/782 [05:58<00:01,  2.19it/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 779/782 [05:58<00:01,  2.19it/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 780/782 [05:59<00:00,  2.19it/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 781/782 [05:59<00:00,  2.19it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 782/782 [05:59<00:00,  2.77it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 10000\n",
      "  Batch size = 64\u001b[0m\n",
      "\u001b[34mNum examples = 10000\n",
      "  Batch size = 64\u001b[0m\n",
      "\u001b[34m0%|          | 0/157 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m1%|▏         | 2/157 [00:00<00:23,  6.50it/s]#033[A\u001b[0m\n",
      "\u001b[34m2%|▏         | 3/157 [00:00<00:33,  4.56it/s]#033[A\u001b[0m\n",
      "\u001b[34m3%|▎         | 4/157 [00:00<00:38,  3.97it/s]#033[A\u001b[0m\n",
      "\u001b[34m3%|▎         | 5/157 [00:01<00:41,  3.67it/s]#033[A\u001b[0m\n",
      "\u001b[34m4%|▍         | 6/157 [00:01<00:42,  3.52it/s]#033[A\u001b[0m\n",
      "\u001b[34m4%|▍         | 7/157 [00:01<00:43,  3.42it/s]#033[A\u001b[0m\n",
      "\u001b[34m5%|▌         | 8/157 [00:02<00:44,  3.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m6%|▌         | 9/157 [00:02<00:44,  3.34it/s]#033[A\u001b[0m\n",
      "\u001b[34m6%|▋         | 10/157 [00:02<00:44,  3.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 11/157 [00:03<00:44,  3.30it/s]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 12/157 [00:03<00:44,  3.29it/s]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 13/157 [00:03<00:43,  3.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m9%|▉         | 14/157 [00:03<00:43,  3.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m10%|▉         | 15/157 [00:04<00:43,  3.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m10%|█         | 16/157 [00:04<00:43,  3.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m11%|█         | 17/157 [00:04<00:42,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m11%|█▏        | 18/157 [00:05<00:42,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 19/157 [00:05<00:42,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m13%|█▎        | 20/157 [00:05<00:42,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m13%|█▎        | 21/157 [00:06<00:41,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m14%|█▍        | 22/157 [00:06<00:41,  3.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▍        | 23/157 [00:06<00:41,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▌        | 24/157 [00:07<00:41,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m16%|█▌        | 25/157 [00:07<00:40,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 26/157 [00:07<00:40,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 27/157 [00:07<00:40,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 28/157 [00:08<00:39,  3.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 29/157 [00:08<00:39,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m19%|█▉        | 30/157 [00:08<00:38,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m20%|█▉        | 31/157 [00:09<00:38,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m20%|██        | 32/157 [00:09<00:38,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m21%|██        | 33/157 [00:09<00:37,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 34/157 [00:10<00:37,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 35/157 [00:10<00:37,  3.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 36/157 [00:10<00:37,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m24%|██▎       | 37/157 [00:11<00:36,  3.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m24%|██▍       | 38/157 [00:11<00:36,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▍       | 39/157 [00:11<00:36,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 40/157 [00:11<00:36,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m26%|██▌       | 41/157 [00:12<00:35,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 42/157 [00:12<00:35,  3.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 43/157 [00:12<00:35,  3.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 44/157 [00:13<00:34,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▊       | 45/157 [00:13<00:34,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 46/157 [00:13<00:34,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m30%|██▉       | 47/157 [00:14<00:33,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m31%|███       | 48/157 [00:14<00:33,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m31%|███       | 49/157 [00:14<00:33,  3.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 50/157 [00:15<00:33,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 51/157 [00:15<00:32,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m33%|███▎      | 52/157 [00:15<00:32,  3.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m34%|███▍      | 53/157 [00:15<00:32,  3.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m34%|███▍      | 54/157 [00:16<00:31,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▌      | 55/157 [00:16<00:31,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m36%|███▌      | 56/157 [00:16<00:31,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m36%|███▋      | 57/157 [00:17<00:31,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 58/157 [00:17<00:30,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 59/157 [00:17<00:30,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 60/157 [00:18<00:30,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m39%|███▉      | 61/157 [00:18<00:29,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m39%|███▉      | 62/157 [00:18<00:29,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 63/157 [00:19<00:29,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m41%|████      | 64/157 [00:19<00:28,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m41%|████▏     | 65/157 [00:19<00:28,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▏     | 66/157 [00:20<00:28,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 67/157 [00:20<00:27,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 68/157 [00:20<00:27,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 69/157 [00:20<00:27,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m45%|████▍     | 70/157 [00:21<00:26,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m45%|████▌     | 71/157 [00:21<00:26,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▌     | 72/157 [00:21<00:26,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▋     | 73/157 [00:22<00:25,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 74/157 [00:22<00:25,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 75/157 [00:22<00:25,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 76/157 [00:23<00:25,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m49%|████▉     | 77/157 [00:23<00:24,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|████▉     | 78/157 [00:23<00:24,  3.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 79/157 [00:24<00:24,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████     | 80/157 [00:24<00:23,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 81/157 [00:24<00:23,  3.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 82/157 [00:24<00:23,  3.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 83/157 [00:25<00:22,  3.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 84/157 [00:25<00:22,  3.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 85/157 [00:25<00:22,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 86/157 [00:26<00:21,  3.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 87/157 [00:26<00:21,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 88/157 [00:26<00:21,  3.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 89/157 [00:27<00:20,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 90/157 [00:27<00:20,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 91/157 [00:27<00:20,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 92/157 [00:28<00:19,  3.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 93/157 [00:28<00:19,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 94/157 [00:28<00:19,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████    | 95/157 [00:28<00:19,  3.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████    | 96/157 [00:29<00:18,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 97/157 [00:29<00:18,  3.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 98/157 [00:29<00:18,  3.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 99/157 [00:30<00:17,  3.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 100/157 [00:30<00:17,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 101/157 [00:30<00:17,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 102/157 [00:31<00:16,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 103/157 [00:31<00:16,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 104/157 [00:31<00:16,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 105/157 [00:32<00:15,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 106/157 [00:32<00:15,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 107/157 [00:32<00:15,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 108/157 [00:32<00:15,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 109/157 [00:33<00:14,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m70%|███████   | 110/157 [00:33<00:14,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 111/157 [00:33<00:14,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 112/157 [00:34<00:13,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 113/157 [00:34<00:13,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 114/157 [00:34<00:13,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 115/157 [00:35<00:12,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 116/157 [00:35<00:12,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 117/157 [00:35<00:12,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 118/157 [00:36<00:11,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 119/157 [00:36<00:11,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 120/157 [00:36<00:11,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 121/157 [00:36<00:11,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 122/157 [00:37<00:10,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 123/157 [00:37<00:10,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 124/157 [00:37<00:10,  3.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 125/157 [00:38<00:09,  3.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 126/157 [00:38<00:09,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m81%|████████  | 127/157 [00:38<00:09,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 128/157 [00:39<00:08,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 129/157 [00:39<00:08,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 130/157 [00:39<00:08,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 131/157 [00:39<00:07,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 132/157 [00:40<00:07,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 133/157 [00:40<00:07,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 134/157 [00:40<00:07,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 135/157 [00:41<00:06,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 136/157 [00:41<00:06,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 137/157 [00:41<00:06,  3.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 138/157 [00:42<00:05,  3.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 139/157 [00:42<00:05,  3.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 140/157 [00:42<00:05,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 141/157 [00:43<00:04,  3.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m90%|█████████ | 142/157 [00:43<00:04,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m91%|█████████ | 143/157 [00:43<00:04,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 144/157 [00:43<00:03,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 145/157 [00:44<00:03,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 146/157 [00:44<00:03,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 147/157 [00:44<00:03,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 148/157 [00:45<00:02,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 149/157 [00:45<00:02,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 150/157 [00:45<00:02,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 151/157 [00:46<00:01,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 152/157 [00:46<00:01,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 153/157 [00:46<00:01,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 154/157 [00:47<00:00,  3.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 155/157 [00:47<00:00,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 156/157 [00:47<00:00,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.18693424761295319, 'eval_accuracy': 0.9286, 'eval_f1': 0.9276302452868436, 'eval_precision': 0.9340681771790161, 'eval_recall': 0.9212804509764445, 'eval_runtime': 48.0411, 'eval_samples_per_second': 208.155, 'eval_steps_per_second': 3.268, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 782/782 [06:47<00:00,  2.77it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 157/157 [00:47<00:00,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015                                                 #033[A\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m100%|██████████| 782/782 [06:47<00:00,  2.77it/s]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 407.7786, 'train_samples_per_second': 61.308, 'train_steps_per_second': 1.918, 'train_loss': 0.3068606286402554, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 782/782 [06:47<00:00,  1.92it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 10000\u001b[0m\n",
      "\u001b[34mNum examples = 10000\n",
      "  Batch size = 64\u001b[0m\n",
      "\u001b[34mBatch size = 64\u001b[0m\n",
      "\u001b[34m0%|          | 0/157 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 2/157 [00:00<00:23,  6.57it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 3/157 [00:00<00:33,  4.63it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 4/157 [00:00<00:38,  4.02it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 5/157 [00:01<00:40,  3.73it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 6/157 [00:01<00:42,  3.57it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 7/157 [00:01<00:43,  3.47it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 8/157 [00:02<00:43,  3.40it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 9/157 [00:02<00:44,  3.36it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 10/157 [00:02<00:44,  3.33it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 11/157 [00:03<00:44,  3.31it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 12/157 [00:03<00:43,  3.30it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 13/157 [00:03<00:43,  3.28it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 14/157 [00:03<00:43,  3.28it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 15/157 [00:04<00:43,  3.28it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 16/157 [00:04<00:42,  3.28it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 17/157 [00:04<00:42,  3.28it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 18/157 [00:05<00:42,  3.28it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 19/157 [00:05<00:42,  3.28it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 20/157 [00:05<00:41,  3.27it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 21/157 [00:06<00:41,  3.26it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 22/157 [00:06<00:41,  3.27it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 23/157 [00:06<00:40,  3.27it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 24/157 [00:07<00:40,  3.27it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 25/157 [00:07<00:40,  3.27it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 26/157 [00:07<00:40,  3.26it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 27/157 [00:07<00:39,  3.27it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 28/157 [00:08<00:39,  3.27it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 29/157 [00:08<00:39,  3.28it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 30/157 [00:08<00:38,  3.27it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 31/157 [00:09<00:38,  3.27it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 32/157 [00:09<00:38,  3.27it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 33/157 [00:09<00:37,  3.27it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 34/157 [00:10<00:37,  3.27it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 35/157 [00:10<00:37,  3.27it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 36/157 [00:10<00:36,  3.27it/s]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 37/157 [00:11<00:36,  3.27it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 38/157 [00:11<00:36,  3.27it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 39/157 [00:11<00:36,  3.28it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 40/157 [00:11<00:35,  3.28it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 41/157 [00:12<00:35,  3.27it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 42/157 [00:12<00:35,  3.28it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 43/157 [00:12<00:34,  3.27it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 44/157 [00:13<00:34,  3.28it/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 45/157 [00:13<00:34,  3.27it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 46/157 [00:13<00:33,  3.27it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 47/157 [00:14<00:33,  3.28it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 48/157 [00:14<00:33,  3.28it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 49/157 [00:14<00:32,  3.28it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 50/157 [00:14<00:32,  3.28it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 51/157 [00:15<00:32,  3.27it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 52/157 [00:15<00:32,  3.27it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 53/157 [00:15<00:31,  3.26it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 54/157 [00:16<00:31,  3.27it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 55/157 [00:16<00:31,  3.27it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 56/157 [00:16<00:30,  3.27it/s]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 57/157 [00:17<00:30,  3.27it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 58/157 [00:17<00:30,  3.27it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 59/157 [00:17<00:29,  3.27it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 60/157 [00:18<00:29,  3.27it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 61/157 [00:18<00:29,  3.28it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 62/157 [00:18<00:29,  3.27it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 63/157 [00:18<00:28,  3.26it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 64/157 [00:19<00:28,  3.27it/s]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 65/157 [00:19<00:28,  3.27it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 66/157 [00:19<00:27,  3.27it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 67/157 [00:20<00:27,  3.26it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 68/157 [00:20<00:27,  3.27it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 69/157 [00:20<00:26,  3.26it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 70/157 [00:21<00:26,  3.27it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 71/157 [00:21<00:26,  3.27it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 72/157 [00:21<00:25,  3.27it/s]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 73/157 [00:22<00:25,  3.27it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 74/157 [00:22<00:25,  3.28it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 75/157 [00:22<00:25,  3.28it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 76/157 [00:22<00:24,  3.28it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 77/157 [00:23<00:24,  3.27it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 78/157 [00:23<00:24,  3.26it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 79/157 [00:23<00:23,  3.27it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 80/157 [00:24<00:23,  3.26it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 81/157 [00:24<00:23,  3.26it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 82/157 [00:24<00:22,  3.27it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 83/157 [00:25<00:22,  3.27it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 84/157 [00:25<00:22,  3.27it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 85/157 [00:25<00:22,  3.26it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 86/157 [00:25<00:21,  3.26it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 87/157 [00:26<00:21,  3.27it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 88/157 [00:26<00:21,  3.27it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 89/157 [00:26<00:20,  3.26it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 90/157 [00:27<00:20,  3.27it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 91/157 [00:27<00:20,  3.27it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 92/157 [00:27<00:19,  3.27it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 93/157 [00:28<00:19,  3.27it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 94/157 [00:28<00:19,  3.25it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 95/157 [00:28<00:19,  3.25it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 96/157 [00:29<00:18,  3.24it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 97/157 [00:29<00:18,  3.25it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 98/157 [00:29<00:18,  3.26it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 99/157 [00:29<00:17,  3.26it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 100/157 [00:30<00:17,  3.26it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 101/157 [00:30<00:17,  3.27it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 102/157 [00:30<00:16,  3.27it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 103/157 [00:31<00:16,  3.27it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 104/157 [00:31<00:16,  3.26it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 105/157 [00:31<00:15,  3.26it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 106/157 [00:32<00:15,  3.26it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 107/157 [00:32<00:15,  3.27it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 108/157 [00:32<00:14,  3.27it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 109/157 [00:33<00:14,  3.27it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 110/157 [00:33<00:14,  3.27it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 111/157 [00:33<00:14,  3.27it/s]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 112/157 [00:33<00:13,  3.26it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 113/157 [00:34<00:13,  3.27it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 114/157 [00:34<00:13,  3.28it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 115/157 [00:34<00:12,  3.27it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 116/157 [00:35<00:12,  3.27it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 117/157 [00:35<00:12,  3.27it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 118/157 [00:35<00:11,  3.27it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 119/157 [00:36<00:11,  3.27it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 120/157 [00:36<00:11,  3.28it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 121/157 [00:36<00:10,  3.28it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 122/157 [00:37<00:10,  3.28it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 123/157 [00:37<00:10,  3.28it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 124/157 [00:37<00:10,  3.27it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 125/157 [00:37<00:09,  3.27it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 126/157 [00:38<00:09,  3.27it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 127/157 [00:38<00:09,  3.26it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 128/157 [00:38<00:08,  3.26it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 129/157 [00:39<00:08,  3.26it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 130/157 [00:39<00:08,  3.27it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 131/157 [00:39<00:07,  3.27it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 132/157 [00:40<00:07,  3.27it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 133/157 [00:40<00:07,  3.27it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 134/157 [00:40<00:07,  3.27it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 135/157 [00:40<00:06,  3.27it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 136/157 [00:41<00:06,  3.26it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 137/157 [00:41<00:06,  3.26it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 138/157 [00:41<00:05,  3.26it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 139/157 [00:42<00:05,  3.27it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 140/157 [00:42<00:05,  3.27it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 141/157 [00:42<00:04,  3.26it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 142/157 [00:43<00:04,  3.26it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 143/157 [00:43<00:04,  3.27it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 144/157 [00:43<00:03,  3.25it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 145/157 [00:44<00:03,  3.26it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 146/157 [00:44<00:03,  3.27it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 147/157 [00:44<00:03,  3.27it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 148/157 [00:44<00:02,  3.28it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 149/157 [00:45<00:02,  3.28it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 150/157 [00:45<00:02,  3.28it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 151/157 [00:45<00:01,  3.28it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 152/157 [00:46<00:01,  3.27it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 153/157 [00:46<00:01,  3.27it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 154/157 [00:46<00:00,  3.27it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 155/157 [00:47<00:00,  3.28it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 156/157 [00:47<00:00,  3.28it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 157/157 [00:47<00:00,  3.31it/s]\u001b[0m\n",
      "\u001b[34m***** Eval results *****\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m2024-01-22 05:49:23,458 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-01-22 05:49:23,459 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-01-22 05:49:23,459 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-01-22 05:49:30 Uploading - Uploading generated training model\n",
      "2024-01-22 05:49:51 Completed - Training job completed\n",
      "Training seconds: 798\n",
      "Billable seconds: 798\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1,\n",
    "                 'train_batch_size': 32,\n",
    "                 'model_name':'distilbert-base-uncased'\n",
    "                 }\n",
    "\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            transformers_version='4.26',\n",
    "                            pytorch_version='1.13',\n",
    "                            py_version='py39',\n",
    "                            hyperparameters = hyperparameters)\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff99d5b-5c62-4c92-9dda-1cd88af53eb5",
   "metadata": {},
   "source": [
    "## Deploying the endpoint\n",
    "\n",
    "To deploy our endpoint, we call deploy() on our HuggingFace estimator object, passing in our desired number of instances and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60401041-176e-45b8-9b3a-5e8b0d84f97e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-training-2024-01-22-06-04-25-481\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-pytorch-training-2024-01-22-06-04-25-481\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-pytorch-training-2024-01-22-06-04-25-481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.7979011535644531}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor = huggingface_estimator.deploy(1, \"ml.g4dn.xlarge\")\n",
    "\n",
    "sentiment_input= {\"inputs\":\"I am not sure whether I will use the new Inference DLC.\"}\n",
    "predictor.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b745c06b-83a1-4445-91ae-066c5e92e6d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9020131826400757}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_input= {\"inputs\":\"I love using the new Inference DLC.\"}\n",
    "predictor.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7c6b7fe-dd23-4d67-a8b4-bb9f58425f27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.7141469120979309}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_input= {\"inputs\":\"Caring is most important\"}\n",
    "predictor.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8cb4ab6e-35ad-408d-8856-ea53126df3a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: huggingface-pytorch-training-2024-01-22-06-04-25-481\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: huggingface-pytorch-training-2024-01-22-06-04-25-481\n",
      "INFO:sagemaker:Deleting endpoint with name: huggingface-pytorch-training-2024-01-22-06-04-25-481\n"
     ]
    }
   ],
   "source": [
    "# Delete the endpoint\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7689e6-84b8-42fb-ba61-84fc3cbacd44",
   "metadata": {},
   "source": [
    "## Estimator Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39e223d2-1aa9-419f-9404-204697e6fa63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "container image used for training job: \n",
      "None\n",
      "\n",
      "s3 uri where the trained model is located: \n",
      "s3://sagemaker-us-east-1-802575742115/huggingface-pytorch-training-2024-01-22-05-16-45-400/output/model.tar.gz\n",
      "\n",
      "latest training job name for this estimator: \n",
      "huggingface-pytorch-training-2024-01-22-05-16-45-400\n",
      "\n",
      "2024-01-22 05:22:39 Starting - Starting the training job\n",
      "2024-01-22 05:22:39 Pending - Preparing the instances for training\n",
      "2024-01-22 05:22:39 Downloading - Downloading the training image\n",
      "2024-01-22 05:22:39 Training - Training image download completed. Training in progress.\n",
      "2024-01-22 05:22:39 Uploading - Uploading generated training model\n",
      "2024-01-22 05:22:39 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-01-22 05:22:24,724 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-01-22 05:22:24,744 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-22 05:22:24,756 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-01-22 05:22:24,759 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-01-22 05:22:25,049 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-22 05:22:25,083 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-22 05:22:25,117 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-22 05:22:25,130 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"train_batch_size\": 32\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2024-01-22-05-16-45-400\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-802575742115/huggingface-pytorch-training-2024-01-22-05-16-45-400/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-802575742115/huggingface-pytorch-training-2024-01-22-05-16-45-400/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2024-01-22-05-16-45-400\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-802575742115/huggingface-pytorch-training-2024-01-22-05-16-45-400/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"32\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\u001b[0m\n",
      "\u001b[34m[2024-01-22 05:22:27.236: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2024-01-22 05:22:27,244 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2024-01-22 05:22:27,279 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m2024-01-22 05:22:27,322 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-01-22 05:22:27,322 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-01-22 05:22:27,322 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# container image used for training job\n",
    "print(f\"container image used for training job: \\n{huggingface_estimator.image_uri}\\n\")\n",
    "\n",
    "# s3 uri where the trained model is located\n",
    "print(f\"s3 uri where the trained model is located: \\n{huggingface_estimator.model_data}\\n\")\n",
    "\n",
    "# latest training job name for this estimator\n",
    "print(f\"latest training job name for this estimator: \\n{huggingface_estimator.latest_training_job.name}\\n\")\n",
    "\n",
    "# access the logs of the training job\n",
    "huggingface_estimator.sagemaker_session.logs_for_job(huggingface_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f6617a-bb40-4732-8780-3b3bd80dc326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfe86fd-6b60-4b65-853d-e7a141e463fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87011e27-a0fc-4d34-9e2f-541053637588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
