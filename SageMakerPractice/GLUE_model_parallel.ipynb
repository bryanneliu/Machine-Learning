{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e57f4e-f437-40a5-8ffe-e0d8e0e7be99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model Parallelism\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-intro.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f755c15d-3ef6-4419-9245-b9ea0a971fcc",
   "metadata": {},
   "source": [
    "!pip install \"sagemaker>=2.48.0\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2270339a-6825-424e-96f4-6e261962bf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de391323-7948-4297-9d54-cc149913eec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::802575742115:role/service-role/AmazonSageMaker-ExecutionRole-20230929T143152\n",
      "sagemaker bucket: sagemaker-us-east-1-802575742115\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54b3e27b-c34e-4113-b250-480c0a1695d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.sagemaker import SageMakerTrainingArguments as TrainingArguments, SageMakerTrainer as Trainer\n",
    "from sagemaker.huggingface import HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38b07074-98aa-40b1-a84b-739090a5a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'model_name_or_path':'roberta-large',\n",
    "    'task_name': 'mnli',\n",
    "    'per_device_train_batch_size': 16,\n",
    "    'per_device_eval_batch_size': 16,\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'do_predict': True,\n",
    "    'num_train_epochs': 2,\n",
    "    'output_dir':'/opt/ml/model',\n",
    "    'max_steps': 500,\n",
    "}\n",
    "\n",
    "# git configuration to download our fine-tuning script\n",
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.28.1'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2299993-50c5-40de-962b-b1e0ad8e2181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration for running training on smdistributed Model Parallel\n",
    "mpi_options = {\n",
    "    \"enabled\" : True,\n",
    "    \"processes_per_host\" : 8,\n",
    "}\n",
    "smp_options = {\n",
    "    \"enabled\":True,\n",
    "    \"parameters\": {\n",
    "        \"microbatches\": 4,\n",
    "        \"placement_strategy\": \"spread\",\n",
    "        \"pipeline\": \"interleaved\",\n",
    "        \"optimize\": \"speed\",\n",
    "        \"partitions\": 4,\n",
    "        \"ddp\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "distribution={\n",
    "    \"smdistributed\": {\"modelparallel\": smp_options},\n",
    "    \"mpi\": mpi_options\n",
    "}\n",
    "\n",
    "# instance configurations\n",
    "instance_type='ml.p3.16xlarge'\n",
    "instance_count=1\n",
    "volume_size=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c5a7839-c6a3-4479-8d3d-50136546001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric definition to extract the results\n",
    "metric_definitions=[\n",
    "     {'Name': 'train_runtime', 'Regex':\"train_runtime.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'train_samples_per_second', 'Regex': \"train_samples_per_second.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'epoch', 'Regex': \"epoch.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'f1', 'Regex': \"f1.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'exact_match', 'Regex': \"exact_match.*=\\D*(.*?)$\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cedbae73-c1b5-472b-9bb3-6bc645959609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator\n",
    "huggingface_estimator = HuggingFace(entry_point='run_glue.py',\n",
    "                                    source_dir='./examples/pytorch/text-classification',\n",
    "                                    git_config=git_config,\n",
    "                                    metrics_definition=metric_definitions,\n",
    "                                    instance_type=instance_type,\n",
    "                                    instance_count=instance_count,\n",
    "                                    volume_size=volume_size,\n",
    "                                    role=role,\n",
    "                                    transformers_version='4.28.1',\n",
    "                                    pytorch_version='2.0.0',\n",
    "                                    py_version='py310',\n",
    "                                    distribution= distribution,\n",
    "                                    hyperparameters = hyperparameters,\n",
    "                                    debugger_hook_config=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6d0b472-0578-4c66-9bec-c4b0efac6d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name_or_path': '\"roberta-large\"',\n",
       " 'task_name': '\"mnli\"',\n",
       " 'per_device_train_batch_size': '16',\n",
       " 'per_device_eval_batch_size': '16',\n",
       " 'do_train': 'true',\n",
       " 'do_eval': 'true',\n",
       " 'do_predict': 'true',\n",
       " 'num_train_epochs': '2',\n",
       " 'output_dir': '\"/opt/ml/model\"',\n",
       " 'max_steps': '500',\n",
       " 'sagemaker_mpi_enabled': 'true',\n",
       " 'sagemaker_mpi_num_of_processes_per_host': '8',\n",
       " 'sagemaker_mpi_custom_mpi_options': '\"\"',\n",
       " 'mp_parameters': '{\"microbatches\": 4, \"placement_strategy\": \"spread\", \"pipeline\": \"interleaved\", \"optimize\": \"speed\", \"partitions\": 4, \"ddp\": true}',\n",
       " 'sagemaker_distributed_dataparallel_enabled': 'false',\n",
       " 'sagemaker_instance_type': '\"ml.p3.16xlarge\"'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_estimator.hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81e8b2f7-a18a-49a3-8804-fe1e8defc58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "Cloning into '/tmp/tmpp8jnbk_g'...\n",
      "Note: switching to 'v4.28.1'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n",
      "HEAD is now at 04ab5605f Patch release: v4.28.1\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2024-01-23-06-06-54-216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-23 06:07:18 Starting - Starting the training job...\n",
      "2024-01-23 06:07:28 Pending - Training job waiting for capacity......\n",
      "2024-01-23 06:08:31 Pending - Preparing the instances for training......\n",
      "2024-01-23 06:09:41 Downloading - Downloading input data...\n",
      "2024-01-23 06:10:05 Downloading - Downloading the training image....................................\n",
      "2024-01-23 06:16:02 Training - Training image download completed. Training in progress....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-01-23 06:16:40,456 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-01-23 06:16:40,516 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-23 06:16:40,526 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-01-23 06:16:40,529 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-01-23 06:16:42,282 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate>=0.12.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (0.19.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.1.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (1.10.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (1.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (3.20.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.3->-r requirements.txt (line 7)) (3.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.3->-r requirements.txt (line 7)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.3->-r requirements.txt (line 7)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.3->-r requirements.txt (line 7)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.3->-r requirements.txt (line 7)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.3->-r requirements.txt (line 7)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.3->-r requirements.txt (line 7)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 23.3.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-01-23 06:16:45,752 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-01-23 06:16:45,752 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-01-23 06:16:45,814 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-23 06:16:45,885 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-23 06:16:45,896 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2024-01-23 06:16:45,896 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2024-01-23 06:16:45,896 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2024-01-23 06:16:45,896 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1'] Hosts: ['algo-1:8'] process_per_hosts: 8 num_processes: 8\u001b[0m\n",
      "\u001b[34m2024-01-23 06:16:45,897 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2024-01-23 06:16:45,957 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-23 06:16:45,969 sagemaker-training-toolkit INFO     PyTorch version is 2.0.0\u001b[0m\n",
      "\u001b[34m2024-01-23 06:16:46,029 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-23 06:16:46,040 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": false,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_mpi_enabled\": true,\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 8\n",
      "    },\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"do_eval\": true,\n",
      "        \"do_predict\": true,\n",
      "        \"do_train\": true,\n",
      "        \"max_steps\": 500,\n",
      "        \"model_name_or_path\": \"roberta-large\",\n",
      "        \"mp_parameters\": {\n",
      "            \"microbatches\": 4,\n",
      "            \"placement_strategy\": \"spread\",\n",
      "            \"pipeline\": \"interleaved\",\n",
      "            \"optimize\": \"speed\",\n",
      "            \"partitions\": 4,\n",
      "            \"ddp\": true\n",
      "        },\n",
      "        \"num_train_epochs\": 2,\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"per_device_eval_batch_size\": 16,\n",
      "        \"per_device_train_batch_size\": 16,\n",
      "        \"task_name\": \"mnli\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.16xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": true,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2024-01-23-06-06-54-216\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-802575742115/huggingface-pytorch-training-2024-01-23-06-06-54-216/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_glue\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.16xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_glue.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"do_eval\":true,\"do_predict\":true,\"do_train\":true,\"max_steps\":500,\"model_name_or_path\":\"roberta-large\",\"mp_parameters\":{\"ddp\":true,\"microbatches\":4,\"optimize\":\"speed\",\"partitions\":4,\"pipeline\":\"interleaved\",\"placement_strategy\":\"spread\"},\"num_train_epochs\":2,\"output_dir\":\"/opt/ml/model\",\"per_device_eval_batch_size\":16,\"per_device_train_batch_size\":16,\"task_name\":\"mnli\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_glue.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.16xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_glue\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-802575742115/huggingface-pytorch-training-2024-01-23-06-06-54-216/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.16xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.16xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"do_eval\":true,\"do_predict\":true,\"do_train\":true,\"max_steps\":500,\"model_name_or_path\":\"roberta-large\",\"mp_parameters\":{\"ddp\":true,\"microbatches\":4,\"optimize\":\"speed\",\"partitions\":4,\"pipeline\":\"interleaved\",\"placement_strategy\":\"spread\"},\"num_train_epochs\":2,\"output_dir\":\"/opt/ml/model\",\"per_device_eval_batch_size\":16,\"per_device_train_batch_size\":16,\"task_name\":\"mnli\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2024-01-23-06-06-54-216\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-802575742115/huggingface-pytorch-training-2024-01-23-06-06-54-216/source/sourcedir.tar.gz\",\"module_name\":\"run_glue\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_glue.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--do_eval\",\"True\",\"--do_predict\",\"True\",\"--do_train\",\"True\",\"--max_steps\",\"500\",\"--model_name_or_path\",\"roberta-large\",\"--mp_parameters\",\"ddp=True,microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread\",\"--num_train_epochs\",\"2\",\"--output_dir\",\"/opt/ml/model\",\"--per_device_eval_batch_size\",\"16\",\"--per_device_train_batch_size\",\"16\",\"--task_name\",\"mnli\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=true\u001b[0m\n",
      "\u001b[34mSM_HP_DO_PREDICT=true\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=500\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=roberta-large\u001b[0m\n",
      "\u001b[34mSM_HP_MP_PARAMETERS={\"ddp\":true,\"microbatches\":4,\"optimize\":\"speed\",\"partitions\":4,\"pipeline\":\"interleaved\",\"placement_strategy\":\"spread\"}\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_TASK_NAME=mnli\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8 -np 8 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.10/site-packages/gethostname.cpython-310-x86_64-linux-gnu.so -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_CURRENT_INSTANCE_TYPE -x SM_CURRENT_INSTANCE_GROUP -x SM_CURRENT_INSTANCE_GROUP_HOSTS -x SM_INSTANCE_GROUPS -x SM_INSTANCE_GROUPS_DICT -x SM_DISTRIBUTION_INSTANCE_GROUPS -x SM_IS_HETERO -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_NUM_NEURONS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_HP_DO_EVAL -x SM_HP_DO_PREDICT -x SM_HP_DO_TRAIN -x SM_HP_MAX_STEPS -x SM_HP_MODEL_NAME_OR_PATH -x SM_HP_MP_PARAMETERS -x SM_HP_NUM_TRAIN_EPOCHS -x SM_HP_OUTPUT_DIR -x SM_HP_PER_DEVICE_EVAL_BATCH_SIZE -x SM_HP_PER_DEVICE_TRAIN_BATCH_SIZE -x SM_HP_TASK_NAME -x PYTHONPATH -x NCCL_PROTO=simple -x NCCL_ALGO=ring smddpmprun -i ml.p3.16xlarge --allow-bypass /opt/conda/bin/python3.10 -m mpi4py run_glue.py --do_eval True --do_predict True --do_train True --max_steps 500 --model_name_or_path roberta-large --mp_parameters ddp=True,microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread --num_train_epochs 2 --output_dir /opt/ml/model --per_device_eval_batch_size 16 --per_device_train_batch_size 16 --task_name mnli\u001b[0m\n",
      "\u001b[34m2024-01-23 06:16:46,100 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m[2024-01-23 06:16:47.448: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2024-01-23 06:16:47,467 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mData for JOB [41169,1] offset 0 Total slots allocated 8\n",
      " ========================   JOB MAP   ========================\n",
      " Data for node: algo-1#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [41169,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [41169,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [41169,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [41169,1] App: 0 Process rank: 3 Bound: N/A\n",
      " #011Process OMPI jobid: [41169,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [41169,1] App: 0 Process rank: 5 Bound: N/A\n",
      " #011Process OMPI jobid: [41169,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [41169,1] App: 0 Process rank: 7 Bound: N/A\n",
      " =============================================================\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:SMDDPCollectivesInitWarning: The system is not compatible or not configured to run SMDDP collectives optimized for AWS infrastructure. The training job will fall back to NCCL.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Refer to the following information to troubleshoot the issue:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:#011The instance type is not supported for running the training job. Please use one of the following: ml.p4d.24xlarge or ml.p4de.24xlarge.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:SMDDPCollectivesInitWarning: The system is not compatible or not configured to run SMDDP collectives optimized for AWS infrastructure. The training job will fall back to NCCL.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Refer to the following information to troubleshoot the issue:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:#011The instance type is not supported for running the training job. Please use one of the following: ml.p4d.24xlarge or ml.p4de.24xlarge.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:SMDDPCollectivesInitWarning: The system is not compatible or not configured to run SMDDP collectives optimized for AWS infrastructure. The training job will fall back to NCCL.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Refer to the following information to troubleshoot the issue:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:#011The instance type is not supported for running the training job. Please use one of the following: ml.p4d.24xlarge or ml.p4de.24xlarge.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:SMDDPCollectivesInitWarning: The system is not compatible or not configured to run SMDDP collectives optimized for AWS infrastructure. The training job will fall back to NCCL.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Refer to the following information to troubleshoot the issue:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:#011The instance type is not supported for running the training job. Please use one of the following: ml.p4d.24xlarge or ml.p4de.24xlarge.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:SMDDPCollectivesInitWarning: The system is not compatible or not configured to run SMDDP collectives optimized for AWS infrastructure. The training job will fall back to NCCL.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Refer to the following information to troubleshoot the issue:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:#011The instance type is not supported for running the training job. Please use one of the following: ml.p4d.24xlarge or ml.p4de.24xlarge.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:SMDDPCollectivesInitWarning: The system is not compatible or not configured to run SMDDP collectives optimized for AWS infrastructure. The training job will fall back to NCCL.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Refer to the following information to troubleshoot the issue:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:#011The instance type is not supported for running the training job. Please use one of the following: ml.p4d.24xlarge or ml.p4de.24xlarge.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:SMDDPCollectivesInitWarning: The system is not compatible or not configured to run SMDDP collectives optimized for AWS infrastructure. The training job will fall back to NCCL.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Refer to the following information to troubleshoot the issue:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:#011The instance type is not supported for running the training job. Please use one of the following: ml.p4d.24xlarge or ml.p4de.24xlarge.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:SMDDPCollectivesInitWarning: The system is not compatible or not configured to run SMDDP collectives optimized for AWS infrastructure. The training job will fall back to NCCL.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Refer to the following information to troubleshoot the issue:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:#011The instance type is not supported for running the training job. Please use one of the following: ml.p4d.24xlarge or ml.p4de.24xlarge.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2024-01-23 06:16:51.637: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2024-01-23 06:16:51.640: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2024-01-23 06:16:51.641: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2024-01-23 06:16:51.666: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:51.741: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2024-01-23 06:16:51.741: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2024-01-23 06:16:51.741: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2024-01-23 06:16:51.817: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.257: W smdistributed/modelparallel/backend/core.py:423] smddp backend does not support training on single node. Falling back to nccl backend.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2024-01-23 06:16:55.258: W smdistributed/modelparallel/backend/core.py:423] smddp backend does not support training on single node. Falling back to nccl backend.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2024-01-23 06:16:55.258: W smdistributed/modelparallel/backend/core.py:423] smddp backend does not support training on single node. Falling back to nccl backend.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2024-01-23 06:16:55.259: W smdistributed/modelparallel/backend/core.py:423] smddp backend does not support training on single node. Falling back to nccl backend.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2024-01-23 06:16:55.259: W smdistributed/modelparallel/backend/core.py:423] smddp backend does not support training on single node. Falling back to nccl backend.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.259: I smdistributed/modelparallel/backend/core.py:481] [smddp] SMDATAPARALLEL_DEVICE_NAME is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.260: I smdistributed/modelparallel/backend/core.py:481] [smddp] SAGEMAKER_INSTANCE_TYPE is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.260: I smdistributed/modelparallel/torch/state_mod.py:100] [0] Initializing torch distributed process groups with nccl backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2024-01-23 06:16:55.260: I smdistributed/modelparallel/backend/core.py:481] [smddp] SMDATAPARALLEL_DEVICE_NAME is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2024-01-23 06:16:55.260: W smdistributed/modelparallel/backend/core.py:423] smddp backend does not support training on single node. Falling back to nccl backend.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2024-01-23 06:16:55.260: I smdistributed/modelparallel/backend/core.py:481] [smddp] SAGEMAKER_INSTANCE_TYPE is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2024-01-23 06:16:55.260: I smdistributed/modelparallel/torch/state_mod.py:100] [1] Initializing torch distributed process groups with nccl backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2024-01-23 06:16:55.260: W smdistributed/modelparallel/backend/core.py:423] smddp backend does not support training on single node. Falling back to nccl backend.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2024-01-23 06:16:55.260: W smdistributed/modelparallel/backend/core.py:423] smddp backend does not support training on single node. Falling back to nccl backend.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2024-01-23 06:16:55.260: I smdistributed/modelparallel/backend/core.py:481] [smddp] SMDATAPARALLEL_DEVICE_NAME is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2024-01-23 06:16:55.260: I smdistributed/modelparallel/backend/core.py:481] [smddp] SAGEMAKER_INSTANCE_TYPE is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2024-01-23 06:16:55.261: I smdistributed/modelparallel/torch/state_mod.py:100] [2] Initializing torch distributed process groups with nccl backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2024-01-23 06:16:55.261: I smdistributed/modelparallel/backend/core.py:481] [smddp] SMDATAPARALLEL_DEVICE_NAME is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2024-01-23 06:16:55.261: I smdistributed/modelparallel/backend/core.py:481] [smddp] SMDATAPARALLEL_DEVICE_NAME is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2024-01-23 06:16:55.261: I smdistributed/modelparallel/backend/core.py:481] [smddp] SAGEMAKER_INSTANCE_TYPE is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2024-01-23 06:16:55.261: I smdistributed/modelparallel/backend/core.py:481] [smddp] SAGEMAKER_INSTANCE_TYPE is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2024-01-23 06:16:55.261: I smdistributed/modelparallel/torch/state_mod.py:100] [3] Initializing torch distributed process groups with nccl backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2024-01-23 06:16:55.261: I smdistributed/modelparallel/torch/state_mod.py:100] [4] Initializing torch distributed process groups with nccl backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2024-01-23 06:16:55.261: I smdistributed/modelparallel/backend/core.py:481] [smddp] SMDATAPARALLEL_DEVICE_NAME is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2024-01-23 06:16:55.262: I smdistributed/modelparallel/backend/core.py:481] [smddp] SAGEMAKER_INSTANCE_TYPE is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2024-01-23 06:16:55.262: I smdistributed/modelparallel/torch/state_mod.py:100] [5] Initializing torch distributed process groups with nccl backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2024-01-23 06:16:55.262: I smdistributed/modelparallel/backend/core.py:481] [smddp] SMDATAPARALLEL_DEVICE_NAME is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2024-01-23 06:16:55.262: I smdistributed/modelparallel/backend/core.py:481] [smddp] SMDATAPARALLEL_DEVICE_NAME is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2024-01-23 06:16:55.262: I smdistributed/modelparallel/backend/core.py:481] [smddp] SAGEMAKER_INSTANCE_TYPE is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2024-01-23 06:16:55.262: I smdistributed/modelparallel/backend/core.py:481] [smddp] SAGEMAKER_INSTANCE_TYPE is defined in os.environ: False.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2024-01-23 06:16:55.262: I smdistributed/modelparallel/torch/state_mod.py:100] [7] Initializing torch distributed process groups with nccl backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2024-01-23 06:16:55.262: I smdistributed/modelparallel/torch/state_mod.py:100] [6] Initializing torch distributed process groups with nccl backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2024-01-23 06:16:55.454: I smdistributed/modelparallel/torch/state_mod.py:163] [2] Finished initializing torch distributed process groups. pp_rank: 1, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2024-01-23 06:16:55.455: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2024-01-23 06:16:55.456: I smdistributed/modelparallel/torch/state_mod.py:163] [3] Finished initializing torch distributed process groups. pp_rank: 1, tp_rank: 0, dp_rank: 1, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2024-01-23 06:16:55.456: I smdistributed/modelparallel/torch/state_mod.py:163] [7] Finished initializing torch distributed process groups. pp_rank: 3, tp_rank: 0, dp_rank: 1, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2024-01-23 06:16:55.456: I smdistributed/modelparallel/torch/state_mod.py:163] [5] Finished initializing torch distributed process groups. pp_rank: 2, tp_rank: 0, dp_rank: 1, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2024-01-23 06:16:55.456: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2024-01-23 06:16:55.456: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2024-01-23 06:16:55.456: I smdistributed/modelparallel/torch/state_mod.py:163] [6] Finished initializing torch distributed process groups. pp_rank: 3, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2024-01-23 06:16:55.457: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2024-01-23 06:16:55.457: I smdistributed/modelparallel/torch/state_mod.py:163] [4] Finished initializing torch distributed process groups. pp_rank: 2, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2024-01-23 06:16:55.457: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2024-01-23 06:16:55.458: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2024-01-23 06:16:55.464: I smdistributed/modelparallel/torch/state_mod.py:163] [1] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 1, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.464: I smdistributed/modelparallel/torch/state_mod.py:163] [0] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.464: I smdistributed/modelparallel/torch/throttler.py:37] Using NCCL throttle limit of 8.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.464: I smdistributed/modelparallel/backend/config.py:314] Configuration parameters:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.465: I smdistributed/modelparallel/backend/config.py:317]   activation_loading_horizon: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2024-01-23 06:16:55.465: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.465: I smdistributed/modelparallel/backend/config.py:317]   active_microbatches: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.465: I smdistributed/modelparallel/backend/config.py:317]   auto_partition: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.465: I smdistributed/modelparallel/backend/config.py:317]   bf16: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.465: I smdistributed/modelparallel/backend/config.py:317]   contiguous: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.465: I smdistributed/modelparallel/backend/config.py:317]   ddp: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.465: I smdistributed/modelparallel/backend/config.py:317]   ddp_dist_backend: auto\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.465: I smdistributed/modelparallel/backend/config.py:317]   ddp_port: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.465: I smdistributed/modelparallel/backend/config.py:317]   default_partition: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.465: I smdistributed/modelparallel/backend/config.py:317]   delayed_parameter_initialization: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.466: I smdistributed/modelparallel/backend/config.py:317]   fp16: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.466: I smdistributed/modelparallel/backend/config.py:317]   fp16_params: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.466: I smdistributed/modelparallel/backend/config.py:317]   horovod: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.466: I smdistributed/modelparallel/backend/config.py:317]   memory_weight: 0.8\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.466: I smdistributed/modelparallel/backend/config.py:317]   microbatches: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.466: I smdistributed/modelparallel/backend/config.py:317]   offload_activations: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.466: I smdistributed/modelparallel/backend/config.py:317]   optimize: speed\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.466: I smdistributed/modelparallel/backend/config.py:317]   pipeline: interleaved\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.466: I smdistributed/modelparallel/backend/config.py:317]   pipeline_parallel_degree: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.467: I smdistributed/modelparallel/backend/config.py:317]   placement_strategy: spread\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.467: I smdistributed/modelparallel/backend/config.py:317]   predefined_hooks: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.467: I smdistributed/modelparallel/backend/config.py:317]   prescaled_batch: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.467: I smdistributed/modelparallel/backend/config.py:317]   sdp_gradient_clipping: 1.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.467: I smdistributed/modelparallel/backend/config.py:317]   sdp_hierarchical_allgather: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.467: I smdistributed/modelparallel/backend/config.py:317]   sdp_max_live_parameters: 1000000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.467: I smdistributed/modelparallel/backend/config.py:317]   sdp_param_persistence_threshold: 1000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.467: I smdistributed/modelparallel/backend/config.py:317]   sdp_reduce_bucket_size: 500000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.467: I smdistributed/modelparallel/backend/config.py:317]   shard_optimizer_state: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.467: I smdistributed/modelparallel/backend/config.py:317]   sharded_data_parallel_degree: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.467: I smdistributed/modelparallel/backend/config.py:317]   skip_tracing: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.468: I smdistributed/modelparallel/backend/config.py:317]   tensor_parallel_degree: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.468: I smdistributed/modelparallel/backend/config.py:317]   tensor_parallel_seed: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.468: W smdistributed/modelparallel/backend/config.py:323] WARNING: \"fp16_params\" is a deprecated config key, please use \"fp16\" instead\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 06:16:55.468: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:WARNING:__main__:Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING:__main__:Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:WARNING:__main__:Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:WARNING:__main__:Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:WARNING:__main__:Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:WARNING:__main__:Process rank: 7, device: cuda:7, n_gpu: 1distributed training: True, 16-bits training: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__:Training/evaluation parameters TrainingArguments(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:_n_gpu=1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:adafactor=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:adam_beta1=0.9,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:adam_beta2=0.999,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:adam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:auto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:bf16=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:bf16_full_eval=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:data_seed=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:dataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:dataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:dataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:ddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:ddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:ddp_timeout=1800,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:debug=[],\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:deepspeed=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:disable_tqdm=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:do_eval=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:do_predict=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:do_train=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:eval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:eval_delay=0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:eval_steps=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:evaluation_strategy=no,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:fp16=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:fp16_backend=auto,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:fp16_full_eval=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:fp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:fsdp=[],\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:fsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:fsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:full_determinism=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:gradient_accumulation_steps=1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:gradient_checkpointing=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:greater_is_better=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:group_by_length=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:half_precision_backend=auto,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:hub_model_id=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:hub_private_repo=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:hub_strategy=every_save,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:hub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:ignore_data_skip=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:include_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:jit_mode_eval=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:label_names=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:label_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:learning_rate=5e-05,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:length_column_name=length,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:load_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:local_rank=0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:log_level=passive,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:log_level_replica=warning,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:log_on_each_node=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:logging_dir=/opt/ml/model/runs/Jan23_06-16-55_algo-1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:logging_first_step=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:logging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:logging_steps=500,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:logging_strategy=steps,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:lr_scheduler_type=linear,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:max_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:max_steps=500,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:metric_for_best_model=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:mp_parameters=ddp=True,microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:no_cuda=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:num_train_epochs=2.0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:optim=adamw_hf,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:optim_args=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:output_dir=/opt/ml/model,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:overwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:past_index=-1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:per_device_eval_batch_size=16,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:per_device_train_batch_size=16,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:prediction_loss_only=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:push_to_hub=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:push_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:push_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:ray_scope=last,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:remove_unused_columns=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:report_to=['tensorboard'],\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:resume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:run_name=/opt/ml/model,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:save_on_each_node=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:save_safetensors=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:save_steps=500,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:save_strategy=steps,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:save_total_limit=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:seed=42,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:sharded_ddp=[],\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:skip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:tf32=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:torch_compile=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:torch_compile_backend=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:torch_compile_mode=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:torchdynamo=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:tpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:tpu_num_cores=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:use_ipex=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:use_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:use_mps_device=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:warmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:warmup_steps=0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:weight_decay=0.0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:xpu_backend=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:WARNING:__main__:Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading readme:   0%|          | 0.00/31.9k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading readme: 100%|| 31.9k/31.9k [00:00<00:00, 21.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading readme:   0%|          | 0.00/31.9k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading readme: 100%|| 31.9k/31.9k [00:00<00:00, 18.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Downloading and preparing dataset None/ax to /root/.cache/huggingface/datasets/parquet/ax-97eb8006079b1fff/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:datasets.builder:Using custom data configuration ax-97eb8006079b1fff\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading readme:   0%|          | 0.00/31.9k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading readme: 100%|| 31.9k/31.9k [00:00<00:00, 20.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/251k [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 251k/251k [00:00<00:00, 61.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/52.2M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:  13%|        | 6.70M/52.2M [00:00<00:00, 67.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:  27%|       | 14.3M/52.2M [00:00<00:00, 72.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:  42%|     | 22.0M/52.2M [00:00<00:00, 74.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:  57%|    | 29.7M/52.2M [00:00<00:00, 75.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:  72%|  | 37.4M/52.2M [00:00<00:00, 76.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:  86%| | 45.1M/52.2M [00:00<00:00, 76.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 52.2M/52.2M [00:00<00:00, 75.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/649k [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 649k/649k [00:00<00:00, 50.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/17.5M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:  36%|      | 6.30M/17.5M [00:00<00:00, 63.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:  77%|  | 13.4M/17.5M [00:00<00:00, 67.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 17.5M/17.5M [00:00<00:00, 68.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/33.6M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:  21%|        | 7.02M/33.6M [00:00<00:00, 70.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:  44%|     | 14.8M/33.6M [00:00<00:00, 74.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:  68%|   | 22.7M/33.6M [00:00<00:00, 76.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:  91%|| 30.7M/33.6M [00:00<00:00, 77.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 33.6M/33.6M [00:00<00:00, 76.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/584k [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 584k/584k [00:00<00:00, 75.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/3.11M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 3.11M/3.11M [00:00<00:00, 68.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/502k [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 502k/502k [00:00<00:00, 58.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/38.8k [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 38.8k/38.8k [00:00<00:00, 31.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data files:  33%|      | 1/3 [00:02<00:05,  2.89s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/37.6k [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 37.6k/37.6k [00:00<00:00, 27.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/1.21M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 1.21M/1.21M [00:00<00:00, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/1.25M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 1.25M/1.25M [00:00<00:00, 74.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/1.21M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 1.21M/1.21M [00:00<00:00, 52.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/1.25M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 1.25M/1.25M [00:00<00:00, 67.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/75.7k [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 75.7k/75.7k [00:00<00:00, 30.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/872k [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 872k/872k [00:00<00:00, 62.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/3.73M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 3.73M/3.73M [00:00<00:00, 65.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/69.0k [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 69.0k/69.0k [00:00<00:00, 32.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/72.8k [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 72.8k/72.8k [00:00<00:00, 35.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/151k [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 151k/151k [00:00<00:00, 41.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/11.1k [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 11.1k/11.1k [00:00<00:00, 14.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data files:  67%|   | 2/3 [00:05<00:02,  2.49s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/80.8k [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 80.8k/80.8k [00:00<00:00, 26.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/37.7k [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 37.7k/37.7k [00:00<00:00, 24.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/1.22M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 1.22M/1.22M [00:00<00:00, 52.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/1.26M [00:00<?, ?B/s][1,mpirank:3,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 1.26M/1.26M [00:00<00:00, 63.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/1.22M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 1.22M/1.22M [00:00<00:00, 60.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/1.26M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 1.26M/1.26M [00:00<00:00, 75.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/308k [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 308k/308k [00:00<00:00, 45.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/877k [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 877k/877k [00:00<00:00, 66.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/36.7M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:  23%|       | 8.42M/36.7M [00:00<00:00, 84.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:  49%|     | 17.9M/36.7M [00:00<00:00, 90.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:  75%|  | 27.6M/36.7M [00:00<00:00, 93.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 36.7M/36.7M [00:00<00:00, 93.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/621k [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 621k/621k [00:00<00:00, 72.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/148k [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 148k/148k [00:00<00:00, 48.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/114k [00:00<?, ?B/s][1,mpirank:3,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 114k/114k [00:00<00:00, 30.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data:   0%|          | 0.00/13.6k [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data: 100%|| 13.6k/13.6k [00:00<00:00, 13.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data files: 100%|| 3/3 [00:07<00:00,  2.60s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading data files: 100%|| 3/3 [00:07<00:00,  2.61s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Extracting data files: 100%|| 3/3 [00:00<00:00, 162.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Downloading and preparing dataset None/ax to /root/.cache/huggingface/datasets/parquet/ax-97eb8006079b1fff/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading data files: 100%|| 3/3 [00:00<00:00, 1298.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Extracting data files: 100%|| 3/3 [00:00<00:00, 162.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Downloading and preparing dataset None/ax to /root/.cache/huggingface/datasets/parquet/ax-97eb8006079b1fff/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading data files: 100%|| 3/3 [00:00<00:00, 1228.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Extracting data files: 100%|| 3/3 [00:00<00:00, 153.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Downloading and preparing dataset None/ax to /root/.cache/huggingface/datasets/parquet/ax-97eb8006079b1fff/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading data files: 100%|| 3/3 [00:00<00:00, 1497.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Extracting data files: 100%|| 3/3 [00:00<00:00, 193.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:datasets.builder:Generating dataset parquet (/root/.cache/huggingface/datasets/parquet/ax-97eb8006079b1fff/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Downloading and preparing dataset None/ax to /root/.cache/huggingface/datasets/parquet/ax-97eb8006079b1fff/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:datasets.builder:Dataset not on Hf google storage. Downloading and preparing it from source\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading data files: 100%|| 3/3 [00:00<00:00, 1298.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:datasets.download.download_manager:Downloading took 0.0 min\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Extracting data files: 100%|| 3/3 [00:00<00:00, 155.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:datasets.builder:Generating train split\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Downloading and preparing dataset None/ax to /root/.cache/huggingface/datasets/parquet/ax-97eb8006079b1fff/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading data files: 100%|| 3/3 [00:00<00:00, 1546.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Extracting data files: 100%|| 3/3 [00:00<00:00, 192.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Downloading and preparing dataset None/ax to /root/.cache/huggingface/datasets/parquet/ax-97eb8006079b1fff/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Downloading data files: 100%|| 3/3 [00:00<00:00, 1269.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Extracting data files: 100%|| 3/3 [00:00<00:00, 184.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Downloading and preparing dataset None/ax to /root/.cache/huggingface/datasets/parquet/ax-97eb8006079b1fff/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading data files: 100%|| 3/3 [00:00<00:00, 1347.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Extracting data files: 100%|| 3/3 [00:00<00:00, 165.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: Traceback (most recent call last) \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:196 in _run_module_as_main                \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   193    main_globals = sys.modules[\"__main__\"].__dict__                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   194    if alter_argv:                                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   195       sys.argv[0] = mod_spec.origin                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  196    return _run_code(code, main_globals, None,                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   197                 \"__main__\", mod_spec)                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   198                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   199 def run_module(mod_name, init_globals=None,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:86 in _run_code                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    83                   __loader__ = loader,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    84                   __package__ = pkg_name,                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    85                   __spec__ = mod_spec)                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   86    exec(code, run_globals)                                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    87    return run_globals                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    88                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    89 def _run_module_code(code, init_globals=None,                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/mpi4py/__main__.py:7 in <module>     \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   4 from .run import main                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   5                                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   6 if __name__ == '__main__':                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  7    main()                                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   8                                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/mpi4py/run.py:198 in main            \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   195    # Run user code. In case of an unhandled exception, abort          \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   196    # execution of the MPI program by calling 'MPI_Abort()'.           \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   197    try:                                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  198       run_command_line(args)                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   199    except SystemExit as exc:                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   200       set_abort_status(exc.code)                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   201       raise                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/mpi4py/run.py:47 in run_command_line \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    44       from os.path import realpath, dirname                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    45       if not getattr(sys.flags, 'isolated', 0):  # pragma: no branch \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    46          sys.path[0] = realpath(dirname(sys.argv[0]))  # Fix sys.pa \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   47       run_path(sys.argv[0], run_name='__main__')                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    48                                                                        [1,mpirank:6,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    49                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    50 def set_abort_status(status):                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:289 in run_path                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   286       # Not a valid sys.path entry, so run the code directly         \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   287       # execfile() doesn't help as we want to allow compiled files   \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   288       code, fname = _get_code_from_file(run_name, path_name)         \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  289       return _run_module_code(code, init_globals, run_name,          \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   290                         pkg_name=pkg_name, script_name=fname)  \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   291    else:                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   292       # Finder is defined for path, so add it to                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:96 in _run_module_code                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    93    fname = script_name if mod_spec is None else mod_spec.origin       \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    94    with _TempModule(mod_name) as temp_module, _ModifiedArgv0(fname):  \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    95       mod_globals = temp_module.module.__dict__                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   96       _run_code(code, mod_globals, init_globals,                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    97               mod_name, mod_spec, pkg_name, script_name)           \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    98    # Copy the globals of the temporary module, as they                \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    99    # may be cleared when the temporary module goes away               \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:86 in _run_code                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    83                   __loader__ = loader,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    84                   __package__ = pkg_name,                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    85                   __spec__ = mod_spec)                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   86    exec(code, run_globals)                                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    87    return run_globals                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    88                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    89 def _run_module_code(code, init_globals=None,                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: /opt/ml/code/run_glue.py:626 in <module>                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   623                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   624                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   625 if __name__ == \"__main__\":                                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  626    main()                                                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   627                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: /opt/ml/code/run_glue.py:280 in main                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   277    # download the dataset.                                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   278    if data_args.task_name is not None:                                \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   279       # Downloading and loading a dataset from the hub.              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  280       raw_datasets = load_dataset(                [1,mpirank:6,algo-1]<stderr>:                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   281          \"glue\",                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   282          data_args.task_name,                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   283          cache_dir=model_args.cache_dir,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/load.py:1797 in             \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: load_dataset                                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   1794    try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES          \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   1795                                                                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   1796    # Download and prepare data                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  1797    builder_instance.download_and_prepare(                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   1798       download_config=download_config,                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   1799       download_mode=download_mode,                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   1800       verification_mode=verification_mode,                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/builder.py:890 in           \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: download_and_prepare                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    887                      prepare_split_kwargs[\"max_shard_size\"] =  \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    888                   if num_proc is not None:                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    889                      prepare_split_kwargs[\"num_proc\"] = num_pr \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   890                   self._download_and_prepare(                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    891                      dl_manager=dl_manager,                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    892                      verification_mode=verification_mode,      \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    893                      **prepare_split_kwargs,                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/builder.py:985 in           \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: _download_and_prepare                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    982                                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    983          try:                                                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    984             # Prepare split will record examples associated to th \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   985             self._prepare_split(split_generator, **prepare_split_ \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    986          except OSError as e:                                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    987             raise OSError(                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:    988                \"Cannot find data file. \"                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/builder.py:1706 in          \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: _prepare_split                                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   1703       path_join = os.path.join if is_local else posixpath.join      \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   1704                                                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   1705       if self.info.splits is not None:                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  1706          split_info = self.info.splits[split_generator.name]       \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   1707       else:                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   1708          split[1,mpirank:6,algo-1]<stderr>:_info = split_generator.split_info                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   1709                                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/splits.py:530 in            \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: __getitem__                                                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   527          return super().__getitem__(str(key))                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   528       # 2nd case: Uses instructions: `info.splits['train[50%]']`     \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   529       else:                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  530          instructions = make_file_instructions(                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   531             name=self.dataset_name,                                \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   532             split_infos=self.values(),                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   533             instruction=key,                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/arrow_reader.py:112 in      \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: make_file_instructions                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   109    \"\"\"                                                                \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   110    name2len = {info.name: info.num_examples for info in split_infos}  \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   111    name2shard_lengths = {info.name: info.shard_lengths for info in sp \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  112    name2filenames = {                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   113       info.name: filenames_for_dataset_split(                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   114          path=prefix_path,                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   115          dataset_name=name,                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/arrow_reader.py:113 in      \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: <dictcomp>                                                                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   110    name2len = {info.name: info.num_examples for info in split_infos}  \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   111    name2shard_lengths = {info.name: info.shard_lengths for info in sp \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   112    name2filenames = {                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  113       info.name: filenames_for_dataset_split(                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   114          path=prefix_path,                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   115          dataset_name=name,                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   116          split=info.name,                                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/naming.py:70 in             \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: filenames_for_dataset_split                                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   67                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   68                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   69 def filenames_for_dataset_split(path, dataset_name, split, filetype_suf \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  70    prefix = filename_prefix_for_split(dataset_name, split)             \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   71    prefix = os.path.join(path, prefix)                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   72                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   73    if shard_lengths:                                                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: /opt/conda/li\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:b/python3.10/site-packages/datasets/naming.py:54 in             \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: filename_prefix_for_split                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   51                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   52                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   53 def filename_prefix_for_split(name, split):                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  54    if os.path.basename(name) != name:                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   55       raise ValueError(f\"Should be a dataset name, not a path: {name} \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   56    if not re.match(_split_re, split):                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   57       raise ValueError(f\"Split name should match '{_split_re}'' but g \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: /opt/conda/lib/python3.10/posixpath.py:142 in basename                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   139                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   140 def basename(p):                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   141    \"\"\"Returns the final component of a pathname\"\"\"                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  142    p = os.fspath(p)                                                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   143    sep = _get_sep(p)                                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   144    i = p.rfind(sep) + 1                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:   145    return p[i:]                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:TypeError: expected str, bytes or os.PathLike object, not NoneType\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: Traceback (most recent call last) \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:196 in _run_module_as_main                \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   193    main_globals = sys.modules[\"__main__\"].__dict__                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   194    if alter_argv:                                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   195       sys.argv[0] = mod_spec.origin                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  196    return _run_code(code, main_globals, None,                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   197                 \"__main__\", mod_spec)                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   198                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   199 def run_module(mod_name, init_globals=None,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:86 in _run_code                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    83                   __loader__ = loader,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    84                   __package__ = pkg_name,                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    85                   __spec__ = mod_spec)                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   86    exec(code, run_globals)                                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    87    return run_globals                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    88                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    89 def _run_module_code(code, init_globals=None,                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/mpi4py/__main__.py:7 in <module>     \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   4 from .run import main                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   5                                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   6 if __name__ == '__main__':                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  7    main()                                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   8                                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/mpi4py/run.py:198 in main            \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   195    # Run user code. In case of an unhandled exception, abort          \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   196    # execution of the MPI program by calling 'MPI_Abort()'.           \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   197    try:                                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  198       run_command_line(args)                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   199    except SystemExit as exc:                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   200       set_abort_status(exc.code)                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   201       raise                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/mpi4py/run.py:47 in run_command_line \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    44       from os.path import realpath, dirname                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    45       if not getattr(sys.flags, 'isolated', 0):  # pragma: no branch \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    46          sys.path[0] = realpath(dirname(sys.argv[0]))  # Fix sys.pa \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   47       run_path(sys.argv[0], run_name='__main__')                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    48                                                                        [1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    49                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    50 def set_abort_status(status):                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:289 in run_path                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   286       # Not a valid sys.path entry, so run the code directly         \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   287       # execfile() doesn't help as we want to allow compiled files   \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   288       code, fname = _get_code_from_file(run_name, path_name)         \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  289       return _run_module_code(code, init_globals, run_name,          \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   290                         pkg_name=pkg_name, script_name=fname)  \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   291    else:                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   292       # Finder is defined for path, so add it to                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:96 in _run_module_code                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    93    fname = script_name if mod_spec is None else mod_spec.origin       \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    94    with _TempModule(mod_name) as temp_module, _ModifiedArgv0(fname):  \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    95       mod_globals = temp_module.module.__dict__                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   96       _run_code(code, mod_globals, init_globals,                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    97               mod_name, mod_spec, pkg_name, script_name)           \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    98    # Copy the globals of the temporary module, as they                \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    99    # may be cleared when the temporary module goes away               \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:86 in _run_code                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    83                   __loader__ = loader,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    84                   __package__ = pkg_name,                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    85                   __spec__ = mod_spec)                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   86    exec(code, run_globals)                                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    87    return run_globals                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    88                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    89 def _run_module_code(code, init_globals=None,                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: /opt/ml/code/run_glue.py:626 in <module>                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   623                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   624                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   625 if __name__ == \"__main__\":                                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  626    main()                                                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   627                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: /opt/ml/code/run_glue.py:280 in main                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   277    # download the dataset.                                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   278    if data_args.task_name is not None:                                \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   279       # Downloading and loading a dataset from the hub.              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  280       raw_datasets = load_dataset(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   281          \"glue\",                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   282          data_args.task_name,                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   283          cache_dir=model_args.cache_dir,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/load.py:1797 in             \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: load_dataset                                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   1794    try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES          \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   1795                                                                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   1796    # Download and prepare data                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  1797    builder_instance.download_and_prepare(                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   1798       download_config=download_config,                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   1799       download_mode=download_mode,                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   1800       verification_mode=verification_mode,                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/builder.py:890 in           \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: download_and_prepare                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    887                      prepare_split_kwargs[\"max_shard_size\"] =  \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    888                   if num_proc is not None:                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    889                      prepare_split_kwargs[\"num_proc\"] = num_pr \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   890                   self._download_and_prepare(                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    891                      dl_manager=dl_manager,                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    892                      verification_mode=verification_mode,      \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    893                      **prepare_split_kwargs,                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/builder.py:985 in           \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: _download_and_prepare                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    982                                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    983          try:                                                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    984             # Prepare split will record examples associated to th \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   985             self._prepare_split(split_generator, **prepare_split_ \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    986          except OSError as e:                                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    987             raise OSError(                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:    988                \"Cannot find data file. \"                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/builder.py:1706 in          \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: _prepare_split                                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   1703       path_join = os.path.join if is_local else posixpath.join      \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   1704                                                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   1705       if self.info.splits is not None:                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  1706          split_info = self.info.splits[split_generator.name]       \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   1707       else:                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   1708          split[1,mpirank:3,algo-1]<stderr>:_info = split_generator.split_info                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   1709                                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/splits.py:530 in            \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: __getitem__                                                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   527          return super().__getitem__(str(key))                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   528       # 2nd case: Uses instructions: `info.splits['train[50%]']`     \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   529       else:                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  530          instructions = make_file_instructions(                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   531             name=self.dataset_name,                                \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   532             split_infos=self.values(),                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   533             instruction=key,                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/arrow_reader.py:112 in      \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: make_file_instructions                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   109    \"\"\"                                                                \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   110    name2len = {info.name: info.num_examples for info in split_infos}  \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   111    name2shard_lengths = {info.name: info.shard_lengths for info in sp \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  112    name2filenames = {                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   113       info.name: filenames_for_dataset_split(                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   114          path=prefix_path,                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   115          dataset_name=name,                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/arrow_reader.py:113 in      \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: <dictcomp>                                                                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   110    name2len = {info.name: info.num_examples for info in split_infos}  \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   111    name2shard_lengths = {info.name: info.shard_lengths for info in sp \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   112    name2filenames = {                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  113       info.name: filenames_for_dataset_split(                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   114          path=prefix_path,                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   115          dataset_name=name,                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   116          split=info.name,                                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/naming.py:70 in             \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: filenames_for_dataset_split                                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   67                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   68                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   69 def filenames_for_dataset_split(path, dataset_name, split, filetype_suf \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  70    prefix = filename_prefix_for_split(dataset_name, split)             \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   71    prefix = os.path.join(path, prefix)                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   72                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   73    if shard_lengths:                                                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: /opt/conda/li\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:b/python3.10/site-packages/datasets/naming.py:54 in             \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: filename_prefix_for_split                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   51                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   52                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   53 def filename_prefix_for_split(name, split):                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  54    if os.path.basename(name) != name:                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   55       raise ValueError(f\"Should be a dataset name, not a path: {name} \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   56    if not re.match(_split_re, split):                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   57       raise ValueError(f\"Split name should match '{_split_re}'' but g \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: /opt/conda/lib/python3.10/posixpath.py:142 in basename                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   139                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   140 def basename(p):                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   141    \"\"\"Returns the final component of a pathname\"\"\"                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  142    p = os.fspath(p)                                                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   143    sep = _get_sep(p)                                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   144    i = p.rfind(sep) + 1                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:   145    return p[i:]                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:TypeError: expected str, bytes or os.PathLike object, not NoneType\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: Traceback (most recent call last) \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:196 in _run_module_as_main                \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   193    main_globals = sys.modules[\"__main__\"].__dict__                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   194    if alter_argv:                                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   195       sys.argv[0] = mod_spec.origin                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  196    return _run_code(code, main_globals, None,                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   197                 \"__main__\", mod_spec)                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   198                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   199 def run_module(mod_name, init_globals=None,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:86 in _run_code                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    83                   __loader__ = loader,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    84                   __package__ = pkg_name,                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    85                   __spec__ = mod_spec)                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   86    exec(code, run_globals)                                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    87    return run_globals                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    88                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    89 def _run_module_code(code, init_globals=None,                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/mpi4py/__main__.py:7 in <module>     \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   4 from .run import main                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   5                                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   6 if __name__ == '__main__':                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  7    main()                                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   8                                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/mpi4py/run.py:198 in main            \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   195    # Run user code. In case of an unhandled exception, abort          \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   196    # execution of the MPI program by calling 'MPI_Abort()'.           \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   197    try:                                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  198       run_command_line(args)                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   199    except SystemExit as exc:                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   200       set_abort_status(exc.code)                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   201       raise                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/mpi4py/run.py:47 in run_command_line \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    44       from os.path import realpath, dirname                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    45       if not getattr(sys.flags, 'isolated', 0):  # pragma: no branch \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    46          sys.path[0] = realpath(dirname(sys.argv[0]))  # Fix sys.pa \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   47       run_path(sys.argv[0], run_name='__main__')                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    48                                                                        [1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    49                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    50 def set_abort_status(status):                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:289 in run_path                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   286       # Not a valid sys.path entry, so run the code directly         \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   287       # execfile() doesn't help as we want to allow compiled files   \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   288       code, fname = _get_code_from_file(run_name, path_name)         \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  289       return _run_module_code(code, init_globals, run_name,          \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   290                         pkg_name=pkg_name, script_name=fname)  \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   291    else:                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   292       # Finder is defined for path, so add it to                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:96 in _run_module_code                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    93    fname = script_name if mod_spec is None else mod_spec.origin       \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    94    with _TempModule(mod_name) as temp_module, _ModifiedArgv0(fname):  \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    95       mod_globals = temp_module.module.__dict__                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   96       _run_code(code, mod_globals, init_globals,                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    97               mod_name, mod_spec, pkg_name, script_name)           \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    98    # Copy the globals of the temporary module, as they                \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    99    # may be cleared when the temporary module goes away               \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:86 in _run_code                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    83                   __loader__ = loader,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    84                   __package__ = pkg_name,                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    85                   __spec__ = mod_spec)                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   86    exec(code, run_globals)                                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    87    return run_globals                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    88                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    89 def _run_module_code(code, init_globals=None,                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: /opt/ml/code/run_glue.py:626 in <module>                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   623                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   624                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   625 if __name__ == \"__main__\":                                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  626    main()                                                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   627                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: /opt/ml/code/run_glue.py:280 in main                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   277    # download the dataset.                                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   278    if data_args.task_name is not None:                                \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   279       # Downloading and loading a dataset from the hub.              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  280       raw_datasets = load_dataset(                [1,mpirank:4,algo-1]<stderr>:                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   281          \"glue\",                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   282          data_args.task_name,                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   283          cache_dir=model_args.cache_dir,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/load.py:1797 in             \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: load_dataset                                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   1794    try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES          \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   1795                                                                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   1796    # Download and prepare data                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  1797    builder_instance.download_and_prepare(                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   1798       download_config=download_config,                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   1799       download_mode=download_mode,                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   1800       verification_mode=verification_mode,                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/builder.py:890 in           \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: download_and_prepare                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    887                      prepare_split_kwargs[\"max_shard_size\"] =  \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    888                   if num_proc is not None:                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    889                      prepare_split_kwargs[\"num_proc\"] = num_pr \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   890                   self._download_and_prepare(                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    891                      dl_manager=dl_manager,                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    892                      verification_mode=verification_mode,      \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    893                      **prepare_split_kwargs,                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/builder.py:985 in           \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: _download_and_prepare                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    982                                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    983          try:                                                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    984             # Prepare split will record examples associated to th \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   985             self._prepare_split(split_generator, **prepare_split_ \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    986          except OSError as e:                                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    987             raise OSError(                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:    988                \"Cannot find data file. \"                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/builder.py:1706 in          \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: _prepare_split                                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   1703       path_join = os.path.join if is_local else posixpath.join      \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   1704                                                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   1705       if self.info.splits is not None:                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  1706          split_info = self.info.splits[split_generator.name]       \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   1707       else:                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   1708          split[1,mpirank:4,algo-1]<stderr>:_info = split_generator.split_info                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   1709                                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/splits.py:530 in            \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: __getitem__                                                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   527          return super().__getitem__(str(key))                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   528       # 2nd case: Uses instructions: `info.splits['train[50%]']`     \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   529       else:                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  530          instructions = make_file_instructions(                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   531             name=self.dataset_name,                                \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   532             split_infos=self.values(),                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   533             instruction=key,                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/arrow_reader.py:112 in      \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: make_file_instructions                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   109    \"\"\"                                                                \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   110    name2len = {info.name: info.num_examples for info in split_infos}  \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   111    name2shard_lengths = {info.name: info.shard_lengths for info in sp \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  112    name2filenames = {                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   113       info.name: filenames_for_dataset_split(                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   114          path=prefix_path,                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   115          dataset_name=name,                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/arrow_reader.py:113 in      \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: <dictcomp>                                                                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   110    name2len = {info.name: info.num_examples for info in split_infos}  \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   111    name2shard_lengths = {info.name: info.shard_lengths for info in sp \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   112    name2filenames = {                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  113       info.name: filenames_for_dataset_split(                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   114          path=prefix_path,                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   115          dataset_name=name,                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   116          split=info.name,                                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/naming.py:70 in             \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: filenames_for_dataset_split                                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   67                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   68                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   69 def filenames_for_dataset_split(path, dataset_name, split, filetype_suf \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  70    prefix = filename_prefix_for_split(dataset_name, split)             \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   71    prefix = os.path.join(path, prefix)                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   72                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   73    if shard_lengths:                                                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: /opt/conda/li[1,mpirank:4,algo-1]<stderr>:b/python3.10/site-packages/datasets/naming.py:54 in             \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: filename_prefix_for_split                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   51                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   52                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   53 def filename_prefix_for_split(name, split):                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  54    if os.path.basename(name) != name:                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   55       raise ValueError(f\"Should be a dataset name, not a path: {name} \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   56    if not re.match(_split_re, split):                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   57       raise ValueError(f\"Split name should match '{_split_re}'' but g \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: /opt/conda/lib/python3.10/posixpath.py:142 in basename                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   139                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   140 def basename(p):                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   141    \"\"\"Returns the final component of a pathname\"\"\"                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  142    p = os.fspath(p)                                                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   143    sep = _get_sep(p)                                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   144    i = p.rfind(sep) + 1                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:   145    return p[i:]                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:TypeError: expected str, bytes or os.PathLike object, not NoneType\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: Traceback (most recent call last) \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:196 in _run_module_as_main                \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   193    main_globals = sys.modules[\"__main__\"].__dict__                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   194    if alter_argv:                                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   195       sys.argv[0] = mod_spec.origin                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  196    return _run_code(code, main_globals, None,                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   197                 \"__main__\", mod_spec)                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   198                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   199 def run_module(mod_name, init_globals=None,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:86 in _run_code                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    83                   __loader__ = loader,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    84                   __package__ = pkg_name,                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    85                   __spec__ = mod_spec)                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   86    exec(code, run_globals)                                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    87    return run_globals                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    88                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    89 def _run_module_code(code, init_globals=None,                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/mpi4py/__main__.py:7 in <module>     \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   4 from .run import main                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   5                                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   6 if __name__ == '__main__':                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  7    main()                                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   8                                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/mpi4py/run.py:198 in main            \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   195    # Run user code. In case of an unhandled exception, abort          \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   196    # execution of the MPI program by calling 'MPI_Abort()'.           \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   197    try:                                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  198       run_command_line(args)                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   199    except SystemExit as exc:                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   200       set_abort_status(exc.code)                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   201       raise                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/mpi4py/run.py:47 in run_command_line \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    44       from os.path import realpath, dirname                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    45       if not getattr(sys.flags, 'isolated', 0):  # pragma: no branch \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    46          sys.path[0] = realpath(dirname(sys.argv[0]))  # Fix sys.pa \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   47       run_path(sys.argv[0], run_name='__main__')                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    48                                                                        [1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    49                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    50 def set_abort_status(status):                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:289 in run_path                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   286       # Not a valid sys.path entry, so run the code directly         \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   287       # execfile() doesn't help as we want to allow compiled files   \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   288       code, fname = _get_code_from_file(run_name, path_name)         \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  289       return _run_module_code(code, init_globals, run_name,          \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   290                         pkg_name=pkg_name, script_name=fname)  \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   291    else:                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   292       # Finder is defined for path, so add it to                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:96 in _run_module_code                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    93    fname = script_name if mod_spec is None else mod_spec.origin       \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    94    with _TempModule(mod_name) as temp_module, _ModifiedArgv0(fname):  \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    95       mod_globals = temp_module.module.__dict__                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   96       _run_code(code, mod_globals, init_globals,                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    97               mod_name, mod_spec, pkg_name, script_name)           \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    98    # Copy the globals of the temporary module, as they                \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    99    # may be cleared when the temporary module goes away               \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:86 in _run_code                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    83                   __loader__ = loader,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    84                   __package__ = pkg_name,                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    85                   __spec__ = mod_spec)                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   86    exec(code, run_globals)                                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    87    return run_globals                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    88                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    89 def _run_module_code(code, init_globals=None,                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: /opt/ml/code/run_glue.py:626 in <module>                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   623                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   624                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   625 if __name__ == \"__main__\":                                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  626    main()                                                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   627                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: /opt/ml/code/run_glue.py:280 in main                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   277    # download the dataset.                                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   278    if data_args.task_name is not None:                                \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   279       # Downloading and loading a dataset from the hub.              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  280       raw_datasets = load_dataset(                [1,mpirank:7,algo-1]<stderr>:                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   281          \"glue\",                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   282          data_args.task_name,                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   283          cache_dir=model_args.cache_dir,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/load.py:1797 in             \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: load_dataset                                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   1794    try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES          \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   1795                                                                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   1796    # Download and prepare data                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  1797    builder_instance.download_and_prepare(                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   1798       download_config=download_config,                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   1799       download_mode=download_mode,                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   1800       verification_mode=verification_mode,                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/builder.py:890 in           \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: download_and_prepare                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    887                      prepare_split_kwargs[\"max_shard_size\"] =  \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    888                   if num_proc is not None:                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    889                      prepare_split_kwargs[\"num_proc\"] = num_pr \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   890                   self._download_and_prepare(                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    891                      dl_manager=dl_manager,                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    892                      verification_mode=verification_mode,      \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    893                      **prepare_split_kwargs,                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/builder.py:985 in           \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: _download_and_prepare                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    982                                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    983          try:                                                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    984             # Prepare split will record examples associated to th \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   985             self._prepare_split(split_generator, **prepare_split_ \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    986          except OSError as e:                                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    987             raise OSError(                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:    988                \"Cannot find data file. \"                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/builder.py:1706 in          \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: _prepare_split                                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   1703       path_join = os.path.join if is_local else posixpath.join      \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   1704                                                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   1705       if self.info.splits is not None:                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  1706          split_info = self.info.splits[split_generator.name]       \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   1707       else:                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   1708          split[1,mpirank:7,algo-1]<stderr>:_info = split_generator.split_info                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   1709                                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/splits.py:530 in            \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: __getitem__                                                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   527          return super().__getitem__(str(key))                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   528       # 2nd case: Uses instructions: `info.splits['train[50%]']`     \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   529       else:                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  530          instructions = make_file_instructions(                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   531             name=self.dataset_name,                                \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   532             split_infos=self.values(),                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   533             instruction=key,                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/arrow_reader.py:112 in      \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: make_file_instructions                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   109    \"\"\"                                                                \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   110    name2len = {info.name: info.num_examples for info in split_infos}  \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   111    name2shard_lengths = {info.name: info.shard_lengths for info in sp \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  112    name2filenames = {                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   113       info.name: filenames_for_dataset_split(                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   114          path=prefix_path,                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   115          dataset_name=name,                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/arrow_reader.py:113 in      \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: <dictcomp>                                                                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   110    name2len = {info.name: info.num_examples for info in split_infos}  \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   111    name2shard_lengths = {info.name: info.shard_lengths for info in sp \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   112    name2filenames = {                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  113       info.name: filenames_for_dataset_split(                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   114          path=prefix_path,                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   115          dataset_name=name,                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   116          split=info.name,                                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/naming.py:70 in             \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: filenames_for_dataset_split                                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   67                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   68                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   69 def filenames_for_dataset_split(path, dataset_name, split, filetype_suf \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  70    prefix = filename_prefix_for_split(dataset_name, split)             \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   71    prefix = os.path.join(path, prefix)                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   72                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   73    if shard_lengths:                                                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: /opt/conda/li[1,mpirank:7,algo-1]<stderr>:b/python3.10/site-packages/datasets/naming.py:54 in             \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: filename_prefix_for_split                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   51                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   52                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   53 def filename_prefix_for_split(name, split):                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  54    if os.path.basename(name) != name:                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   55       raise ValueError(f\"Should be a dataset name, not a path: {name} \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   56    if not re.match(_split_re, split):                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   57       raise ValueError(f\"Split name should match '{_split_re}'' but g \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: /opt/conda/lib/python3.10/posixpath.py:142 in basename                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   139                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   140 def basename(p):                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   141    \"\"\"Returns the final component of a pathname\"\"\"                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  142    p = os.fspath(p)                                                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   143    sep = _get_sep(p)                                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   144    i = p.rfind(sep) + 1                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:   145    return p[i:]                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:TypeError: expected str, bytes or os.PathLike object, not NoneType\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: Traceback (most recent call last) \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:196 in _run_module_as_main                \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   193    main_globals = sys.modules[\"__main__\"].__dict__                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   194    if alter_argv:                                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   195       sys.argv[0] = mod_spec.origin                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  196    return _run_code(code, main_globals, None,                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   197                 \"__main__\", mod_spec)                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   198                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   199 def run_module(mod_name, init_globals=None,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:86 in _run_code                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    83                   __loader__ = loader,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    84                   __package__ = pkg_name,                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    85                   __spec__ = mod_spec)                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   86    exec(code, run_globals)                                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    87    return run_globals                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    88                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    89 def _run_module_code(code, init_globals=None,                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/mpi4py/__main__.py:7 in <module>     \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   4 from .run import main                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   5                                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   6 if __name__ == '__main__':                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  7    main()                                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   8                                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/mpi4py/run.py:198 in main            \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   195    # Run user code. In case of an unhandled exception, abort          \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   196    # execution of the MPI program by calling 'MPI_Abort()'.           \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   197    try:                                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  198       run_command_line(args)                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   199    except SystemExit as exc:                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   200       set_abort_status(exc.code)                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   201       raise                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/mpi4py/run.py:47 in run_command_line \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    44       from os.path import realpath, dirname                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    45       if not getattr(sys.flags, 'isolated', 0):  # pragma: no branch \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    46          sys.path[0] = realpath(dirname(sys.argv[0]))  # Fix sys.pa \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   47       run_path(sys.argv[0], run_name='__main__')                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    48                                                                        [1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    49                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    50 def set_abort_status(status):                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:289 in run_path                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   286       # Not a valid sys.path entry, so run the code directly         \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   287       # execfile() doesn't help as we want to allow compiled files   \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   288       code, fname = _get_code_from_file(run_name, path_name)         \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  289       return _run_module_code(code, init_globals, run_name,          \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   290                         pkg_name=pkg_name, script_name=fname)  \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   291    else:                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   292       # Finder is defined for path, so add it to                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:96 in _run_module_code                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    93    fname = script_name if mod_spec is None else mod_spec.origin       \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    94    with _TempModule(mod_name) as temp_module, _ModifiedArgv0(fname):  \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    95       mod_globals = temp_module.module.__dict__                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   96       _run_code(code, mod_globals, init_globals,                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    97               mod_name, mod_spec, pkg_name, script_name)           \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    98    # Copy the globals of the temporary module, as they                \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    99    # may be cleared when the temporary module goes away               \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:86 in _run_code                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    83                   __loader__ = loader,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    84                   __package__ = pkg_name,                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    85                   __spec__ = mod_spec)                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   86    exec(code, run_globals)                                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    87    return run_globals                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    88                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    89 def _run_module_code(code, init_globals=None,                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: /opt/ml/code/run_glue.py:626 in <module>                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   623                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   624                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   625 if __name__ == \"__main__\":                                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  626    main()                                                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   627                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: /opt/ml/code/run_glue.py:280 in main                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   277    # download the dataset.                                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   278    if data_args.task_name is not None:                                \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   279       # Downloading and loading a dataset from the hub.              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  280       raw_datasets = load_dataset(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   281          \"glue\",                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   282          data_args.task_name,                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   283          cache_dir=model_args.cache_dir,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/load.py:1797 in             \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: load_dataset                                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   1794    try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES          \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   1795                                                                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   1796    # Download and prepare data                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  1797    builder_instance.download_and_prepare(                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   1798       download_config=download_config,                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   1799       download_mode=download_mode,                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   1800       verification_mode=verification_mode,                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/builder.py:890 in           \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: download_and_prepare                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    887                      prepare_split_kwargs[\"max_shard_size\"] =  \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    888                   if num_proc is not None:                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    889                      prepare_split_kwargs[\"num_proc\"] = num_pr \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   890                   self._download_and_prepare(                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    891                      dl_manager=dl_manager,                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    892                      verification_mode=verification_mode,      \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    893                      **prepare_split_kwargs,                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/builder.py:985 in           \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: _download_and_prepare                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    982                                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    983          try:                                                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    984             # Prepare split will record examples associated to th \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   985             self._prepare_split(split_generator, **prepare_split_ \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    986          except OSError as e:                                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    987             raise OSError(                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    988                \"Cannot find data file. \"                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/builder.py:1706 in          \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: _prepare_split                                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   1703       path_join = os.path.join if is_local else posixpath.join      \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   1704                                                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   1705       if self.info.splits is not None:                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  1706          split_info = self.info.splits[split_generator.name]       \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   1707       else:                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   1708          split[1,mpirank:0,algo-1]<stderr>:_info = split_generator.split_info                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   1709                                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/splits.py:530 in            \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: __getitem__                                                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   527          return super().__getitem__(str(key))                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   528       # 2nd case: Uses instructions: `info.splits['train[50%]']`     \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   529       else:                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  530          instructions = make_file_instructions(                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   531             name=self.dataset_name,                                \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   532             split_infos=self.values(),                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   533             instruction=key,                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/arrow_reader.py:112 in      \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: make_file_instructions                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   109    \"\"\"                                                                \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   110    name2len = {info.name: info.num_examples for info in split_infos}  \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   111    name2shard_lengths = {info.name: info.shard_lengths for info in sp \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  112    name2filenames = {                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   113       info.name: filenames_for_dataset_split(                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   114          path=prefix_path,                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   115          dataset_name=name,                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/arrow_reader.py:113 in      \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: <dictcomp>                                                                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   110    name2len = {info.name: info.num_examples for info in split_infos}  \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   111    name2shard_lengths = {info.name: info.shard_lengths for info in sp \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   112    name2filenames = {                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  113       info.name: filenames_for_dataset_split(                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   114          path=prefix_path,                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   115          dataset_name=name,                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   116          split=info.name,                                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/naming.py:70 in             \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: filenames_for_dataset_split                                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   67                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   68                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   69 def filenames_for_dataset_split(path, dataset_name, split, filetype_suf \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  70    prefix = filename_prefix_for_split(dataset_name, split)             \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   71    prefix = os.path.join(path, prefix)                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   72                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   73    if shard_lengths:                                                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: /opt/conda/li[1,mpirank:0,algo-1]<stderr>:b/python3.10/site-packages/datasets/naming.py:54 in             \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: filename_prefix_for_split                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   51                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   52                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   53 def filename_prefix_for_split(name, split):                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  54    if os.path.basename(name) != name:                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   55       raise ValueError(f\"Should be a dataset name, not a path: {name} \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   56    if not re.match(_split_re, split):                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   57       raise ValueError(f\"Split name should match '{_split_re}'' but g \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: /opt/conda/lib/python3.10/posixpath.py:142 in basename                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   139                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   140 def basename(p):                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   141    \"\"\"Returns the final component of a pathname\"\"\"                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  142    p = os.fspath(p)                                                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   143    sep = _get_sep(p)                                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   144    i = p.rfind(sep) + 1                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:   145    return p[i:]                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:TypeError: expected str, bytes or os.PathLike object, not NoneType\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: Traceback (most recent call last) \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:196 in _run_module_as_main                \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   193    main_globals = sys.modules[\"__main__\"].__dict__                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   194    if alter_argv:                                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   195       sys.argv[0] = mod_spec.origin                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  196    return _run_code(code, main_globals, None,                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   197                 \"__main__\", mod_spec)                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   198                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   199 def run_module(mod_name, init_globals=None,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:86 in _run_code                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    83                   __loader__ = loader,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    84                   __package__ = pkg_name,                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    85                   __spec__ = mod_spec)                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   86    exec(code, run_globals)                                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    87    return run_globals                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    88                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    89 def _run_module_code(code, init_globals=None,                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/mpi4py/__main__.py:7 in <module>     \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   4 from .run import main                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   5                                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   6 if __name__ == '__main__':                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  7    main()                                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   8                                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/mpi4py/run.py:198 in main            \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   195    # Run user code. In case of an unhandled exception, abort          \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   196    # execution of the MPI program by calling 'MPI_Abort()'.           \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   197    try:                                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  198       run_command_line(args)                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   199    except SystemExit as exc:                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   200       set_abort_status(exc.code)                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   201       raise                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/mpi4py/run.py:47 in run_command_line \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    44       from os.path import realpath, dirname                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    45       if not getattr(sys.flags, 'isolated', 0):  # pragma: no branch \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    46          sys.path[0] = realpath(dirname(sys.argv[0]))  # Fix sys.pa \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   47       run_path(sys.argv[0], run_name='__main__')                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    48                                                                        [1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    49                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    50 def set_abort_status(status):                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:289 in run_path                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   286       # Not a valid sys.path entry, so run the code directly         \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   287       # execfile() doesn't help as we want to allow compiled files   \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   288       code, fname = _get_code_from_file(run_name, path_name)         \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  289       return _run_module_code(code, init_globals, run_name,          \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   290                         pkg_name=pkg_name, script_name=fname)  \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   291    else:                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   292       # Finder is defined for path, so add it to                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:96 in _run_module_code                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    93    fname = script_name if mod_spec is None else mod_spec.origin       \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    94    with _TempModule(mod_name) as temp_module, _ModifiedArgv0(fname):  \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    95       mod_globals = temp_module.module.__dict__                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   96       _run_code(code, mod_globals, init_globals,                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    97               mod_name, mod_spec, pkg_name, script_name)           \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    98    # Copy the globals of the temporary module, as they                \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    99    # may be cleared when the temporary module goes away               \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:86 in _run_code                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    83                   __loader__ = loader,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    84                   __package__ = pkg_name,                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    85                   __spec__ = mod_spec)                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   86    exec(code, run_globals)                                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    87    return run_globals                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    88                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    89 def _run_module_code(code, init_globals=None,                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: /opt/ml/code/run_glue.py:626 in <module>                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   623                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   624                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   625 if __name__ == \"__main__\":                                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  626    main()                                                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   627                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: /opt/ml/code/run_glue.py:280 in main                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   277    # download the dataset.                                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   278    if data_args.task_name is not None:                                \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   279       # Downloading and loading a dataset from the hub.              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  280       raw_datasets = load_dataset(                [1,mpirank:2,algo-1]<stderr>:                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   281          \"glue\",                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   282          data_args.task_name,                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   283          cache_dir=model_args.cache_dir,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/load.py:1797 in             \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: load_dataset                                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   1794    try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES          \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   1795                                                                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   1796    # Download and prepare data                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  1797    builder_instance.download_and_prepare(                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   1798       download_config=download_config,                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   1799       download_mode=download_mode,                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   1800       verification_mode=verification_mode,                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/builder.py:890 in           \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: download_and_prepare                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    887                      prepare_split_kwargs[\"max_shard_size\"] =  \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    888                   if num_proc is not None:                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    889                      prepare_split_kwargs[\"num_proc\"] = num_pr \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   890                   self._download_and_prepare(                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    891                      dl_manager=dl_manager,                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    892                      verification_mode=verification_mode,      \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    893                      **prepare_split_kwargs,                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/builder.py:985 in           \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: _download_and_prepare                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    982                                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    983          try:                                                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    984             # Prepare split will record examples associated to th \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   985             self._prepare_split(split_generator, **prepare_split_ \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    986          except OSError as e:                                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    987             raise OSError(                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:    988                \"Cannot find data file. \"                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/builder.py:1706 in          \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: _prepare_split                                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   1703       path_join = os.path.join if is_local else posixpath.join      \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   1704                                                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   1705       if self.info.splits is not None:                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  1706          split_info = self.info.splits[split_generator.name]       \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   1707       else:                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   1708          split\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:_info = split_generator.split_info                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   1709                                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/splits.py:530 in            \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: __getitem__                                                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   527          return super().__getitem__(str(key))                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   528       # 2nd case: Uses instructions: `info.splits['train[50%]']`     \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   529       else:                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  530          instructions = make_file_instructions(                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   531             name=self.dataset_name,                                \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   532             split_infos=self.values(),                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   533             instruction=key,                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/arrow_reader.py:112 in      \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: make_file_instructions                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   109    \"\"\"                                                                \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   110    name2len = {info.name: info.num_examples for info in split_infos}  \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   111    name2shard_lengths = {info.name: info.shard_lengths for info in sp \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  112    name2filenames = {                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   113       info.name: filenames_for_dataset_split(                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   114          path=prefix_path,                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   115          dataset_name=name,                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/arrow_reader.py:113 in      \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: <dictcomp>                                                                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   110    name2len = {info.name: info.num_examples for info in split_infos}  \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   111    name2shard_lengths = {info.name: info.shard_lengths for info in sp \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   112    name2filenames = {                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  113       info.name: filenames_for_dataset_split(                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   114          path=prefix_path,                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   115          dataset_name=name,                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   116          split=info.name,                                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/naming.py:70 in             \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: filenames_for_dataset_split                                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   67                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   68                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   69 def filenames_for_dataset_split(path, dataset_name, split, filetype_suf \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  70    prefix = filename_prefix_for_split(dataset_name, split)             \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   71    prefix = os.path.join(path, prefix)                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   72                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   73    if shard_lengths:                                                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: /opt/conda/li[1,mpirank:2,algo-1]<stderr>:b/python3.10/site-packages/datasets/naming.py:54 in             \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: filename_prefix_for_split                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   51                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   52                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   53 def filename_prefix_for_split(name, split):                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  54    if os.path.basename(name) != name:                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   55       raise ValueError(f\"Should be a dataset name, not a path: {name} \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   56    if not re.match(_split_re, split):                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   57       raise ValueError(f\"Split name should match '{_split_re}'' but g \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: /opt/conda/lib/python3.10/posixpath.py:142 in basename                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   139                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   140 def basename(p):                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   141    \"\"\"Returns the final component of a pathname\"\"\"                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  142    p = os.fspath(p)                                                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   143    sep = _get_sep(p)                                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   144    i = p.rfind(sep) + 1                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:   145    return p[i:]                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:TypeError: expected str, bytes or os.PathLike object, not NoneType\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: Traceback (most recent call last) \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:196 in _run_module_as_main                \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   193    main_globals = sys.modules[\"__main__\"].__dict__                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   194    if alter_argv:                                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   195       sys.argv[0] = mod_spec.origin                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  196    return _run_code(code, main_globals, None,                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   197                 \"__main__\", mod_spec)                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   198                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   199 def run_module(mod_name, init_globals=None,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:86 in _run_code                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    83                   __loader__ = loader,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    84                   __package__ = pkg_name,                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    85                   __spec__ = mod_spec)                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   86    exec(code, run_globals)                                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    87    return run_globals                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    88                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    89 def _run_module_code(code, init_globals=None,                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/mpi4py/__main__.py:7 in <module>     \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   4 from .run import main                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   5                                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   6 if __name__ == '__main__':                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  7    main()                                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   8                                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/mpi4py/run.py:198 in main            \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   195    # Run user code. In case of an unhandled exception, abort          \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   196    # execution of the MPI program by calling 'MPI_Abort()'.           \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   197    try:                                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  198       run_command_line(args)                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   199    except SystemExit as exc:                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   200       set_abort_status(exc.code)                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   201       raise                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/mpi4py/run.py:47 in run_command_line \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    44       from os.path import realpath, dirname                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    45       if not getattr(sys.flags, 'isolated', 0):  # pragma: no branch \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    46          sys.path[0] = realpath(dirname(sys.argv[0]))  # Fix sys.pa \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   47       run_path(sys.argv[0], run_name='__main__')                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    48                                                                        [1,mpirank:5,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    49                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    50 def set_abort_status(status):                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:289 in run_path                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   286       # Not a valid sys.path entry, so run the code directly         \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   287       # execfile() doesn't help as we want to allow compiled files   \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   288       code, fname = _get_code_from_file(run_name, path_name)         \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  289       return _run_module_code(code, init_globals, run_name,          \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   290                         pkg_name=pkg_name, script_name=fname)  \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   291    else:                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   292       # Finder is defined for path, so add it to                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:96 in _run_module_code                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    93    fname = script_name if mod_spec is None else mod_spec.origin       \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    94    with _TempModule(mod_name) as temp_module, _ModifiedArgv0(fname):  \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    95       mod_globals = temp_module.module.__dict__                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   96       _run_code(code, mod_globals, init_globals,                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    97               mod_name, mod_spec, pkg_name, script_name)           \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    98    # Copy the globals of the temporary module, as they                \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    99    # may be cleared when the temporary module goes away               \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:86 in _run_code                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    83                   __loader__ = loader,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    84                   __package__ = pkg_name,                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    85                   __spec__ = mod_spec)                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   86    exec(code, run_globals)                                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    87    return run_globals                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    88                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    89 def _run_module_code(code, init_globals=None,                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: /opt/ml/code/run_glue.py:626 in <module>                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   623                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   624                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   625 if __name__ == \"__main__\":                                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  626    main()                                                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   627                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: /opt/ml/code/run_glue.py:280 in main                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   277    # download the dataset.                                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   278    if data_args.task_name is not None:                                \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   279       # Downloading and loading a dataset from the hub.              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  280       raw_datasets = load_dataset(                [1,mpirank:5,algo-1]<stderr>:                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   281          \"glue\",                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   282          data_args.task_name,                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   283          cache_dir=model_args.cache_dir,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/load.py:1797 in             \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: load_dataset                                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   1794    try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES          \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   1795                                                                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   1796    # Download and prepare data                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  1797    builder_instance.download_and_prepare(                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   1798       download_config=download_config,                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   1799       download_mode=download_mode,                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   1800       verification_mode=verification_mode,                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/builder.py:890 in           \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: download_and_prepare                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    887                      prepare_split_kwargs[\"max_shard_size\"] =  \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    888                   if num_proc is not None:                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    889                      prepare_split_kwargs[\"num_proc\"] = num_pr \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   890                   self._download_and_prepare(                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    891                      dl_manager=dl_manager,                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    892                      verification_mode=verification_mode,      \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    893                      **prepare_split_kwargs,                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/builder.py:985 in           \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: _download_and_prepare                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    982                                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    983          try:                                                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    984             # Prepare split will record examples associated to th \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   985             self._prepare_split(split_generator, **prepare_split_ \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    986          except OSError as e:                                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    987             raise OSError(                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:    988                \"Cannot find data file. \"                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/builder.py:1706 in          \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: _prepare_split                                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   1703       path_join = os.path.join if is_local else posixpath.join      \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   1704                                                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   1705       if self.info.splits is not None:                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  1706          split_info = self.info.splits[split_generator.name]       \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   1707       else:                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   1708          split[1,mpirank:5,algo-1]<stderr>:_info = split_generator.split_info                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   1709                                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/splits.py:530 in            \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: __getitem__                                                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   527          return super().__getitem__(str(key))                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   528       # 2nd case: Uses instructions: `info.splits['train[50%]']`     \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   529       else:                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  530          instructions = make_file_instructions(                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   531             name=self.dataset_name,                                \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   532             split_infos=self.values(),                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   533             instruction=key,                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/arrow_reader.py:112 in      \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: make_file_instructions                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   109    \"\"\"                                                                \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   110    name2len = {info.name: info.num_examples for info in split_infos}  \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   111    name2shard_lengths = {info.name: info.shard_lengths for info in sp \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  112    name2filenames = {                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   113       info.name: filenames_for_dataset_split(                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   114          path=prefix_path,                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   115          dataset_name=name,                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/arrow_reader.py:113 in      \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: <dictcomp>                                                                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   110    name2len = {info.name: info.num_examples for info in split_infos}  \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   111    name2shard_lengths = {info.name: info.shard_lengths for info in sp \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   112    name2filenames = {                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  113       info.name: filenames_for_dataset_split(                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   114          path=prefix_path,                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   115          dataset_name=name,                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   116          split=info.name,                                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/naming.py:70 in             \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: filenames_for_dataset_split                                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   67                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   68                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   69 def filenames_for_dataset_split(path, dataset_name, split, filetype_suf \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  70    prefix = filename_prefix_for_split(dataset_name, split)             \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   71    prefix = os.path.join(path, prefix)                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   72                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   73    if shard_lengths:                                                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: /opt/conda/li\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:b/python3.10/site-packages/datasets/naming.py:54 in             \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: filename_prefix_for_split                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   51                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   52                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   53 def filename_prefix_for_split(name, split):                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  54    if os.path.basename(name) != name:                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   55       raise ValueError(f\"Should be a dataset name, not a path: {name} \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   56    if not re.match(_split_re, split):                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   57       raise ValueError(f\"Split name should match '{_split_re}'' but g \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: /opt/conda/lib/python3.10/posixpath.py:142 in basename                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   139                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   140 def basename(p):                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   141    \"\"\"Returns the final component of a pathname\"\"\"                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  142    p = os.fspath(p)                                                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   143    sep = _get_sep(p)                                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   144    i = p.rfind(sep) + 1                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:   145    return p[i:]                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:TypeError: expected str, bytes or os.PathLike object, not NoneType\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: Traceback (most recent call last) \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:196 in _run_module_as_main                \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   193    main_globals = sys.modules[\"__main__\"].__dict__                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   194    if alter_argv:                                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   195       sys.argv[0] = mod_spec.origin                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  196    return _run_code(code, main_globals, None,                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   197                 \"__main__\", mod_spec)                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   198                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   199 def run_module(mod_name, init_globals=None,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:86 in _run_code                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    83                   __loader__ = loader,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    84                   __package__ = pkg_name,                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    85                   __spec__ = mod_spec)                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   86    exec(code, run_globals)                                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    87    return run_globals                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    88                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    89 def _run_module_code(code, init_globals=None,                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/mpi4py/__main__.py:7 in <module>     \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   4 from .run import main                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   5                                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   6 if __name__ == '__main__':                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  7    main()                                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   8                                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/mpi4py/run.py:198 in main            \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   195    # Run user code. In case of an unhandled exception, abort          \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   196    # execution of the MPI program by calling 'MPI_Abort()'.           \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   197    try:                                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  198       run_command_line(args)                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   199    except SystemExit as exc:                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   200       set_abort_status(exc.code)                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   201       raise                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/mpi4py/run.py:47 in run_command_line \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    44       from os.path import realpath, dirname                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    45       if not getattr(sys.flags, 'isolated', 0):  # pragma: no branch \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    46          sys.path[0] = realpath(dirname(sys.argv[0]))  # Fix sys.pa \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   47       run_path(sys.argv[0], run_name='__main__')                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    48                                                                        [1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    49                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    50 def set_abort_status(status):                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:289 in run_path                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   286       # Not a valid sys.path entry, so run the code directly         \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   287       # execfile() doesn't help as we want to allow compiled files   \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   288       code, fname = _get_code_from_file(run_name, path_name)         \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  289       return _run_module_code(code, init_globals, run_name,          \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   290                         pkg_name=pkg_name, script_name=fname)  \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   291    else:                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   292       # Finder is defined for path, so add it to                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:96 in _run_module_code                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    93    fname = script_name if mod_spec is None else mod_spec.origin       \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    94    with _TempModule(mod_name) as temp_module, _ModifiedArgv0(fname):  \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    95       mod_globals = temp_module.module.__dict__                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   96       _run_code(code, mod_globals, init_globals,                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    97               mod_name, mod_spec, pkg_name, script_name)           \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    98    # Copy the globals of the temporary module, as they                \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    99    # may be cleared when the temporary module goes away               \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: /opt/conda/lib/python3.10/runpy.py:86 in _run_code                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    83                   __loader__ = loader,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    84                   __package__ = pkg_name,                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    85                   __spec__ = mod_spec)                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   86    exec(code, run_globals)                                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    87    return run_globals                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    88                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    89 def _run_module_code(code, init_globals=None,                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: /opt/ml/code/run_glue.py:626 in <module>                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   623                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   624                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   625 if __name__ == \"__main__\":                                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  626    main()                                                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   627                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: /opt/ml/code/run_glue.py:280 in main                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   277    # download the dataset.                                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   278    if data_args.task_name is not None:                                \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   279       # Downloading and loading a dataset from the hub.              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  280       raw_datasets = load_dataset(                [1,mpirank:1,algo-1]<stderr>:                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   281          \"glue\",                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   282          data_args.task_name,                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   283          cache_dir=model_args.cache_dir,                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/load.py:1797 in             \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: load_dataset                                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   1794    try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES          \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   1795                                                                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   1796    # Download and prepare data                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  1797    builder_instance.download_and_prepare(                            \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   1798       download_config=download_config,                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   1799       download_mode=download_mode,                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   1800       verification_mode=verification_mode,                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/builder.py:890 in           \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: download_and_prepare                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    887                      prepare_split_kwargs[\"max_shard_size\"] =  \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    888                   if num_proc is not None:                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    889                      prepare_split_kwargs[\"num_proc\"] = num_pr \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   890                   self._download_and_prepare(                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    891                      dl_manager=dl_manager,                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    892                      verification_mode=verification_mode,      \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    893                      **prepare_split_kwargs,                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/builder.py:985 in           \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: _download_and_prepare                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    982                                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    983          try:                                                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    984             # Prepare split will record examples associated to th \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   985             self._prepare_split(split_generator, **prepare_split_ \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    986          except OSError as e:                                      \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    987             raise OSError(                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:    988                \"Cannot find data file. \"                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/builder.py:1706 in          \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: _prepare_split                                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   1703       path_join = os.path.join if is_local else posixpath.join      \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   1704                                                                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   1705       if self.info.splits is not None:                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  1706          split_info = self.info.splits[split_generator.name]       \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   1707       else:                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   1708          split[1,mpirank:1,algo-1]<stderr>:_info = split_generator.split_info                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   1709                                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/splits.py:530 in            \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: __getitem__                                                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   527          return super().__getitem__(str(key))                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   528       # 2nd case: Uses instructions: `info.splits['train[50%]']`     \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   529       else:                                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  530          instructions = make_file_instructions(                     \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   531             name=self.dataset_name,                                \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   532             split_infos=self.values(),                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   533             instruction=key,                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/arrow_reader.py:112 in      \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: make_file_instructions                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   109    \"\"\"                                                                \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   110    name2len = {info.name: info.num_examples for info in split_infos}  \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   111    name2shard_lengths = {info.name: info.shard_lengths for info in sp \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  112    name2filenames = {                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   113       info.name: filenames_for_dataset_split(                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   114          path=prefix_path,                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   115          dataset_name=name,                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/arrow_reader.py:113 in      \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: <dictcomp>                                                                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   110    name2len = {info.name: info.num_examples for info in split_infos}  \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   111    name2shard_lengths = {info.name: info.shard_lengths for info in sp \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   112    name2filenames = {                                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  113       info.name: filenames_for_dataset_split(                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   114          path=prefix_path,                                          \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   115          dataset_name=name,                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   116          split=info.name,                                           \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: /opt/conda/lib/python3.10/site-packages/datasets/naming.py:70 in             \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: filenames_for_dataset_split                                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   67                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   68                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   69 def filenames_for_dataset_split(path, dataset_name, split, filetype_suf \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  70    prefix = filename_prefix_for_split(dataset_name, split)             \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   71    prefix = os.path.join(path, prefix)                                 \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   72                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   73    if shard_lengths:                                                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: /opt/conda/li[1,mpirank:1,algo-1]<stderr>:b/python3.10/site-packages/datasets/naming.py:54 in             \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: filename_prefix_for_split                                                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   51                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   52                                                                         \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   53 def filename_prefix_for_split(name, split):                             \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  54    if os.path.basename(name) != name:                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   55       raise ValueError(f\"Should be a dataset name, not a path: {name} \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   56    if not re.match(_split_re, split):                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   57       raise ValueError(f\"Split name should match '{_split_re}'' but g \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: /opt/conda/lib/python3.10/posixpath.py:142 in basename                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:                                                                              \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   139                                                                        \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   140 def basename(p):                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   141    \"\"\"Returns the final component of a pathname\"\"\"                    \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  142    p = os.fspath(p)                                                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   143    sep = _get_sep(p)                                                  \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   144    i = p.rfind(sep) + 1                                               \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:   145    return p[i:]                                                       \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:TypeError: expected str, bytes or os.PathLike object, not NoneType\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mPrimary job  terminated normally, but 1 process returned\u001b[0m\n",
      "\u001b[34ma non-zero exit code. Per user-direction, the job has been aborted.\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mmpirun.real detected that one or more processes exited with non-zero status, thus causing\u001b[0m\n",
      "\u001b[34mthe job to be terminated. The first process to do so was:\n",
      "  Process name: [[41169,1],3]\n",
      "  Exit code:    1\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m2024-01-23 06:17:08,117 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-01-23 06:17:08,118 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-01-23 06:17:08,119 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2024-01-23 06:17:08,120 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mExitCode 1\u001b[0m\n",
      "\u001b[34mErrorMessage \"    986          except OSError as e:                                      \n",
      "     987             raise OSError(                                        \n",
      "     988                \"Cannot find data file. \"                         \n",
      "                                                                               \n",
      "  /opt/conda/lib/python3.10/site-packages/datasets/builder.py:1706 in          \n",
      "  _prepare_split                                                               \n",
      "    1703       path_join = os.path.join if is_local else posixpath.join      \n",
      "    1704                                                                     \n",
      "    1705       if self.info.splits is not None:                              \n",
      "   1706          split_info = self.info.splits[split_generator.name]       \n",
      "    1707       else:                                                         \n",
      "    1708          split:_info = split_generator.split_info                   \n",
      "    1709                                                                       \n",
      "  /opt/conda/lib/python3.10/site-packages/datasets/splits.py:530 in            \n",
      "  __getitem__                                                                  \n",
      "    527          return super().__getitem__(str(key))                       \n",
      "    528       # 2nd case: Uses instructions: `info.splits['train[50%]']`     \n",
      "    529       else:                                                          \n",
      "   530          instructions = make_file_instructions(                     \n",
      "    531             name=self.dataset_name,                                \n",
      "    532             split_infos=self.values(),                             \n",
      "    533             instruction=key,                                       \n",
      "  /opt/conda/lib/python3.10/site-packages/datasets/arrow_reader.py:112 in      \n",
      "  make_file_instructions                                                       \n",
      "    109    \"\"\"                                                                \n",
      "    110    name2len = {info.name: info.num_examples for info in split_infos}  \n",
      "    111    name2shard_lengths = {info.name: info.shard_lengths for info in sp \n",
      "   112    name2filenames = {                                                 \n",
      "    113       info.name: filenames_for_dataset_split(                        \n",
      "    114          path=prefix_path,                                          \n",
      "    115          dataset_name=name,                                         \n",
      "  /opt/conda/lib/python3.10/site-packages/datasets/arrow_reader.py:113 in      \n",
      "  <dictcomp>                                                                   \n",
      "    112    name2filenames = {                                                 \n",
      "   113       info.name: filenames_for_dataset_split(                        \n",
      "    116          split=info.name,                                           \n",
      "  /opt/conda/lib/python3.10/site-packages/datasets/naming.py:70 in             \n",
      "  filenames_for_dataset_split                                                  \n",
      "    67                                                                         \n",
      "    68                                                                         \n",
      "    69 def filenames_for_dataset_split(path, dataset_name, split, filetype_suf \n",
      "   70    prefix = filename_prefix_for_split(dataset_name, split)             \n",
      "    71    prefix = os.path.join(path, prefix)                                 \n",
      "    72                                                                        \n",
      "    73    if shard_lengths:                                                   \n",
      "  /opt/conda/li\n",
      " b/python3.10/site-packages/datasets/naming.py:54 in             \n",
      "  filename_prefix_for_split                                                    \n",
      "    51                                                                         \n",
      "    52                                                                         \n",
      "    53 def filename_prefix_for_split(name, split):                             \n",
      "   54    if os.path.basename(name) != name:                                  \n",
      "    55       raise ValueError(f\"Should be a dataset name, not a path: {name} \n",
      "    56    if not re.match(_split_re, split):                                  \n",
      "    57       raise ValueError(f\"Split name should match '{_split_re}'' but g \n",
      "  /opt/conda/lib/python3.10/posixpath.py:142 in basename                       \n",
      "    139                                                                        \n",
      "    140 def basename(p):                                                       \n",
      "    141    \"\"\"Returns the final component of a pathname\"\"\"                    \n",
      "   142    p = os.fspath(p)                                                   \n",
      "    143    sep = _get_sep(p)                                                  \n",
      "    144    i = p.rfind(sep) + 1                                               \n",
      "    145    return p[i:]                                                       \n",
      " \n",
      " TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
      "  Traceback (most recent call last) \n",
      "  /opt/conda/lib/python3.10/runpy.py:196 in _run_module_as_main                \n",
      "    193    main_globals = sys.modules[\"__main__\"].__dict__                    \n",
      "    194    if alter_argv:                                                     \n",
      "    195       sys.argv[0] = mod_spec.origin                                  \n",
      "   196    return _run_code(code, main_globals, None,                         \n",
      "    197                 \"__main__\", mod_spec)                             \n",
      "    198                                                                        \n",
      "    199 def run_module(mod_name, init_globals=None,                            \n",
      "  /opt/conda/lib/python3.10/runpy.py:86 in _run_code                           \n",
      "     83                   __loader__ = loader,                            \n",
      "     84                   __package__ = pkg_name,                         \n",
      "     85                   __spec__ = mod_spec)                            \n",
      "    86    exec(code, run_globals)                                            \n",
      "     87    return run_globals                                                 \n",
      "     88                                                                        \n",
      "     89 def _run_module_code(code, init_globals=None,                          \n",
      "  /opt/conda/lib/python3.10/site-packages/mpi4py/__main__.py:7 in <module>     \n",
      "    4 from .run import main                                                    \n",
      "    5                                                                          \n",
      "    6 if __name__ == '__main__':                                               \n",
      "   7    main()                                                               \n",
      "    8                                                                          \n",
      "  /opt/conda/lib/python3.10/site-packages/mpi4py/run.py:198 in main            \n",
      "    195    # Run user code. In case of an unhandled exception, abort          \n",
      "    196    # execution of the MPI program by calling 'MPI_Abort()'.           \n",
      "    197    try:                                                               \n",
      "   198       run_command_line(args)                                         \n",
      "    199    except SystemExit as exc:                                          \n",
      "    200       set_abort_status(exc.code)                                     \n",
      "    201       raise                                                          \n",
      "  /opt/conda/lib/python3.10/site-packages/mpi4py/run.py:47 in run_command_line \n",
      "     44       from os.path import realpath, dirname                          \n",
      "     45       if not getattr(sys.flags, 'isolated', 0):  # pragma: no branch \n",
      "     46          sys.path[0] = realpath(dirname(sys.argv[0]))  # Fix sys.pa \n",
      "    47       run_path(sys.argv[0], run_name='__main__')                     \n",
      "     48                                                                        :\n",
      "     49                                                                        \n",
      "     50 def set_abort_status(status):                                          \n",
      "  /opt/conda/lib/python3.10/runpy.py:289 in run_path                           \n",
      "    286       # Not a valid sys.path entry, so run the code directly         \n",
      "    287       # execfile() doesn't help as we want to allow compiled files   \n",
      "    288       code, fname = _get_code_from_file(run_name, path_name)         \n",
      "   289       return _run_module_code(code, init_globals, run_name,          \n",
      "    290                         pkg_name=pkg_name, script_name=fname)  \n",
      "    291    else:                                                              \n",
      "    292       # Finder is defined for path, so add it to                     \n",
      "  /opt/conda/lib/python3.10/runpy.py:96 in _run_module_code                    \n",
      "     93    fname = script_name if mod_spec is None else mod_spec.origin       \n",
      "     94    with _TempModule(mod_name) as temp_module, _ModifiedArgv0(fname):  \n",
      "     95       mod_globals = temp_module.module.__dict__                      \n",
      "    96       _run_code(code, mod_globals, init_globals,                     \n",
      "     97               mod_name, mod_spec, pkg_name, script_name)           \n",
      "     98    # Copy the globals of the temporary module, as they                \n",
      "     99    # may be cleared when the temporary module goes away               \n",
      "  /opt/ml/code/run_glue.py:626 in <module>                                     \n",
      "    623                                                                        \n",
      "    624                                                                        \n",
      "    625 if __name__ == \"__main__\":                                             \n",
      "   626    main()                                                             \n",
      "    627                                                                        \n",
      "  /opt/ml/code/run_glue.py:280 in main                                         \n",
      "    277    # download the dataset.                                            \n",
      "    278    if data_args.task_name is not None:                                \n",
      "    279       # Downloading and loading a dataset from the hub.              \n",
      "   280       raw_datasets = load_dataset(\n",
      " \n",
      "    281          \"glue\",                                                    \n",
      "    282          data_args.task_name,                                       \n",
      "    283          cache_dir=model_args.cache_dir,                            \n",
      "  /opt/conda/lib/python3.10/site-packages/datasets/load.py:1797 in             \n",
      "  load_dataset                                                                 \n",
      "    1794    try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES          \n",
      "    1795                                                                      \n",
      "    1796    # Download and prepare data                                       \n",
      "   1797    builder_instance.download_and_prepare(                            \n",
      "    1798       download_config=download_config,                              \n",
      "    1799       download_mode=download_mode,                                  \n",
      "    1800       verification_mode=verification_mode,                          \n",
      "  /opt/conda/lib/python3.10/site-packages/datasets/builder.py:890 in           \n",
      "  download_and_prepare                                                         \n",
      "     887                      prepare_split_kwargs[\"max_shard_size\"] =  \n",
      "     888                   if num_proc is not None:                      \n",
      "     889                      prepare_split_kwargs[\"num_proc\"] = num_pr \n",
      "    890                   self._download_and_prepare(                   \n",
      "     891                      dl_manager=dl_manager,                    \n",
      "     892                      verification_mode=verification_mode,      \n",
      "     893                      **prepare_split_kwargs,                   \n",
      "  /opt/conda/lib/python3.10/site-packages/datasets/builder.py:985 in           \n",
      "  _download_and_prepare                                                        \n",
      "     982                                                                    \n",
      "     983          try:                                                      \n",
      "     984             # Prepare split will record examples associated to th \n",
      "    985             self._prepare_split(split_generator, **prepare_split_ \n",
      "   280       raw_datasets = load_dataset(                :                   \n",
      "  /opt/conda/li:b/python3.10/site-packages/datasets/naming.py:54 in             \n",
      "    1708          split\n",
      " _info = split_generator.split_info                   \n",
      " --------------------------------------------------------------------------\n",
      " Primary job  terminated normally, but 1 process returned\n",
      " a non-zero exit code. Per user-direction, the job has been aborted.\n",
      " mpirun.real detected that one or more processes exited with non-zero status, thus causing\n",
      " the job to be terminated. The first process to do so was\n",
      " \n",
      " Process name: [[41169,1],3]\n",
      " Exit code:    1\"\u001b[0m\n",
      "\u001b[34mCommand \"mpirun --host algo-1:8 -np 8 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.10/site-packages/gethostname.cpython-310-x86_64-linux-gnu.so -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_CURRENT_INSTANCE_TYPE -x SM_CURRENT_INSTANCE_GROUP -x SM_CURRENT_INSTANCE_GROUP_HOSTS -x SM_INSTANCE_GROUPS -x SM_INSTANCE_GROUPS_DICT -x SM_DISTRIBUTION_INSTANCE_GROUPS -x SM_IS_HETERO -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_NUM_NEURONS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_HP_DO_EVAL -x SM_HP_DO_PREDICT -x SM_HP_DO_TRAIN -x SM_HP_MAX_STEPS -x SM_HP_MODEL_NAME_OR_PATH -x SM_HP_MP_PARAMETERS -x SM_HP_NUM_TRAIN_EPOCHS -x SM_HP_OUTPUT_DIR -x SM_HP_PER_DEVICE_EVAL_BATCH_SIZE -x SM_HP_PER_DEVICE_TRAIN_BATCH_SIZE -x SM_HP_TASK_NAME -x PYTHONPATH -x NCCL_PROTO=simple -x NCCL_ALGO=ring smddpmprun -i ml.p3.16xlarge --allow-bypass /opt/conda/bin/python3.10 -m mpi4py run_glue.py --do_eval True --do_predict True --do_train True --max_steps 500 --model_name_or_path roberta-large --mp_parameters ddp=True,microbatches=4,optimize=speed,partitions=4,pipeline=interleaved,placement_strategy=spread --num_train_epochs 2 --output_dir /opt/ml/model --per_device_eval_batch_size 16 --per_device_train_batch_size 16 --task_name mnli\"\u001b[0m\n",
      "\u001b[34m2024-01-23 06:17:08,120 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\n",
      "2024-01-23 06:17:23 Uploading - Uploading generated training model\n",
      "2024-01-23 06:17:23 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job huggingface-pytorch-training-2024-01-23-06-06-54-216: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"    986          except OSError as e:                                      \n     987             raise OSError(                                        \n     988                \"Cannot find data file. \"                         \n                                                                               \n  /opt/conda/lib/python3.10/site-packages/datasets/builder.py:1706 in          \n  _prepare_split                                                               \n    1703       path_join = os.path.join if is_local else posixpath.join      \n    1704                                                                     \n    1705       if self.info.splits is not None:                              \n   1706          split_info = self.info.splits[split_generator.name]       \n    1707       else:                                           , exit code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# starting the train job with our uploaded datasets as input\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mhuggingface_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/estimator.py:1341\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_training_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/estimator.py:2677\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2675\u001b[0m \u001b[38;5;66;03m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2677\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2678\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2679\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:5506\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   5485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogs_for_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, poll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, log_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   5486\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   5487\u001b[0m \n\u001b[1;32m   5488\u001b[0m \u001b[38;5;124;03m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5504\u001b[0m \u001b[38;5;124;03m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   5505\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5506\u001b[0m     \u001b[43m_logs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:7634\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(sagemaker_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   7631\u001b[0m             last_profiler_rule_statuses \u001b[38;5;241m=\u001b[39m profiler_rule_statuses\n\u001b[1;32m   7633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 7634\u001b[0m     \u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrainingJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[1;32m   7636\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:7687\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   7681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   7682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   7683\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   7684\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   7685\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   7686\u001b[0m     )\n\u001b[0;32m-> 7687\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   7688\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   7689\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   7690\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   7691\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job huggingface-pytorch-training-2024-01-23-06-06-54-216: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"    986          except OSError as e:                                      \n     987             raise OSError(                                        \n     988                \"Cannot find data file. \"                         \n                                                                               \n  /opt/conda/lib/python3.10/site-packages/datasets/builder.py:1706 in          \n  _prepare_split                                                               \n    1703       path_join = os.path.join if is_local else posixpath.join      \n    1704                                                                     \n    1705       if self.info.splits is not None:                              \n   1706          split_info = self.info.splits[split_generator.name]       \n    1707       else:                                           , exit code: 1"
     ]
    }
   ],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3eb593-0fa4-4c1c-8b64-5f3db98e9c51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
