{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e9af52-2514-4661-a54e-09e3770396b4",
   "metadata": {},
   "source": [
    "# SageMaker distributed data parallelism (SMDDP)\n",
    "\n",
    "```python\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}\n",
    "\n",
    "# git configuration to download our fine-tuning script\n",
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.26.0'}\n",
    "\n",
    "instance_type='ml.p3.16xlarge'\n",
    "instance_count=2\n",
    "volume_size=200\n",
    "\n",
    "huggingface_estimator = HuggingFace(entry_point='run_qa.py',\n",
    "                                    source_dir='./examples/pytorch/question-answering',\n",
    "                                    git_config=git_config,\n",
    "                                    instance_type=instance_type,\n",
    "                                    instance_count=instance_count,\n",
    "                                    volume_size=volume_size,\n",
    "                                    distribution= distribution)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd048926-82e5-43a8-9f8a-ef11c4794d62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker>=2.140.0 in /opt/conda/lib/python3.10/site-packages (2.203.1)\n",
      "Requirement already satisfied: transformers==4.26.1 in /opt/conda/lib/python3.10/site-packages (4.26.1)\n",
      "Requirement already satisfied: datasets==2.10.1 in /opt/conda/lib/python3.10/site-packages (from datasets[s3]==2.10.1) (2.10.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from transformers==4.26.1) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (2022.7.9)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1) (4.64.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1->datasets[s3]==2.10.1) (14.0.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1->datasets[s3]==2.10.1) (0.3.6)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1->datasets[s3]==2.10.1) (2.1.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1->datasets[s3]==2.10.1) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1->datasets[s3]==2.10.1) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.11.1->datasets==2.10.1->datasets[s3]==2.10.1) (2022.7.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1->datasets[s3]==2.10.1) (3.9.1)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets==2.10.1->datasets[s3]==2.10.1) (0.18.0)\n",
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.10/site-packages (from datasets[s3]==2.10.1) (0.4.2)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.33.3 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (1.33.9)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (0.2.0)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (4.25.1)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (4.11.3)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (0.3.0)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (0.7.5)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (4.20.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (2.5.2)\n",
      "Requirement already satisfied: tblib<3,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (1.7.0)\n",
      "Requirement already satisfied: urllib3<1.27 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (1.26.18)\n",
      "Requirement already satisfied: uvicorn==0.22.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (0.22.0)\n",
      "Requirement already satisfied: fastapi==0.95.2 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (0.95.2)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (6.1.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.140.0) (5.9.0)\n",
      "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from fastapi==0.95.2->sagemaker>=2.140.0) (1.10.13)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi==0.95.2->sagemaker>=2.140.0) (0.27.0)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn==0.22.0->sagemaker>=2.140.0) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn==0.22.0->sagemaker>=2.140.0) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.10/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: botocore<1.34.0,>=1.33.9 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker>=2.140.0) (1.33.9)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker>=2.140.0) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.9.0,>=0.8.2 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker>=2.140.0) (0.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker>=2.140.0) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.26.1) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.26.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.26.1) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.26.1) (2023.11.17)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1->datasets[s3]==2.10.1) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1->datasets[s3]==2.10.1) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1->datasets[s3]==2.10.1) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1->datasets[s3]==2.10.1) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1->datasets[s3]==2.10.1) (4.0.3)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from docker->sagemaker>=2.140.0) (0.58.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-pasta->sagemaker>=2.140.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker>=2.140.0) (2023.11.2)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker>=2.140.0) (0.31.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker>=2.140.0) (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.10.1->datasets[s3]==2.10.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.10.1->datasets[s3]==2.10.1) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.10.1->datasets[s3]==2.10.1) (2023.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2.140.0) (1.7.6.7)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2.140.0) (0.3.3)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker>=2.140.0) (21.6.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker>=2.140.0) (3.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker>=2.140.0) (1.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.140.0\" \"transformers==4.26.1\" \"datasets[s3]==2.10.1\" \"torch\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e37c6ef-606f-48af-aa30-318894c27dfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4b9294-00fa-4d0a-817d-2f880b0478bc",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e1c3599-1755-4b18-b5e3-7a804e46f1cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::802575742115:role/service-role/AmazonSageMaker-ExecutionRole-20230929T143152\n",
      "sagemaker bucket: sagemaker-us-east-1-802575742115\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "# ACCESS to an IAM role\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc79ebe-2e0c-463d-bb26-24b9c1f56ef8",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d28db1d7-9fae-4a50-b4d9-2c0eaed2ec3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/parquet/plain_text-745310791ff4d097/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f7850433f44989acdfe4209442c366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/parquet/plain_text-745310791ff4d097/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-ec473c0cf9d9f4ca.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.'], 'label': [0, 0]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.'], 'label': [0, 0], 'input_ids': [[101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1000, 1045, 2572, 8025, 1024, 3756, 1000, 2003, 1037, 15544, 19307, 1998, 3653, 6528, 20771, 19986, 8632, 1012, 2009, 2987, 1005, 1056, 3043, 2054, 2028, 1005, 1055, 2576, 5328, 2024, 2138, 2023, 2143, 2064, 6684, 2022, 2579, 5667, 2006, 2151, 2504, 1012, 2004, 2005, 1996, 4366, 2008, 19124, 3287, 16371, 25469, 2003, 2019, 6882, 13316, 1011, 2459, 1010, 2008, 3475, 1005, 1056, 2995, 1012, 1045, 1005, 2310, 2464, 1054, 1011, 6758, 3152, 2007, 3287, 16371, 25469, 1012, 4379, 1010, 2027, 2069, 3749, 2070, 25085, 5328, 1010, 2021, 2073, 2024, 1996, 1054, 1011, 6758, 3152, 2007, 21226, 24728, 22144, 2015, 1998, 20916, 4691, 6845, 2401, 1029, 7880, 1010, 2138, 2027, 2123, 1005, 1056, 4839, 1012, 1996, 2168, 3632, 2005, 2216, 10231, 7685, 5830, 3065, 1024, 8040, 7317, 5063, 2015, 11820, 1999, 1996, 9478, 2021, 2025, 1037, 17962, 21239, 1999, 4356, 1012, 1998, 2216, 3653, 6528, 20771, 10271, 5691, 2066, 1996, 2829, 16291, 1010, 1999, 2029, 2057, 1005, 2128, 5845, 2000, 1996, 2609, 1997, 6320, 25624, 1005, 1055, 17061, 3779, 1010, 2021, 2025, 1037, 7637, 1997, 5061, 5710, 2006, 9318, 7367, 5737, 19393, 1012, 2077, 6933, 1006, 2030, 20242, 1007, 1000, 3313, 1011, 3115, 1000, 1999, 5609, 1997, 16371, 25469, 1010, 1996, 10597, 27885, 5809, 2063, 2323, 2202, 2046, 4070, 2028, 14477, 6767, 8524, 6321, 5793, 28141, 4489, 2090, 2273, 1998, 2308, 1024, 2045, 2024, 2053, 8991, 18400, 2015, 2006, 4653, 2043, 19910, 3544, 15287, 1010, 1998, 1996, 2168, 3685, 2022, 2056, 2005, 1037, 2158, 1012, 1999, 2755, 1010, 2017, 3227, 2180, 1005, 1056, 2156, 2931, 8991, 18400, 2015, 1999, 2019, 2137, 2143, 1999, 2505, 2460, 1997, 22555, 2030, 13216, 14253, 2050, 1012, 2023, 6884, 3313, 1011, 3115, 2003, 2625, 1037, 3313, 3115, 2084, 2019, 4914, 2135, 2139, 24128, 3754, 2000, 2272, 2000, 3408, 20547, 2007, 1996, 19008, 1997, 2308, 1005, 1055, 4230, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "{'labels': tensor([0, 0]), 'input_ids': tensor([[  101,  1045, 12524,  ...,     0,     0,     0],\n",
      "        [  101,  1000,  1045,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, VerificationMode\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer used in preprocessing\n",
    "tokenizer_name = 'distilbert-base-uncased'\n",
    "\n",
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# load dataset\n",
    "train_dataset, test_dataset = load_dataset('imdb', split=['train', 'test'], verification_mode=VerificationMode.NO_CHECKS)\n",
    "test_dataset = test_dataset.shuffle().select(range(10000)) # smaller the size for test dataset to 10k \n",
    "\n",
    "print(train_dataset[0:2])\n",
    "\n",
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "print(train_dataset[0:2])\n",
    "\n",
    "# input_ids are tokenized and mapped to corresponding indices in the model's vocabulary.\n",
    "\n",
    "# segment_ids \n",
    "# BERT was designed to handle tasks involving multiple sentences. segment_ids are used to distinguish different sentences in the input.\n",
    "# For a single sentence task, all elements in segment_ids would be set to 0. For tasks involving multiple sentences, you might have alternating 0s and 1s to indicate different segments.\n",
    "\n",
    "# attention_mask:\n",
    "# BERT models have a fixed input size, the attention mask is used to indicate which elements of the input sequence are actual tokens versus padding tokens.\n",
    "# 0 indicates it is a padding token. This mask helps the model focus only on the relevant tokens and ignore the padding tokens during training.\n",
    "\n",
    "# set format for pytorch\n",
    "# set_format('torch'): This method is used to set the format of the dataset to be compatible with PyTorch. \n",
    "# columns=['input_ids', 'attention_mask', 'labels']: This specifies which columns of the dataset should be converted to PyTorch tensors. \n",
    "train_dataset =  train_dataset.rename_column(\"label\", \"labels\")\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "print(train_dataset[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712ec7fd-cf90-455a-83fe-be37d71cd557",
   "metadata": {},
   "source": [
    "## Uploading data to sagemaker_session_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f1ae09f-5b3f-48fe-b908-a19b13ffa725",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# s3 key prefix for the data\n",
    "s3_prefix = 'samples/datasets/imdb'\n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "train_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "test_dataset.save_to_disk(test_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800fbcb7-0600-4722-bd59-451abb4d56e9",
   "metadata": {},
   "source": [
    "## Fine-tuning & starting SageMaker Training Job\n",
    "In order to create a sagemaker training job we need an HuggingFace Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In a Estimator we define, which fine-tuning script should be used as entry_point, which instance_type should be used, which hyperparameters are passed in .....\n",
    "\n",
    "```python\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            base_job_name='huggingface-sdk-extension',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            transformers_version='4.4',\n",
    "                            pytorch_version='1.6',\n",
    "                            py_version='py36',\n",
    "                            role=role,\n",
    "                            hyperparameters = {'epochs': 1,\n",
    "                                               'train_batch_size': 32,\n",
    "                                               'model_name':'distilbert-base-uncased'\n",
    "                                                })\n",
    "```\n",
    "\n",
    "When we create a SageMaker training job, SageMaker takes care of starting and managing all the required ec2 instances for us with the huggingface container, uploads the provided fine-tuning script train.py and downloads the data from our sagemaker_session_bucket into the container at /opt/ml/input/data. Then, it starts the training job by running.\n",
    "\n",
    "```python\n",
    "/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
    "The hyperparameters you define in the HuggingFace estimator are passed in as named arguments.\n",
    "```\n",
    "\n",
    "Sagemaker is providing useful properties about the training environment through various environment variables, including the following:\n",
    "\n",
    "- SM_MODEL_DIR: A string that represents the path where the training job writes the model artifacts to. After training, artifacts in this directory are uploaded to S3 for model hosting.\n",
    "\n",
    "- SM_NUM_GPUS: An integer representing the number of GPUs available to the host.\n",
    "\n",
    "- SM_CHANNEL_XXXX: A string that represents the path to the directory that contains the input data for the specified channel. For example, if you specify two input channels in the HuggingFace estimatorâ€™s fit call, named train and test, the environment variables SM_CHANNEL_TRAIN and SM_CHANNEL_TEST are set.\n",
    "\n",
    "To run your training job locally you can define instance_type='local' or instance_type='local_gpu' for gpu usage. Note: this does not working within SageMaker Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026e2141-b1b5-41a4-823c-50d4003db5c3",
   "metadata": {},
   "source": [
    "## Creating an Estimator and start a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fd8a9d0-d8e5-4074-89f9-6826eab1ae74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2024-01-23-04-10-36-884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-23 04:10:37 Starting - Starting the training job...\n",
      "2024-01-23 04:10:49 Pending - Training job waiting for capacity......\n",
      "2024-01-23 04:11:55 Pending - Preparing the instances for training.........\n",
      "2024-01-23 04:13:36 Downloading - Downloading the training image...........................\n",
      "2024-01-23 04:17:47 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:10,761 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:10,822 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:10,834 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:10,836 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:10,837 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:11,142 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:11,220 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:11,232 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:11,233 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:11,234 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:11,236 sagemaker-training-toolkit INFO     Cannot connect to host algo-2 at port 22. Retrying...\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:11,236 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:12,249 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:12,415 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:12,416 sagemaker-training-toolkit INFO     Can connect to host algo-2 at port 22\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:12,416 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:12,416 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:12,416 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:12,416 sagemaker-training-toolkit INFO     Host: ['algo-1', 'algo-2']\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:12,417 sagemaker-training-toolkit INFO     sagemaker_communication_backend: None\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:12,417 sagemaker-training-toolkit WARNING  Missing library /opt/conda/lib/libsmddp.so for SMDDP collective\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:12,417 sagemaker-training-toolkit WARNING  The system is not configured to run SMDDP collectives optimizedfor AWS infrastructure.Please use the latest SageMaker Deep Learning Container (DLC) to enable SMDDP Collectives support.\u001b[0m\n",
      "\u001b[34mContinuing model training with default NCCL communication backend.\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:12,417 sagemaker-training-toolkit INFO     instance type: ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:12,418 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1', 'algo-2'] Hosts: ['algo-1:8', 'algo-2:8'] process_per_hosts: 8 num_processes: 16\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:12,504 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:12,518 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": true,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"train_batch_size\": 32\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.16xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2024-01-23-04-10-36-884\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-802575742115/huggingface-pytorch-training-2024-01-23-04-10-36-884/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.16xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-802575742115/huggingface-pytorch-training-2024-01-23-04-10-36-884/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.p3.16xlarge\",\"distribution_hosts\":[\"algo-1\",\"algo-2\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2024-01-23-04-10-36-884\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-802575742115/huggingface-pytorch-training-2024-01-23-04-10-36-884/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"32\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8,algo-2:8 -np 16 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 2 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_HOMOGENEOUS=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.9/site-packages/gethostname.cpython-39-x86_64-linux-gnu.so -x SMDATAPARALLEL_SERVER_ADDR=algo-1 -x SMDATAPARALLEL_SERVER_PORT=7592 -x SAGEMAKER_INSTANCE_TYPE=ml.p3.16xlarge -x USE_SMDDP_COLLECTIVES=0 smddprun /opt/conda/bin/python3.9 -m mpi4py train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2024-01-23 04:18:11,030 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2024-01-23 04:18:11,093 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-01-23 04:18:11,105 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2024-01-23 04:18:11,108 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[35m2024-01-23 04:18:11,108 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2024-01-23 04:18:11,458 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-01-23 04:18:11,535 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-01-23 04:18:11,548 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2024-01-23 04:18:11,548 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2024-01-23 04:18:11,561 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[35m2024-01-23 04:18:11,727 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2024-01-23 04:18:11,727 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2024-01-23 04:18:11,727 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2024-01-23 04:18:11,727 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[35m2024-01-23 04:18:11,732 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m[2024-01-23 04:18:14.692: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2024-01-23 04:18:14,699 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,10.2.215.58' (ECDSA) to the list of known hosts.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[35m2024-01-23 04:18:15,746 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=58, name='orted', status='sleeping', started='04:18:14')]\u001b[0m\n",
      "\u001b[35m2024-01-23 04:18:15,746 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=58, name='orted', status='sleeping', started='04:18:14')]\u001b[0m\n",
      "\u001b[35m2024-01-23 04:18:15,746 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=58, name='orted', status='sleeping', started='04:18:14')]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:2024-01-23 04:18:19,341 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:2024-01-23 04:18:19,341 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:2024-01-23 04:18:19,341 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:2024-01-23 04:18:19,341 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:2024-01-23 04:18:19,362 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:2024-01-23 04:18:19,363 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:2024-01-23 04:18:19,375 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:2024-01-23 04:18:19,375 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:2024-01-23 04:18:19,389 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:2024-01-23 04:18:19,389 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:2024-01-23 04:18:19,391 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:2024-01-23 04:18:19,391 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:2024-01-23 04:18:19,391 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:2024-01-23 04:18:19,391 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:2024-01-23 04:18:19,391 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:2024-01-23 04:18:19,391 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:2024-01-23 04:18:19,416 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:2024-01-23 04:18:19,416 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:2024-01-23 04:18:19,421 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:2024-01-23 04:18:19,421 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:2024-01-23 04:18:19,426 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:2024-01-23 04:18:19,426 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading (â€¦)\"config.json\";:   0%|          | 0.00/483 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:2024-01-23 04:18:19,468 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:2024-01-23 04:18:19,469 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading (â€¦)\"config.json\";: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 483/483 [00:00<00:00, 64.1kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:2024-01-23 04:18:19,478 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:2024-01-23 04:18:19,478 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:2024-01-23 04:18:19,489 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:2024-01-23 04:18:19,490 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading (â€¦)\"config.json\";:   0%|          | 0.00/483 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading (â€¦)\"config.json\";: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 483/483 [00:00<00:00, 63.4kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:2024-01-23 04:18:19,509 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:2024-01-23 04:18:19,509 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:2024-01-23 04:18:19,549 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:2024-01-23 04:18:19,549 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading (â€¦)\"pytorch_model.bin\";:   0%|          | 0.00/268M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading (â€¦)\"pytorch_model.bin\";:   0%|          | 0.00/268M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading (â€¦)\"pytorch_model.bin\";:   8%|â–Š         | 21.0M/268M [00:00<00:01, 156MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading (â€¦)\"pytorch_model.bin\";:  16%|â–ˆâ–Œ        | 41.9M/268M [00:00<00:00, 387MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading (â€¦)\"pytorch_model.bin\";:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 105M/268M [00:00<00:00, 476MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading (â€¦)\"pytorch_model.bin\";:  20%|â–ˆâ–‰        | 52.4M/268M [00:00<00:01, 183MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading (â€¦)\"pytorch_model.bin\";:  27%|â–ˆâ–ˆâ–‹       | 73.4M/268M [00:00<00:01, 185MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading (â€¦)\"pytorch_model.bin\";:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 157M/268M [00:00<00:00, 414MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading (â€¦)\"pytorch_model.bin\";:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 210M/268M [00:00<00:00, 439MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading (â€¦)\"pytorch_model.bin\";:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 94.4M/268M [00:00<00:00, 182MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading (â€¦)\"pytorch_model.bin\";:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 262M/268M [00:00<00:00, 433MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading (â€¦)\"pytorch_model.bin\";: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268M/268M [00:00<00:00, 431MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading (â€¦)\"pytorch_model.bin\";:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 126M/268M [00:00<00:00, 201MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading (â€¦)\"pytorch_model.bin\";:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 157M/268M [00:00<00:00, 211MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading (â€¦)\"pytorch_model.bin\";:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 178M/268M [00:00<00:00, 208MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading (â€¦)\"pytorch_model.bin\";:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 199M/268M [00:01<00:00, 205MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading (â€¦)\"pytorch_model.bin\";:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 220M/268M [00:01<00:00, 184MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading (â€¦)\"pytorch_model.bin\";:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 241M/268M [00:01<00:00, 170MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading (â€¦)\"pytorch_model.bin\";:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 262M/268M [00:01<00:00, 176MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading (â€¦)\"pytorch_model.bin\";: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268M/268M [00:01<00:00, 185MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (â€¦)enizer_config.json\";:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (â€¦)enizer_config.json\";: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.0/28.0 [00:00<00:00, 10.9kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading (â€¦)\"vocab.txt\";:   0%|          | 0.00/232k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading (â€¦)\"vocab.txt\";: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00<00:00, 50.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (â€¦)\"tokenizer.json\";:   0%|          | 0.00/466k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading (â€¦)\"tokenizer.json\";: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 466k/466k [00:00<00:00, 17.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading (â€¦)enizer_config.json\";:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading (â€¦)enizer_config.json\";: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.0/28.0 [00:00<00:00, 8.53kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading (â€¦)\"vocab.txt\";:   0%|          | 0.00/232k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading (â€¦)\"vocab.txt\";: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00<00:00, 36.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading (â€¦)\"tokenizer.json\";:   0%|          | 0.00/466k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading (â€¦)\"tokenizer.json\";: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 466k/466k [00:00<00:00, 20.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:DDP Mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Bootstrap : Using eth0:10.2.193.163<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:NCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.193.163<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Bootstrap : Using eth0:10.2.193.163<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Bootstrap : Using eth0:10.2.193.163<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO NET/Socket : Using [0]eth0:10.2.193.163<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Bootstrap : Using eth0:10.2.193.163<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO NET/Socket : Using [0]eth0:10.2.193.163<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO NET/Socket : Using [0]eth0:10.2.193.163<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Bootstrap : Using eth0:10.2.193.163<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Bootstrap : Using eth0:10.2.215.58<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Bootstrap : Using eth0:10.2.215.58<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Bootstrap : Using eth0:10.2.215.58<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO NET/Socket : Using [0]eth0:10.2.193.163<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Bootstrap : Using eth0:10.2.193.163<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO NET/Socket : Using [0]eth0:10.2.215.58<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Bootstrap : Using eth0:10.2.215.58<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO NET/Socket : Using [0]eth0:10.2.215.58<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO NET/Socket : Using [0]eth0:10.2.215.58<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Bootstrap : Using eth0:10.2.193.163<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO NET/Socket : Using [0]eth0:10.2.193.163<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Bootstrap : Using eth0:10.2.193.163<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.215.58<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO NET/Socket : Using [0]eth0:10.2.193.163<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO NET/Socket : Using [0]eth0:10.2.193.163<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Bootstrap : Using eth0:10.2.215.58<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO NET/Socket : Using [0]eth0:10.2.215.58<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Bootstrap : Using eth0:10.2.215.58<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO NET/Socket : Using [0]eth0:10.2.215.58<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Bootstrap : Using eth0:10.2.215.58<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO NET/Socket : Using [0]eth0:10.2.215.58<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Bootstrap : Using eth0:10.2.215.58<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] find_ofi_provider:608 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO NET/Socket : Using [0]eth0:10.2.215.58<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Trees [0] 4/-1/-1->7->6 [1] 4/-1/-1->7->6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Trees [0] 6/-1/-1->5->1 [1] 6/-1/-1->5->1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Trees [0] 1/-1/-1->2->3 [1] 1/-1/-1->2->3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Trees [0] -1/-1/-1->4->7 [1] -1/-1/-1->4->7\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Trees [0] 5/-1/-1->1->2 [1] 5/-1/-1->1->2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 00/02 :    0   3   2   1   5   6   7   4   8  11  10   9  13  14  15  12\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 01/02 :    0   3   2   1   5   6   7   4   8  11  10   9  13  14  15  12\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Trees [0] 3/8/-1->0->-1 [1] 3/-1/-1->0->8\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Trees [0] 2/-1/-1->3->0 [1] 2/-1/-1->3->0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Trees [0] -1/-1/-1->12->15 [1] -1/-1/-1->12->15\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Trees [0] 10/-1/-1->11->8 [1] 10/-1/-1->11->8\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Trees [0] 14/-1/-1->13->9 [1] 14/-1/-1->13->9\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Trees [0] 9/-1/-1->10->11 [1] 9/-1/-1->10->11\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Trees [0] 11/-1/-1->8->0 [1] 11/0/-1->8->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Trees [0] 13/-1/-1->9->10 [1] 13/-1/-1->9->10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Trees [0] 12/-1/-1->15->14 [1] 12/-1/-1->15->14\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 00/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 00/0 : 13[1c0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 00/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 00/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 00/0 : 9[180] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 00/0 : 8[170] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 01/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 01/0 : 13[1c0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 00/0 : 10[190] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 00/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 01/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 01/0 : 9[180] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 01/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 01/0 : 8[170] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 01/0 : 10[190] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 00/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 01/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 00/0 : 14[1d0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 00/0 : 12[1b0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 01/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 01/0 : 14[1d0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 00/0 : 11[1a0] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 00/0 : 4[1b0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 01/0 : 11[1a0] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 00/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 00/0 : 15[1e0] -> 12[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 01/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 00/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 01/0 : 15[1e0] -> 12[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 01/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 00/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 00/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 01/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 01/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 00/0 : 13[1c0] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 00/0 : 14[1d0] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 01/0 : 13[1c0] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 01/0 : 14[1d0] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 00/0 : 9[180] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 00/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 01/0 : 9[180] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 01/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 00/0 : 10[190] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 00/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 01/0 : 10[190] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 01/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 00/0 : 11[1a0] -> 8[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 00/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 01/0 : 11[1a0] -> 8[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 00/1 : 10[190] -> 12[1b0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 01/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 00/1 : 2[190] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 00/1 : 9[180] -> 12[1b0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 00/1 : 1[180] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 01/1 : 10[190] -> 12[1b0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 01/1 : 2[190] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 01/1 : 9[180] -> 12[1b0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 01/1 : 1[180] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 01/0 : 12[1b0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 01/0 : 4[1b0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:1347 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 00/0 : 12[1b0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:1279 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 00/0 : 4[1b0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:1347 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 01/0 : 12[1b0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:1279 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 01/0 : 4[1b0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 00/0 : 12[1b0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 01/0 : 12[1b0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 00/0 : 15[1e0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 01/0 : 15[1e0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 00/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 01/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 00/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 01/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:1347 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 00/0 : 8[170] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:1279 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 00/0 : 0[170] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:1279 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 01/0 : 0[170] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:1347 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 01/0 : 8[170] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 00/0 : 0[170] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 00/0 : 8[170] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 01/0 : 8[170] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 01/0 : 0[170] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 00/1 : 0[170] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 00/1 : 3[1a0] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 00/1 : 8[170] -> 13[1c0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 00/1 : 11[1a0] -> 12[1b0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 01/1 : 3[1a0] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 01/1 : 0[170] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 01/1 : 8[170] -> 13[1c0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 01/1 : 11[1a0] -> 12[1b0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 00/1 : 3[1a0] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 00/1 : 11[1a0] -> 13[1c0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 01/1 : 3[1a0] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 01/1 : 11[1a0] -> 13[1c0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 00/1 : 3[1a0] -> 6[1d0] via P2P/indirect/7[1e0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 00/1 : 11[1a0] -> 14[1d0] via P2P/indirect/15[1e0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 00/1 : 2[190] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 00/1 : 10[190] -> 13[1c0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 01/1 : 3[1a0] -> 6[1d0] via P2P/indirect/7[1e0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 01/1 : 11[1a0] -> 14[1d0] via P2P/indirect/15[1e0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 01/1 : 2[190] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 01/1 : 10[190] -> 13[1c0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 00/1 : 2[190] -> 7[1e0] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 00/1 : 4[1b0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 00/1 : 1[180] -> 6[1d0] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 00/1 : 10[190] -> 15[1e0] via P2P/indirect/14[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 00/1 : 9[180] -> 14[1d0] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 00/1 : 12[1b0] -> 9[180] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 01/1 : 4[1b0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 01/1 : 12[1b0] -> 9[180] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 01/1 : 2[190] -> 7[1e0] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 01/1 : 10[190] -> 15[1e0] via P2P/indirect/14[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 01/1 : 9[180] -> 14[1d0] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 01/1 : 1[180] -> 6[1d0] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 00/1 : 0[170] -> 6[1d0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 00/1 : 5[1c0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 00/1 : 9[180] -> 15[1e0] via P2P/indirect/11[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 00/1 : 13[1c0] -> 8[170] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 01/1 : 5[1c0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 00/1 : 8[170] -> 14[1d0] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 00/1 : 1[180] -> 7[1e0] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 01/1 : 13[1c0] -> 8[170] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 01/1 : 0[170] -> 6[1d0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 01/1 : 9[180] -> 15[1e0] via P2P/indirect/11[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 01/1 : 8[170] -> 14[1d0] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 01/1 : 1[180] -> 7[1e0] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 00/1 : 8[170] -> 15[1e0] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 00/1 : 6[1d0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 00/1 : 14[1d0] -> 8[170] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 00/1 : 0[170] -> 7[1e0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 01/1 : 8[170] -> 15[1e0] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 01/1 : 6[1d0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 01/1 : 14[1d0] -> 8[170] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 01/1 : 0[170] -> 7[1e0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 00/1 : 15[1e0] -> 8[170] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 00/1 : 7[1e0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 01/1 : 15[1e0] -> 8[170] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 01/1 : 7[1e0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 00/1 : 15[1e0] -> 9[180] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 00/1 : 7[1e0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 01/1 : 15[1e0] -> 9[180] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 01/1 : 7[1e0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 00/1 : 14[1d0] -> 9[180] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 00/1 : 15[1e0] -> 10[190] via P2P/indirect/11[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 00/1 : 6[1d0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 01/1 : 14[1d0] -> 9[180] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 00/1 : 7[1e0] -> 2[190] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 01/1 : 15[1e0] -> 10[190] via P2P/indirect/11[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 01/1 : 6[1d0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 01/1 : 7[1e0] -> 2[190] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 00/1 : 13[1c0] -> 10[190] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 00/1 : 14[1d0] -> 11[1a0] via P2P/indirect/10[190]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 00/1 : 5[1c0] -> 2[190] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 01/1 : 14[1d0] -> 11[1a0] via P2P/indirect/10[190]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 00/1 : 6[1d0] -> 3[1a0] via P2P/indirect/2[190]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 01/1 : 13[1c0] -> 10[190] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 01/1 : 5[1c0] -> 2[190] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 01/1 : 6[1d0] -> 3[1a0] via P2P/indirect/2[190]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 00/1 : 12[1b0] -> 10[190] via P2P/indirect/14[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 01/1 : 12[1b0] -> 10[190] via P2P/indirect/14[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 00/1 : 13[1c0] -> 11[1a0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 00/1 : 5[1c0] -> 3[1a0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 00/1 : 4[1b0] -> 2[190] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 01/1 : 13[1c0] -> 11[1a0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 01/1 : 4[1b0] -> 2[190] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 00/1 : 12[1b0] -> 11[1a0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 01/1 : 5[1c0] -> 3[1a0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 00/1 : 4[1b0] -> 3[1a0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 01/1 : 12[1b0] -> 11[1a0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 01/1 : 4[1b0] -> 3[1a0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO comm 0x5614619a1e10 rank 2 nranks 16 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO comm 0x55e637615030 rank 15 nranks 16 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO comm 0x556aea406600 rank 6 nranks 16 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO comm 0x55bc5f93cee0 rank 13 nranks 16 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO comm 0x562f64a903b0 rank 4 nranks 16 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO comm 0x55a77cad1cf0 rank 11 nranks 16 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO comm 0x55f489270dc0 rank 3 nranks 16 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO comm 0x5608462e8dd0 rank 5 nranks 16 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO comm 0x563f26034550 rank 10 nranks 16 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO comm 0x5575a65358e0 rank 8 nranks 16 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO comm 0x55fa75ba5aa0 rank 7 nranks 16 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO comm 0x55bf8187ec80 rank 9 nranks 16 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:NCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO comm 0x563d0330bb50 rank 12 nranks 16 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO comm 0x55f85b496430 rank 14 nranks 16 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO comm 0x5618e26422c0 rank 1 nranks 16 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO comm 0x562bafa5ba80 rank 0 nranks 16 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Trees [0] 5/-1/-1->1->2 [1] 5/-1/-1->1->2 [2] 2/-1/-1->1->5 [3] 2/-1/-1->1->5 [4] 3/-1/-1->1->0 [5] -1/-1/-1->1->3 [6] 5/-1/-1->1->2 [7] 5/-1/-1->1->2 [8] 2/-1/-1->1->5 [9] 2/-1/-1->1->5 [10] 3/-1/-1->1->0 [11] -1/-1/-1->1->3\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Trees [0] 6/-1/-1->5->1 [1] 6/-1/-1->5->1 [2] 1/-1/-1->5->6 [3] 1/-1/-1->5->6 [4] 4/-1/-1->5->7 [5] 7/-1/-1->5->4 [6] 6/-1/-1->5->1 [7] 6/-1/-1->5->1 [8] 1/-1/-1->5->6 [9] 1/-1/-1->5->6 [10] 4/-1/-1->5->7 [11] 7/-1/-1->5->4\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Trees [0] 2/-1/-1->3->0 [1] 2/-1/-1->3->0 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] 7/-1/-1->3->1 [5] 1/-1/-1->3->7 [6] 2/-1/-1->3->0 [7] 2/-1/-1->3->0 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] 7/-1/-1->3->1 [11] 1/-1/-1->3->7\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Trees [0] 1/-1/-1->2->3 [1] 1/-1/-1->2->3 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] -1/-1/-1->2->6 [5] 6/-1/-1->2->0 [6] 1/-1/-1->2->3 [7] 1/-1/-1->2->3 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] -1/-1/-1->2->6 [11] 6/-1/-1->2->0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Trees [0] 4/-1/-1->7->6 [1] 4/-1/-1->7->6 [2] 6/-1/-1->7->4 [3] 6/-1/-1->7->4 [4] 5/-1/-1->7->3 [5] 3/-1/-1->7->5 [6] 4/-1/-1->7->6 [7] 4/-1/-1->7->6 [8] 6/-1/-1->7->4 [9] 6/-1/-1->7->4 [10] 5/-1/-1->7->3 [11] 3/-1/-1->7->5\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Trees [0] -1/-1/-1->4->7 [1] -1/-1/-1->4->7 [2] 7/-1/-1->4->0 [3] 7/-1/-1->4->0 [4] 6/-1/-1->4->5 [5] 5/-1/-1->4->6 [6] -1/-1/-1->4->7 [7] -1/-1/-1->4->7 [8] 7/-1/-1->4->0 [9] 7/-1/-1->4->0 [10] 6/-1/-1->4->5 [11] 5/-1/-1->4->6\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 5/-1/-1->6->7 [3] 5/-1/-1->6->7 [4] 2/-1/-1->6->4 [5] 4/-1/-1->6->2 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 5/-1/-1->6->7 [9] 5/-1/-1->6->7 [10] 2/-1/-1->6->4 [11] 4/-1/-1->6->2\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1 [1] 3/-1/-1->0->-1 [2] 4/-1/-1->0->-1 [3] 4/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 2/-1/-1->0->-1 [6] 3/-1/-1->0->-1 [7] 3/-1/-1->0->-1 [8] 4/-1/-1->0->-1 [9] 4/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 2/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 05/0 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 04/0 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 11/0 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 10/0 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 02/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 00/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 03/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 01/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 02/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 08/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 00/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 06/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 03/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 09/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 01/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 07/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 08/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 06/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 09/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 05/0 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 07/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 04/0 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 11/0 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 10/0 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 04/0 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 05/0 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 10/0 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 11/0 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 00/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 02/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 03/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 00/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 08/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 02/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 01/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 09/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 03/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 06/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 01/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 08/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 07/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 06/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 09/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 04/0 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 07/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 10/0 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 05/0 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 05/0 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 11/0 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 11/0 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 00/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 04/0 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 01/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 05/0 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 04/0 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 10/0 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 06/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 10/0 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 11/0 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 07/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 02/0 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 03/0 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 08/0 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 00/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 02/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 09/0 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 01/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 03/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 06/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 08/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 07/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 09/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 04/0 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 05/0 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 10/0 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 11/0 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Trees [0] 2/-1/-1->3->0 [1] 2/-1/-1->3->0 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] 7/-1/-1->3->1 [5] 1/-1/-1->3->7 [6] 2/-1/-1->3->0 [7] 2/-1/-1->3->0 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] 7/-1/-1->3->1 [11] 1/-1/-1->3->7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Trees [0] 4/-1/-1->7->6 [1] 4/-1/-1->7->6 [2] 6/-1/-1->7->4 [3] 6/-1/-1->7->4 [4] 5/-1/-1->7->3 [5] 3/-1/-1->7->5 [6] 4/-1/-1->7->6 [7] 4/-1/-1->7->6 [8] 6/-1/-1->7->4 [9] 6/-1/-1->7->4 [10] 5/-1/-1->7->3 [11] 3/-1/-1->7->5\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Trees [0] 6/-1/-1->5->1 [1] 6/-1/-1->5->1 [2] 1/-1/-1->5->6 [3] 1/-1/-1->5->6 [4] 4/-1/-1->5->7 [5] 7/-1/-1->5->4 [6] 6/-1/-1->5->1 [7] 6/-1/-1->5->1 [8] 1/-1/-1->5->6 [9] 1/-1/-1->5->6 [10] 4/-1/-1->5->7 [11] 7/-1/-1->5->4\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Trees [0] -1/-1/-1->4->7 [1] -1/-1/-1->4->7 [2] 7/-1/-1->4->0 [3] 7/-1/-1->4->0 [4] 6/-1/-1->4->5 [5] 5/-1/-1->4->6 [6] -1/-1/-1->4->7 [7] -1/-1/-1->4->7 [8] 7/-1/-1->4->0 [9] 7/-1/-1->4->0 [10] 6/-1/-1->4->5 [11] 5/-1/-1->4->6\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 5/-1/-1->6->7 [3] 5/-1/-1->6->7 [4] 2/-1/-1->6->4 [5] 4/-1/-1->6->2 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 5/-1/-1->6->7 [9] 5/-1/-1->6->7 [10] 2/-1/-1->6->4 [11] 4/-1/-1->6->2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Trees [0] 5/-1/-1->1->2 [1] 5/-1/-1->1->2 [2] 2/-1/-1->1->5 [3] 2/-1/-1->1->5 [4] 3/-1/-1->1->0 [5] -1/-1/-1->1->3 [6] 5/-1/-1->1->2 [7] 5/-1/-1->1->2 [8] 2/-1/-1->1->5 [9] 2/-1/-1->1->5 [10] 3/-1/-1->1->0 [11] -1/-1/-1->1->3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1 [1] 3/-1/-1->0->-1 [2] 4/-1/-1->0->-1 [3] 4/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 2/-1/-1->0->-1 [6] 3/-1/-1->0->-1 [7] 3/-1/-1->0->-1 [8] 4/-1/-1->0->-1 [9] 4/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 2/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Trees [0] 1/-1/-1->2->3 [1] 1/-1/-1->2->3 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] -1/-1/-1->2->6 [5] 6/-1/-1->2->0 [6] 1/-1/-1->2->3 [7] 1/-1/-1->2->3 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] -1/-1/-1->2->6 [11] 6/-1/-1->2->0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 05/0 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 04/0 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 02/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 11/0 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 10/0 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 00/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 00/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 03/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 02/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 01/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 01/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 03/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 06/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 08/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 00/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 08/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 02/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 07/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 06/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 09/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 01/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 09/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 03/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 06/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 07/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 08/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 04/0 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 04/0 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 07/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 09/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 02/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 05/0 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 00/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 05/0 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 10/0 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 10/0 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 01/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 11/0 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 03/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 11/0 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 06/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 08/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 04/0 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 07/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 09/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 10/0 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 05/0 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 04/0 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 11/0 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 10/0 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 00/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 01/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 06/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 02/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 07/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 02/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 03/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 08/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 00/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 00/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 09/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 02/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 03/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 01/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 03/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 06/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 05/0 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 08/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 01/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 07/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 04/0 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 08/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 09/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 11/0 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 05/0 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 06/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 02/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 10/0 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 09/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 11/0 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 00/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 07/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 03/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 01/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 08/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 05/0 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 09/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 11/0 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 06/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 04/0 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 05/0 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 10/0 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 07/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 04/0 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 11/0 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 10/0 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 05/0 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 04/0 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 05/0 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 04/0 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 10/0 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 11/0 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 00/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 11/0 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 10/0 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 00/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 01/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 02/0 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 01/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 02/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 06/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 03/0 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 06/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 05/0 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 03/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 00/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 07/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 07/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 08/0 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 08/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 01/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 11/0 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 00/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 02/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 06/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 09/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 09/0 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 01/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 03/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 06/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 07/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 08/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 07/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 09/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 02/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 05/0 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 04/0 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 03/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 11/0 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 10/0 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 04/0 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 08/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 10/0 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 09/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 05/0 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 04/0 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 11/0 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 10/0 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 00/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 02/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 01/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 03/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 08/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 06/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 00/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 09/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 02/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 07/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 01/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 03/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 06/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 08/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 04/0 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 05/0 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 07/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 09/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 10/0 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 11/0 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 00/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 04/0 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 05/0 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 02/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 01/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 11/0 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 10/0 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 03/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 06/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 08/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 07/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 00/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 09/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 04/0 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 10/0 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 05/0 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 04/0 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 11/0 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 00/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 10/0 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 01/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 02/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 06/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 01/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 03/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 00/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 07/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 06/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 08/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 07/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 01/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 09/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 06/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 02/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 02/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 07/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 03/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 03/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 08/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 09/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 00/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 02/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 08/1 : 7[1e0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 01/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 08/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 03/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 06/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 09/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 05/0 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 08/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 07/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 11/0 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 09/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 04/1 : 6[1d0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 09/1 : 7[1e0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 08/1 : 3[1a0] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 05/0 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 05/1 : 6[1d0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 11/0 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 04/1 : 2[190] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 04/0 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 04/1 : 7[1e0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 09/1 : 3[1a0] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 10/0 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 05/1 : 2[190] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 04/1 : 3[1a0] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 05/1 : 7[1e0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 05/0 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 00/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 11/0 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 01/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 02/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 06/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 03/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 05/0 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 12/1 : 7[1e0] -> 2[190] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 00/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 07/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 08/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 05/1 : 3[1a0] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 11/0 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 01/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 09/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 04/0 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 06/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 10/0 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 07/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO Channel 13/1 : 7[1e0] -> 2[190] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 12/1 : 3[1a0] -> 6[1d0] via P2P/indirect/7[1e0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 12/1 : 6[1d0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 04/0 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 05/0 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 12/1 : 1[180] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 10/0 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 11/0 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 12/1 : 2[190] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO Channel 13/1 : 3[1a0] -> 6[1d0] via P2P/indirect/7[1e0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 12/1 : 5[1c0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 13/1 : 6[1d0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 02/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 13/1 : 1[180] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 03/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 13/1 : 2[190] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 08/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 04/0 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 09/0 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 10/0 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 13/1 : 5[1c0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 10/1 : 4[1b0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 00/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 02/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 01/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 03/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 11/1 : 4[1b0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 06/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 08/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 07/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 09/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 10/1 : 0[170] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 10/1 : 1[180] -> 6[1d0] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 11/1 : 0[170] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 04/0 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 05/0 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 11/1 : 1[180] -> 6[1d0] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 10/0 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 11/0 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 10/1 : 6[1d0] -> 3[1a0] via P2P/indirect/2[190]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 10/1 : 2[190] -> 7[1e0] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 10/1 : 5[1c0] -> 2[190] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO Channel 11/1 : 6[1d0] -> 3[1a0] via P2P/indirect/2[190]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 11/1 : 5[1c0] -> 2[190] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO Channel 11/1 : 2[190] -> 7[1e0] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 06/1 : 4[1b0] -> 2[190] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 06/1 : 0[170] -> 6[1d0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 02/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 03/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 08/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 07/1 : 0[170] -> 6[1d0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 00/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 09/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 05/0 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 04/0 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 01/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 06/1 : 1[180] -> 7[1e0] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 11/0 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 10/0 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 06/1 : 5[1c0] -> 3[1a0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 06/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 07/1 : 4[1b0] -> 2[190] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 00/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 02/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 07/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 01/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 03/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 08/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 06/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO Channel 07/1 : 5[1c0] -> 3[1a0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO Channel 07/1 : 1[180] -> 7[1e0] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 09/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 07/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 14/1 : 4[1b0] -> 3[1a0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 14/1 : 0[170] -> 7[1e0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 08/1 : 3[1a0] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 08/1 : 7[1e0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO Channel 15/1 : 4[1b0] -> 3[1a0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO Channel 15/1 : 0[170] -> 7[1e0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 04/1 : 6[1d0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 04/1 : 2[190] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 09/1 : 3[1a0] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 09/1 : 7[1e0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 04/1 : 7[1e0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 04/1 : 3[1a0] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 05/1 : 6[1d0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 05/1 : 2[190] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 05/1 : 7[1e0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 05/1 : 3[1a0] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 12/1 : 7[1e0] -> 2[190] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 12/1 : 3[1a0] -> 6[1d0] via P2P/indirect/7[1e0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO Channel 13/1 : 7[1e0] -> 2[190] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO Channel 13/1 : 3[1a0] -> 6[1d0] via P2P/indirect/7[1e0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 12/1 : 6[1d0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 12/1 : 5[1c0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 12/1 : 1[180] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 12/1 : 2[190] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 13/1 : 5[1c0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 13/1 : 6[1d0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 13/1 : 1[180] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 10/1 : 0[170] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 13/1 : 2[190] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 10/1 : 4[1b0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 11/1 : 0[170] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 11/1 : 4[1b0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:199:199 [2] NCCL INFO comm 0x563f267b7650 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 10/1 : 6[1d0] -> 3[1a0] via P2P/indirect/2[190]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:205:205 [6] NCCL INFO comm 0x55f85d74e960 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:132:132 [1] NCCL INFO comm 0x55bf81a59500 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:207:207 [3] NCCL INFO comm 0x55a77dce8100 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:198:198 [5] NCCL INFO comm 0x55bc5fb17ac0 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 10/1 : 1[180] -> 6[1d0] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:206:206 [4] NCCL INFO comm 0x563d03644070 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:204:204 [7] NCCL INFO comm 0x55e63842fb10 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO Channel 11/1 : 6[1d0] -> 3[1a0] via P2P/indirect/2[190]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:595:595 [0] NCCL INFO comm 0x5575a6d5f380 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 11/1 : 1[180] -> 6[1d0] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 10/1 : 5[1c0] -> 2[190] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 10/1 : 2[190] -> 7[1e0] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO Channel 11/1 : 2[190] -> 7[1e0] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 11/1 : 5[1c0] -> 2[190] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 06/1 : 4[1b0] -> 2[190] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 06/1 : 1[180] -> 7[1e0] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 07/1 : 4[1b0] -> 2[190] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 06/1 : 0[170] -> 6[1d0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 07/1 : 0[170] -> 6[1d0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 06/1 : 5[1c0] -> 3[1a0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO Channel 07/1 : 5[1c0] -> 3[1a0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 14/1 : 0[170] -> 7[1e0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO Channel 07/1 : 1[180] -> 7[1e0] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 14/1 : 4[1b0] -> 3[1a0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO Channel 15/1 : 0[170] -> 7[1e0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO Channel 15/1 : 4[1b0] -> 3[1a0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:270:270 [6] NCCL INFO comm 0x556aea5e15f0 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:268:268 [3] NCCL INFO comm 0x55f48944ae00 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:263:263 [4] NCCL INFO comm 0x562f64c6f4f0 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:265:265 [2] NCCL INFO comm 0x561462495060 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:271:271 [7] NCCL INFO comm 0x55fa76708660 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:596:596 [0] NCCL INFO comm 0x562bafc51970 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Running smdistributed.dataparallel v1.7.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:SMDDP: Multi node ENA mode\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:261:261 [5] NCCL INFO comm 0x5608464c33c0 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:196:196 [1] NCCL INFO comm 0x5618e2a2b720 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:2024-01-23 04:18:29,796 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:2024-01-23 04:18:29,797 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:2024-01-23 04:18:29,797 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:2024-01-23 04:18:29,797 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:2024-01-23 04:18:29,797 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:2024-01-23 04:18:29,797 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:2024-01-23 04:18:29,797 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:2024-01-23 04:18:29,797 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:2024-01-23 04:18:29,797 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:2024-01-23 04:18:30,797 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:2024-01-23 04:18:30,797 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:2024-01-23 04:18:30,797 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:2024-01-23 04:18:30,797 - torch.distributed.distributed_c10d - INFO - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:2024-01-23 04:18:30,797 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:2024-01-23 04:18:30,797 - torch.distributed.distributed_c10d - INFO - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:2024-01-23 04:18:30,797 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:2024-01-23 04:18:30,797 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:2024-01-23 04:18:30,797 - torch.distributed.distributed_c10d - INFO - Rank 15: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:2024-01-23 04:18:30,797 - torch.distributed.distributed_c10d - INFO - Rank 12: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:2024-01-23 04:18:30,797 - torch.distributed.distributed_c10d - INFO - Added key: store_based_barrier_key:1 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:2024-01-23 04:18:30,797 - torch.distributed.distributed_c10d - INFO - Rank 13: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:2024-01-23 04:18:30,797 - torch.distributed.distributed_c10d - INFO - Rank 14: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:2024-01-23 04:18:30,800 - torch.distributed.distributed_c10d - INFO - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:2024-01-23 04:18:30,800 - torch.distributed.distributed_c10d - INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:2024-01-23 04:18:30,801 - torch.distributed.distributed_c10d - INFO - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:2024-01-23 04:18:30,801 - torch.distributed.distributed_c10d - INFO - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:2024-01-23 04:18:30,801 - torch.distributed.distributed_c10d - INFO - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:2024-01-23 04:18:30,801 - torch.distributed.distributed_c10d - INFO - Rank 8: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:2024-01-23 04:18:30,801 - torch.distributed.distributed_c10d - INFO - Rank 9: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:2024-01-23 04:18:30,801 - torch.distributed.distributed_c10d - INFO - Rank 11: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:2024-01-23 04:18:30,802 - torch.distributed.distributed_c10d - INFO - Rank 10: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:2024-01-23 04:18:30,807 - torch.distributed.distributed_c10d - INFO - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Num examples = 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Instantaneous batch size per device = 32\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  Num examples = 25000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  Instantaneous batch size per device = 32\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  Total train batch size (w. parallel, distributed & accumulation) = 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Total train batch size (w. parallel, distributed & accumulation) = 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Total optimization steps = 49\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  Total optimization steps = 49\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  Number of trainable parameters = 66955010\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Number of trainable parameters = 66955010\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/49 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2024-01-23 04:18:31.177: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2024-01-23 04:18:31.177: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2024-01-23 04:18:31.177: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2024-01-23 04:18:31.177: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 04:18:31.178: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2024-01-23 04:18:31.179: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2024-01-23 04:18:31.180: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:2024-01-23 04:18:31,183 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:2024-01-23 04:18:31,183 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:2024-01-23 04:18:31,183 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:2024-01-23 04:18:31,183 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:2024-01-23 04:18:31,183 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:2024-01-23 04:18:31,185 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:2024-01-23 04:18:31,187 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2024-01-23 04:18:31.187: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2024-01-23 04:18:31.188: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2024-01-23 04:18:31.188: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2024-01-23 04:18:31.188: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2024-01-23 04:18:31.188: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2024-01-23 04:18:31.188: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2024-01-23 04:18:31.188: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:2024-01-23 04:18:31,194 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2024-01-23 04:18:31.193: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:2024-01-23 04:18:31,195 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:2024-01-23 04:18:31,195 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:2024-01-23 04:18:31,195 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:2024-01-23 04:18:31,195 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:2024-01-23 04:18:31,195 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:2024-01-23 04:18:31,195 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2024-01-23 04:18:31.198: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:2024-01-23 04:18:31,200 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:2024-01-23 04:18:31,205 - root - INFO - Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2024-01-23 04:18:31.219 algo-1:265 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 04:18:31.219 algo-1:596 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2024-01-23 04:18:31.219 algo-1:196 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2024-01-23 04:18:31.219 algo-1:261 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2024-01-23 04:18:31.219 algo-1:270 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2024-01-23 04:18:31.222 algo-1:263 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2024-01-23 04:18:31.224 algo-1:268 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2024-01-23 04:18:31.233 algo-1:271 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2024-01-23 04:18:31.233 algo-2:199 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2024-01-23 04:18:31.233 algo-2:132 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2024-01-23 04:18:31.233 algo-2:207 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2024-01-23 04:18:31.234 algo-2:206 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2024-01-23 04:18:31.234 algo-2:205 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2024-01-23 04:18:31.234 algo-2:204 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2024-01-23 04:18:31.239 algo-2:198 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2024-01-23 04:18:31.246 algo-2:595 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2024-01-23 04:18:31.265 algo-1:261 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2024-01-23 04:18:31.265 algo-1:270 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2024-01-23 04:18:31.265 algo-1:265 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 04:18:31.265 algo-1:596 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2024-01-23 04:18:31.265 algo-1:196 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2024-01-23 04:18:31.265 algo-1:263 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2024-01-23 04:18:31.266 algo-1:261 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2024-01-23 04:18:31.266 algo-1:270 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 04:18:31.266 algo-1:596 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2024-01-23 04:18:31.266 algo-1:265 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2024-01-23 04:18:31.266 algo-1:196 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2024-01-23 04:18:31.266 algo-1:263 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2024-01-23 04:18:31.266 algo-1:196 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2024-01-23 04:18:31.266 algo-1:261 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 04:18:31.266 algo-1:596 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2024-01-23 04:18:31.266 algo-1:265 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2024-01-23 04:18:31.266 algo-1:270 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2024-01-23 04:18:31.266 algo-1:263 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2024-01-23 04:18:31.267 algo-1:196 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2024-01-23 04:18:31.267 algo-1:261 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2024-01-23 04:18:31.267 algo-1:196 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2024-01-23 04:18:31.267 algo-1:270 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2024-01-23 04:18:31.267 algo-1:261 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 04:18:31.267 algo-1:596 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2024-01-23 04:18:31.267 algo-1:270 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2024-01-23 04:18:31.267 algo-1:596 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2024-01-23 04:18:31.267 algo-1:265 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2024-01-23 04:18:31.267 algo-1:263 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2024-01-23 04:18:31.267 algo-1:265 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2024-01-23 04:18:31.267 algo-1:263 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2024-01-23 04:18:31.268 algo-1:268 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2024-01-23 04:18:31.269 algo-1:268 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2024-01-23 04:18:31.269 algo-1:268 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2024-01-23 04:18:31.270 algo-1:268 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2024-01-23 04:18:31.270 algo-1:268 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2024-01-23 04:18:31.278 algo-1:271 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2024-01-23 04:18:31.279 algo-1:271 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2024-01-23 04:18:31.279 algo-2:205 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2024-01-23 04:18:31.279 algo-2:204 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2024-01-23 04:18:31.279 algo-2:206 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2024-01-23 04:18:31.279 algo-2:132 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2024-01-23 04:18:31.279 algo-2:207 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2024-01-23 04:18:31.279 algo-2:199 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2024-01-23 04:18:31.279 algo-1:271 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2024-01-23 04:18:31.279 algo-2:205 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2024-01-23 04:18:31.279 algo-2:206 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2024-01-23 04:18:31.279 algo-2:132 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2024-01-23 04:18:31.279 algo-2:207 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2024-01-23 04:18:31.279 algo-2:199 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2024-01-23 04:18:31.279 algo-2:204 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2024-01-23 04:18:31.280 algo-1:271 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2024-01-23 04:18:31.280 algo-2:205 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2024-01-23 04:18:31.280 algo-1:271 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2024-01-23 04:18:31.280 algo-2:206 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2024-01-23 04:18:31.280 algo-2:132 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2024-01-23 04:18:31.280 algo-2:204 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2024-01-23 04:18:31.280 algo-2:207 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2024-01-23 04:18:31.280 algo-2:199 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2024-01-23 04:18:31.280 algo-2:207 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2024-01-23 04:18:31.280 algo-2:199 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2024-01-23 04:18:31.280 algo-2:206 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2024-01-23 04:18:31.280 algo-2:207 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2024-01-23 04:18:31.280 algo-2:205 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2024-01-23 04:18:31.280 algo-2:199 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2024-01-23 04:18:31.281 algo-2:204 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2024-01-23 04:18:31.280 algo-2:206 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2024-01-23 04:18:31.280 algo-2:132 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2024-01-23 04:18:31.281 algo-2:205 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2024-01-23 04:18:31.281 algo-2:204 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2024-01-23 04:18:31.281 algo-2:132 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2024-01-23 04:18:31.286 algo-2:198 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2024-01-23 04:18:31.287 algo-2:198 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2024-01-23 04:18:31.287 algo-2:198 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2024-01-23 04:18:31.288 algo-2:595 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2024-01-23 04:18:31.288 algo-2:198 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2024-01-23 04:18:31.288 algo-2:198 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2024-01-23 04:18:31.288 algo-2:595 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2024-01-23 04:18:31.289 algo-2:595 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2024-01-23 04:18:31.289 algo-2:595 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2024-01-23 04:18:31.289 algo-2:595 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  2%|â–         | 1/49 [00:01<01:34,  1.98s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:2024-01-23 04:18:33,078 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:2024-01-23 04:18:33,078 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:2024-01-23 04:18:33,078 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:2024-01-23 04:18:33,078 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:2024-01-23 04:18:33,078 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:2024-01-23 04:18:33,078 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:2024-01-23 04:18:33,078 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:2024-01-23 04:18:33,078 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:2024-01-23 04:18:33,081 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:2024-01-23 04:18:33,081 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:2024-01-23 04:18:33,081 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:2024-01-23 04:18:33,081 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:2024-01-23 04:18:33,081 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:2024-01-23 04:18:33,081 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:2024-01-23 04:18:33,081 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:2024-01-23 04:18:33,081 - torch.nn.parallel.distributed - INFO - Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  4%|â–         | 2/49 [00:02<00:55,  1.18s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  6%|â–Œ         | 3/49 [00:03<00:41,  1.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  8%|â–Š         | 4/49 [00:03<00:33,  1.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 10%|â–ˆ         | 5/49 [00:04<00:29,  1.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 12%|â–ˆâ–        | 6/49 [00:04<00:27,  1.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 14%|â–ˆâ–        | 7/49 [00:05<00:25,  1.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 16%|â–ˆâ–‹        | 8/49 [00:05<00:23,  1.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 18%|â–ˆâ–Š        | 9/49 [00:06<00:22,  1.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|â–ˆâ–ˆ        | 10/49 [00:06<00:22,  1.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 22%|â–ˆâ–ˆâ–       | 11/49 [00:07<00:21,  1.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 24%|â–ˆâ–ˆâ–       | 12/49 [00:08<00:20,  1.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 27%|â–ˆâ–ˆâ–‹       | 13/49 [00:08<00:19,  1.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 29%|â–ˆâ–ˆâ–Š       | 14/49 [00:09<00:19,  1.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 31%|â–ˆâ–ˆâ–ˆ       | 15/49 [00:09<00:18,  1.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 16/49 [00:10<00:18,  1.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 35%|â–ˆâ–ˆâ–ˆâ–      | 17/49 [00:10<00:17,  1.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 37%|â–ˆâ–ˆâ–ˆâ–‹      | 18/49 [00:11<00:17,  1.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 39%|â–ˆâ–ˆâ–ˆâ–‰      | 19/49 [00:11<00:16,  1.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/49 [00:12<00:16,  1.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 21/49 [00:13<00:15,  1.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/49 [00:13<00:15,  1.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 23/49 [00:14<00:14,  1.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 24/49 [00:14<00:14,  1.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/49 [00:15<00:13,  1.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 26/49 [00:15<00:12,  1.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 27/49 [00:16<00:12,  1.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 28/49 [00:17<00:11,  1.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 29/49 [00:17<00:11,  1.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/49 [00:18<00:10,  1.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 31/49 [00:18<00:09,  1.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 32/49 [00:19<00:09,  1.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 33/49 [00:19<00:08,  1.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 34/49 [00:20<00:08,  1.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 35/49 [00:20<00:07,  1.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 36/49 [00:21<00:07,  1.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 37/49 [00:21<00:06,  1.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 38/49 [00:22<00:05,  1.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 39/49 [00:23<00:05,  1.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 40/49 [00:23<00:04,  1.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 41/49 [00:24<00:04,  1.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 42/49 [00:24<00:03,  1.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 43/49 [00:25<00:03,  1.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 44/49 [00:25<00:02,  1.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45/49 [00:26<00:02,  1.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/49 [00:26<00:01,  1.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 47/49 [00:27<00:01,  1.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 48/49 [00:27<00:00,  1.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:28<00:00,  1.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  Num examples = 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  Batch size = 64\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Num examples = 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Batch size = 64\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/10 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|â–ˆâ–ˆ        | 2/10 [00:00<00:01,  6.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:00<00:01,  4.39it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:00<00:01,  3.81it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:01<00:01,  3.53it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:01<00:01,  3.35it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:01<00:00,  3.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:02<00:00,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:02<00:00,  3.18it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.16it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015                                               #015\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015                                               #015[1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:{'eval_loss': 0.6573686599731445, 'eval_accuracy': 0.7476, 'eval_f1': 0.7683553597650514, 'eval_precision': 0.7070945945945946, 'eval_recall': 0.8412379421221865, 'eval_runtime': 3.2385, 'eval_samples_per_second': 3087.852, 'eval_steps_per_second': 3.088, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:31<00:00,  1.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.16it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015                                               #033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:{'train_runtime': 31.6811, 'train_samples_per_second': 789.113, 'train_steps_per_second': 1.547, 'train_loss': 0.6915402315100845, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:31<00:00,  1.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:31<00:00,  1.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Num examples = 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  Batch size = 64\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  Num examples = 10000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  Batch size = 64\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/10 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|â–ˆâ–ˆ        | 2/10 [00:00<00:01,  6.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:00<00:01,  4.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:00<00:01,  3.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:01<00:01,  3.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:01<00:01,  3.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:01<00:00,  3.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:02<00:00,  3.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:02<00:00,  3.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:***** Eval results *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:***** Eval results *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:***** Eval results *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:***** Eval results *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:***** Eval results *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:***** Eval results *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:***** Eval results *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:***** Eval results *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:***** Eval results *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:***** Eval results *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:***** Eval results *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:***** Eval results *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:***** Eval results *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:***** Eval results *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:***** Eval results *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:***** Eval results *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m2024-01-23 04:19:15,208 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-01-23 04:19:15,208 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-01-23 04:19:15,209 sagemaker-training-toolkit INFO     Begin writing status file from leader node to worker nodes\u001b[0m\n",
      "\u001b[34m2024-01-23 04:19:15,209 sagemaker-training-toolkit INFO     Start writing mpirun finished status to algo-2\u001b[0m\n",
      "\u001b[34m2024-01-23 04:19:15,368 sagemaker-training-toolkit INFO     output from subprocess run CompletedProcess(args=['ssh', 'algo-2', 'touch', '/tmp/done.algo-1'], returncode=0, stdout='', stderr='')\u001b[0m\n",
      "\u001b[34m2024-01-23 04:19:15,368 sagemaker-training-toolkit INFO     Finished writing status file\u001b[0m\n",
      "\u001b[35m2024-01-23 04:19:15,242 sagemaker-training-toolkit INFO     Invoked on_terminate from psutil.wait_for_procs\u001b[0m\n",
      "\u001b[35m2024-01-23 04:19:15,242 sagemaker-training-toolkit INFO     process psutil.Process(pid=58, name='orted', status='terminated', started='04:18:14') terminated with exit code None\u001b[0m\n",
      "\u001b[35m2024-01-23 04:19:15,242 sagemaker-training-toolkit INFO     Reporting status for ORTEd process. gone: [psutil.Process(pid=58, name='orted', status='terminated', started='04:18:14')] alive: []\u001b[0m\n",
      "\u001b[35m2024-01-23 04:19:15,242 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\u001b[34m2024-01-23 04:19:45,398 sagemaker-training-toolkit INFO     Finished writing status file from leader node to worker nodes\u001b[0m\n",
      "\u001b[34m2024-01-23 04:19:45,398 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[35m2024-01-23 04:19:45,273 sagemaker-training-toolkit INFO     Begin looking for status file on algo-2\u001b[0m\n",
      "\u001b[35m2024-01-23 04:19:45,273 sagemaker-training-toolkit INFO     MPI training job status file found. Exit gracefully\u001b[0m\n",
      "\u001b[35m2024-01-23 04:19:45,273 sagemaker-training-toolkit INFO     End looking for status file\u001b[0m\n",
      "\u001b[35m2024-01-23 04:19:45,273 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[35m2024-01-23 04:19:45,273 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-01-23 04:20:09 Uploading - Uploading generated training model\n",
      "2024-01-23 04:20:09 Completed - Training job completed\n",
      "Training seconds: 836\n",
      "Billable seconds: 836\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1,\n",
    "                 'train_batch_size': 32,\n",
    "                 'model_name':'distilbert-base-uncased'\n",
    "                 }\n",
    "\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}\n",
    "\n",
    "# instance configurations\n",
    "instance_type='ml.p3.16xlarge'\n",
    "instance_count=2\n",
    "volume_size=200\n",
    "\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            instance_type=instance_type,\n",
    "                            instance_count=instance_count,\n",
    "                            volume_size=volume_size,\n",
    "                            role=role,\n",
    "                            transformers_version='4.26.0',\n",
    "                            pytorch_version='1.13.1',\n",
    "                            py_version='py39',\n",
    "                            distribution= distribution,\n",
    "                            hyperparameters = hyperparameters)\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff99d5b-5c62-4c92-9dda-1cd88af53eb5",
   "metadata": {},
   "source": [
    "## Deploying the endpoint\n",
    "\n",
    "To deploy our endpoint, we call deploy() on our HuggingFace estimator object, passing in our desired number of instances and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60401041-176e-45b8-9b3a-5e8b0d84f97e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-training-2024-01-23-04-21-48-811\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-pytorch-training-2024-01-23-04-21-48-811\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-pytorch-training-2024-01-23-04-21-48-811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.5309359431266785}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor = huggingface_estimator.deploy(1, \"ml.g4dn.xlarge\")\n",
    "\n",
    "sentiment_input= {\"inputs\":\"I am not sure whether I will use the new Inference DLC.\"}\n",
    "predictor.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b745c06b-83a1-4445-91ae-066c5e92e6d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment_input= {\"inputs\":\"I love using the new Inference DLC.\"}\n",
    "predictor.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c6b7fe-dd23-4d67-a8b4-bb9f58425f27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment_input= {\"inputs\":\"Caring is most important\"}\n",
    "predictor.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb4ab6e-35ad-408d-8856-ea53126df3a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the endpoint\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7689e6-84b8-42fb-ba61-84fc3cbacd44",
   "metadata": {},
   "source": [
    "## Estimator Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e223d2-1aa9-419f-9404-204697e6fa63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# container image used for training job\n",
    "print(f\"container image used for training job: \\n{huggingface_estimator.image_uri}\\n\")\n",
    "\n",
    "# s3 uri where the trained model is located\n",
    "print(f\"s3 uri where the trained model is located: \\n{huggingface_estimator.model_data}\\n\")\n",
    "\n",
    "# latest training job name for this estimator\n",
    "print(f\"latest training job name for this estimator: \\n{huggingface_estimator.latest_training_job.name}\\n\")\n",
    "\n",
    "# access the logs of the training job\n",
    "huggingface_estimator.sagemaker_session.logs_for_job(huggingface_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f6617a-bb40-4732-8780-3b3bd80dc326",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8923c4a-a92a-4163-a135-c4e7286946bb",
   "metadata": {},
   "source": [
    "## Upload the fine-tuned model to huggingface.co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87011e27-a0fc-4d34-9e2f-541053637588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1755be25-40b8-4f5e-8443-279e888a4160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddba9287-5015-47a8-b7ba-704e31ea7e36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ad2ff9-3517-4618-a9a1-e75f9158fea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3923c57-4876-4807-85cb-d7d26e62e195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a545d5d-f917-4e5e-a9a8-c0ec338464db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eda06c-817d-4a00-8ebe-36b363232005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f3e31-706d-4e66-b15d-9b06886272ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feba936-27a0-47dc-8a2e-61d23f55e850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e2e3aa-35da-4d25-a9c2-e38ce2bc3cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c52224b-c43c-4dde-93fb-154401f0b195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e4e2df-72b6-42e6-aeeb-c374c21a9700",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
